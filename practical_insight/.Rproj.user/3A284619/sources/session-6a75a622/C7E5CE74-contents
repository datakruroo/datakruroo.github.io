---
title: "Basic Regression Example"
subtitle: "<b>Practical Insight: Doing Data Science in Education with R</b>"
author: ผศ.ดร.สิวะโชติ ศรีสุทธิยากร
logo: "images/logo.png"
footer: "[Practical Insight: Doing Data Science in Education with R](https://datakruroo.netlify.app/practical_insight/_site/)"
format: 
  html:
    number_sections: true
editor: visual
execute:
  freeze: auto
---

## Types of Data Analytics

<center>![](images/analytictypes.png){width="80%"}</center>

## สถานการณ์

นักวิเคราะห์ต้องการทำนายเงินเดือนของอาจารย์มหาวิทยาลัย ด้วยตัวแปรทำนายได้แก่ เพศ ตำแหน่งทางวิชาการ สาขาวิชา ประสบการณ์ในการเป็นอาจารย์ และประสบการณ์ทำงานตั้งแต่จบปริญญาเอก

```{r}
library(readr)
dat <- read_csv("TeacherSalaryData.csv")
head(dat)
```

## มโนทัศน์ของการพัฒนาโมเดลทำนาย

เราต้องการโมเดลทำนายที่มีคุณสมบัตื

-   Unbiased

-   Minimum variance

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-1638148101.png)

## กระบวนการพัฒนาโมเดลทำนาย

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-1728379488.png)

## Resampling

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-468626522.png)

-   K-fold cross-validation

-   Bootstraping

## K-fold cross-validation

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-1150746472.png)

## [Tidymodels Framework](https://www.tidymodels.org/start/)

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-258190158.png)

-   package-rsample ใช้ในงาน resampling ข้อมูล เช่นการสร้าง training/validation/test dataset การสร้าง cross-validation dataset หรือการสร้าง bootstrape dataset ซึ่งได้กล่าวการใช้งานเบื้องต้นไปแล้ว

-   package-recipes ใช้แปลง/แก้ปัญหาที่เกิดขึ้นในข้อมูลของตัวแปรที่ใช้ในการพัฒนาโมเดล ขั้นตอนนี้เรียกว่า feature engineering

-   package-parsnip ใช้ fit machine learning กับข้อมูล โมเดลที่สามารถ fit ได้จาก parsnip มีหลายโมเดล เช่น linear regression, logistic regression, random forest, support vector machine, neural network, gradient boosting machine, etc. ผู้เรียนสามารถอ้างอิงการระบุโมเดลต่าง ๆ จาก <https://www.tidymodels.org/find/parsnip/>

-   package-Tune และ package-dials มีฟังก์ชันที่อำนวยความสะดวกในการ fine tune hyperparameter ของโมเดลเพื่อเพิ่มประสิทธิภาพการทำนายของโมเดลให้สูงที่สุด

-   package-yardstick มีฟังก์ชันของ metric ที่ใช้ประเมินประสิทธิภาพของโมเดลทำนาย ตัวชี้วัดสำหรับโมเดล regression ได้แก่ RMSE, MAE, MAPE, R-squared, etc.

          **RMSE**

$$
RMSE = \sqrt\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{n}
$$

          **R-squared**

$$
R^2=Corr(y,\hat{y})^2
$$

tidymodels ถูกพัฒนาขึ้นโดยได้รับการออกแบบให้สามารถทำซ้ำกระบวนการพัฒนาโมเดลได้ง่าย โดยใช้ไวยกรณ์ของภาษาในลักษณะเดียวกัน และถูกออกแบบโดยเน้นใช้กับ supervised learning เป็นหลัก ผู้ใช้งานไม่จำเป็นต้องติดตั้งทุก package ในข้างต้นด้วยตนเอง แต่ติดตั้งเพียง package-tidymodels ก็สามารถใช้งานทุก package ภายใต้ framework ดังกล่าวได้แล้ว โดยการพิมพ์คำสั่งต่อไปนี้

```{r message = F}
#install.packages("tidymodels")
library(tidymodels)
```

ขั้นตอนการพัฒนาโมเดลด้วย tidymodels framework มีดังนี้

### 1. แบ่งชุดข้อมูลออกเป็น training และ test set

```{r}
set.seed(123)
split <- initial_split(dat, prop = 0.8)
train <- training(split)
test <- testing(split)
```

### 2. สำรวจข้อมูลเบื้องต้น

```{r eval = F}
library(DataExplorer)
plot_intro(train)
plot_missing(train)
plot_bar(train)
plot_histogram(train)
plot_correlation(train)
```

```{r}
train |> 
  ggplot(aes(x=yrs.service, y=salary))+
  geom_point()+
  geom_smooth(aes(col = discipline), method = "lm")

train |> 
  ggplot(aes(x=yrs.since.phd, y=salary))+
  geom_point()+
  geom_smooth(aes(col = discipline), method = "lm")

train |>
  ggplot(aes(x=rank, y=salary))+
  geom_boxplot(aes(fill = discipline))
```

### 3. สร้าง recipe สำหรับทำ data preprocessing ใน training set

```{r}
### recipe1: no interaction
rec_noint <- recipe(salary ~ ., data = train) |> 
  step_select(-1) |> 
  step_mutate(rank = factor(rank, levels=c("AsstProf", "AssocProf", "Prof")),
              discipline = factor(discipline, labels=c("science","social"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  prep(train) 


### recipe2: with interaction
rec_int <- recipe(salary ~ ., data = train) |> 
  step_select(-1) |> 
  step_mutate(rank = factor(rank, levels=c("AsstProf", "AssocProf", "Prof")),
              discipline = factor(discipline, labels=c("science","social"))) |>
  step_normalize(all_numeric_predictors()) |>
  step_interact(terms = ~ yrs.service:discipline) |>
  step_dummy(all_nominal_predictors()) |>
  prep(train)
```

### 4. สร้าง model specification และ fit model ใน training set

ในกรณีนี้จะลองใช้ 3 โมเดล คือ linear regression, regularized regression และ KNN

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-676691868.png){width="50%"}

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-849757700.png)

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-343319787.png)

ประเภทของ regularization จำแนกได้ 3 ประเภท ได้แก่

-   ridge regression

-   lasso regression

-   elastic net regression

#### หลักการคร่าว ๆ ของ ridge regression

ฟังก์ชันวัตถุประสงค์ของ regularized regression ประเภท ridge regression สามารถเขียนได้ดังนี้

$$
\underset{\beta}{min}\left\{SSE+\lambda\sum_{j=1}^p\beta_j^2\right\}
$$ ![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-75951276.png) จากผลในข้างต้นจะเห็นว่า ridge regression เป็นเทคนิคที่ช่วยปรับขนาดของสัมประสิทธิ์ความชันของตัวแปรอิสระในโมเดลให้มีค่าลดลง แต่จะไม่ได้ทำให้เป็น 0 ดังนั้นตัวแปรอิสระทั้งหมดที่ผู้วิเคราะห์นำเข้าในอัลกอริทึมตอนแรกนั้นจะอยู่ในโมเดลทำนายครบทุกตัว แต่จะถูกปรับลดขนาดของค่าสัมประสิทธิ์ความชันลง ดังนั้น ridge regression จึงไม่ใช่เทคนิคสำหรับคัดเลือกตัวแปรอิสระ แต่เป็นเทคนิคที่เหมาะสำหรับแก้ปัญหา multicollinearity มากกว่า ดังนั้น ridge regression จึงเหมาะที่จะใช้ในสถานการณ์ที่ผู้วิเคราะห์มีตัวแปรอิสระจำนวนมากและตัวแปรดังกล่าวมีความสัมพันธ์กันเองสูงหรือมีความซ้ำซ้อนกัน แต่ผู้วิเคราะห์ไม่ต้องการที่จะตัดตัวแปรอิสระตัวใดออกจากโมเดลทำนาย อัลกอริทึม ridge regression จะช่วยให้ผู้วิเคราะห์สามารถสร้างโมเดลทำนายที่มีตัวแปรอิสระทั้งหมดอยู่ภายในโมเดลโดยหลีกเลี่ยงหรือลดทอนผลกระทบที่เกิดจากปัญหา multicollinearity ได้

#### lasso regression

::: columns
::: {.column width="50%"}
Lasso regression เป็นอัลกอริทึมที่ถูกพัฒนาขึ้นโดยมีวัตถุประสงค์หลักคือการคัดเลือกตัวแปรอิสระ (feature selection) เข้าสู่โมเดลทำนาย อัลกอริทึมนี้เป็น feature selection ที่จัดอยู่ในกลุ่ม embedded method กล่าวคือเป็นอัลกอริทึมการเรียนรู้ที่มีอัลกอริทึมของการคัดเลือกตัวแปรอิสระรวมอยู่ในขั้นตอนการประมาณค่าพารามิเตอร์ของโมเดล หลักการของ lasso regression เหมือนกับ ridge regression แต่มีการใช้ penalty term ในฟังก์ชันวัตถุประสงค์ที่แตกต่างออกไปดังนี้

$$
\underset{\beta}{min}\left\{SSE+\lambda\sum_{j=1}^p|\beta_j|\right\}
$$
:::

::: {.column width="50%"}
![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-1836348765.png){width="100%"}
:::
:::

#### KNN

![](images/knn.png){width="70%"}

```{r}
ols_reg <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

ridge_reg <- linear_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet") %>%
  set_mode("regression")

knn_reg <- nearest_neighbor(weight_func = "gaussian", neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")

rand_forest <- rand_forest(trees = 500, mtry = tune(), min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")
```

### 5. สร้าง workflowset และ tune พารามิเตอร์

```{r}
my_workflowset <- workflow_set(
  preproc = list(rec_noint = rec_noint, rec_int = rec_int, rec_noint = rec_noint, rec_noint = rec_noint),
  models = list(
    ols = ols_reg,
    ridge = ridge_reg,
    knn = knn_reg,
    rand_forest = rand_forest
  ),
  cross = FALSE
)
my_workflowset
```

กำหนด evaluation metric และ tune hyperparameter เมื่อทำการ train เสร็จ tidymodel จะให้ผลลัพธ์เป็น tibble ที่บรรจุผลการ train ของแต่ละโมเดลเอาไว้

```{r}
## กำหนด evaluation metric
eval_metric <- metric_set(rmse, rsq)
train_result <- my_workflowset |> 
  workflow_map(
    grid = 10,
    resamples = vfold_cv(train, v = 5, repeats = 3),
    metrics = eval_metric
  )
train_result
```

### 6. ประเมินผลลัพธ์ของโมเดล

```{r}
train_result |>
  autoplot()

train_result |> 
  rank_results() 


train_result |> 
  collect_metrics() |> 
  filter(.metric == "rsq") |> 
  arrange(desc(mean))
```

### 7. คัด best model ไปตรวจสอบผลลัพธ์ในข้อมูล test

```{r}
### วิเคราะห์ OLS
train_result |> 
  extract_workflow_set_result(id = "rec_noint_ols") |>
  collect_metrics(summarise = T) |> 
  filter(.metric == "rsq") |> 
  arrange(-mean)

### วิเคราะห์ Ridge Regression

train_result |> 
  extract_workflow_set_result(id = "rec_int_ridge") |> 
  collect_metrics(summarise = T) |> 
  filter(.metric == "rsq") |> 
  arrange(-mean)

best_ridge <- train_result |> 
  extract_workflow_set_result(id = "rec_int_ridge") |> 
  show_best("rsq", n = 1)

### วิเคราะห์ KNN algorithm
knn_result <- train_result |> 
  extract_workflow_set_result(id = "rec_noint_knn") |> 
  collect_metrics(summarise = T) |> 
  filter(.metric == "rsq") |> 
  arrange(-mean)

best_knn <- train_result |> 
  extract_workflow_set_result(id = "rec_noint_knn") |> 
  show_best("rsq", n = 1)

### วิเคราะห์ Random Forest


train_result |> 
  extract_workflow_set_result(id = "rec_noint_rand_forest") |> 
  collect_metrics(summarise = T) |> 
  filter(.metric == "rsq") |> 
  arrange(-mean)

best_rf <- train_result |> 
  extract_workflow_set_result(id = "rec_noint_rand_forest") |> 
  show_best("rsq", n = 1)
```

### 8. ประเมินผลใน test data

เมื่อคัดเลือก best model ของแต่ละอัลกอริทึมได้แล้ว จะ train ใหม่กับข้อมูลทั้งหมด เพื่อนำไปตรวจสอบใน test data

```{r}
ols_trained <- train_result |> 
  extract_workflow(id = "rec_noint_ols") |>
  fit(train)

ridge_trained <- train_result |> 
  extract_workflow(id = "rec_int_ridge") |>
  finalize_workflow(best_ridge) |>
  fit(train)

knn_trained <- train_result |> 
  extract_workflow(id = "rec_noint_knn") |>
  finalize_workflow(best_knn) |>
  fit(train)

rf_trained <- train_result |> 
  extract_workflow(id = "rec_noint_rand_forest") |>
  finalize_workflow(best_rf) |>
  fit(train)
```

นำโมเดลทั้งหมดไปทำนาย salary ใน test

```{r}
## data preprocessing
test_noint <- rec_noint |> 
              bake(test)

test_int <- rec_int |>
            bake(test)


## ทำนาย
ols_pred <- ols_trained |> 
  extract_fit_parsnip() |>
  predict(test_noint) |> 
  bind_cols(test_noint) |> 
  select(salary, .pred)

ridge_pred <- ridge_trained |> 
  extract_fit_parsnip() |>
  predict(test_int) |> 
  bind_cols(test_int) |> 
  select(salary, .pred)

knn_pred <- knn_trained |> 
  extract_fit_parsnip() |>
  predict(test_noint) |> 
  bind_cols(test_noint) |> 
  select(salary, .pred)

rf_pred <- rf_trained |> 
  extract_fit_parsnip() |>
  predict(test_noint) |> 
  bind_cols(test_noint) |> 
  select(salary, .pred)

pred_dat <- bind_rows(
  ols_pred |> mutate(model = "ols"),
  ridge_pred |> mutate(model = "ridge"),
  knn_pred |> mutate(model = "knn"),
  rf_pred |> mutate(model = "rf")
)

pred_dat |> 
  ggplot(aes(x = salary, y = .pred)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)+
  facet_wrap(~model, scales = "fixed")

```
