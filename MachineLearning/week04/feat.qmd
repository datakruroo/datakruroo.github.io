---
title: "Feature Engineering"
output: html_document
editor: visual
echo: false
---


## 1. ความหมายความสำคัญของ Feature Engineering

- Feature Engineering คือกระบวนการแปลงหรือปรับแต่งข้อมูลดิบ (raw data) ให้อยู่ในรูปแบบที่เหมาะสมสำหรับการสร้างโมเดลการเรียนรู้ของเครื่อง 

- การสร้างโมเดล ML หากไม่มีการเตรียมข้อมูลที่ดี โมเดลก็อาจไม่สามารถเรียนรู้รูปแบบที่สำคัญจากข้อมูลได้อย่างมีประสิทธิภาพ

- Feature Engineering มีความสำคัญดังนี้

1. เพิ่มประสิทธิภาพของโมเดล

2. ลดข้อจำกัดของข้อมูล

3. ช่วยให้การแปลความหมายชัดเจนขึ้น เข้าใจได้มากขึ้น หรือช่วยให้เราพบ insight ที่ซ่อนอยู่ในข้อมูล


- ควรทำ feature engineering ตอนไหน?

1. ก่อนที่จะ train model

2. ระหว่างที่ train model

3. หลังจากที่ train model แล้ว


### 1.1 หลักการสำคัญในการทำ feature engineering

- ทำความเข้าใจข้อมูลเสมอ ได้แก่ ทราบสถานะของข้อมูล ทำความเข้าใจการแจกแจงของข้อมูล เข้าใจความสัมพันธ์เบื้องต้นระหว่างตัวแปร

- มีเป้าหมายในการดำเนินงานที่ชัดเจน มีตัวชี้วัด

- ระวังการเกิดปัญหา data leakage : กระบวนการนี้ทำบน training data เท่านั้น

## 2. เตรียมชุดข้อมูล

```{r}
library(tidyverse)
library(tidymodels)
data <- read_delim("student-mat.csv", delim= ";")
data <- data %>%
  mutate_at(vars(school, sex, address, famsize, Pstatus, 
                 Medu, Fedu, Mjob, Fjob, reason, guardian,
                 traveltime, studytime, schoolsup,
                 famsup, paid, activities, nursery, higher,internet,
                 romantic, famrel, freetime, goout, Dalc, Walc, health), 
            factor)

set.seed(123)
split <- initial_split(data, prop = 0.8, strata = G3)
train <- training(split)
test <- testing(split)

glimpse(train)
```

## 3. ประเภทของ Feature Engineering

- Encoding Categorical Variables

- Transforming Numerical Variables

- Interaction Terms

- Handling Missing Data

การทำ feature engineering ใน tidymodels จะใช้ library-{recipe}

```{r}
train %>% 
  mutate(absences_norm = scale(absences)) %>% 
  mutate(school_MS = ifelse(school == "MS", 1,0))
```

```{r}
recipe(formula = G3 ~ ., data = train) %>% 
  step_normalize(absences) %>% 
  step_dummy(school) %>% 
  prep() %>% 
  juice()

lm_rec_trained <- recipe(formula = G3 ~ ., data = train) %>% 
  step_normalize(absences) %>% 
  step_dummy(school) %>% 
  prep()

lm_rec_trained %>% 
  bake(new_data =test)
```


```{r}
library(tidymodels)

## 1. การสร้าง data preprocessing model ด้วย recipe
lm_rec <- recipe(formula = G3 ~ ., data = train) %>% 
  step_normalize(absences) %>% 
  step_dummy(school) 

## 2. regression model
lm_mod <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

## 3. สร้าง workflow
lm_wf <- workflow() %>% 
  add_recipe(lm_rec) %>% 
  add_model(lm_mod)
```


## 4. Encoding Categorical Variables

วัตถุประสงค์คือการแปลงข้อมูลจัดประเภทให้อยู่ในรูปแบบที่สามารถนำไปวิเคราะห์ได้ด้วยโมเดลการเรียนรู้ของเครื่อง

```{r}
data <- read_delim("student-mat.csv", delim= ";")

set.seed(123)
split <- initial_split(data, prop = 0.8, strata = G3)
train <- training(split)
test <- testing(split)


recipe(G3 ~. ,data= train) %>% 
  step_string2factor() %>% 
  step_other(Mjob, other = "other2", threshold = 0.30) %>% 
  prep() %>% 
  juice() %>% 
  count(Mjob)
```


```{r}
train %>% 
  mutate(G1_intervel = cut(G1, breaks = 5)) %>%
  select(G1, G1_intervel)
```



```{r}
## install.packages("embed")
library(embed)
```

```{r eval = F}

step_string2factor()
step_factor2string()
step_num2factor()
step_unorder()
step_dummy()
step_other()
step_unknown()
step_novel()

step_discretize()
step_discretize_cart()
step_discretize_xgb()


recipe(G3 ~ ., data= train) %>% 
  step_lencode_glm(Fjob, outcome = "G3") %>% 
  prep() %>% 
  juice()

step_lencode_glm()
step_lencode_bayes()
step_lencode_mixed()


step_woe()

recipe(romantic ~ ., data= train) %>% 
  step_woe(Fjob, outcome = "romantic") %>% 
  prep() %>% 
  juice()


```


Note: 

- `step_discretize_cart()` และ `step_discretize_xgb()` ใช้การแบ่งกลุ่มของตัวแปรเชิงปริมาณให้เป็นกลุ่มหรือตัวแปรประเภท โดยใช้โมเดล Classification and Regression Trees (CART) และ Extreme Gradient Boosting (XGBoost) ตามลำดับ หลักการทำงานคร่าว ๆ ทั้งสองอัลกอริทึมเป็นกลุ่ม supervised discretization ใช้การเรียนรู้ของเครื่องช่วยแบ่งตัวแปรเชิงปริมาณให้ได้ผลลัพธ์เป็นตัวแปรจัดประเภทที่มีความสัมพันธ์กับตัวแปรตามมากที่สุด ทั้งสองฟังก์ชันสามารถ tune ค่า hyperparameter ได้เพื่อให้ได้ผลลัพธ์ที่ต้องการ


- `step_lencode_glm()` เป็นฟังก์ชันในกลุ่ม Supervised Factor Conversion มีหน้าที่แปลง Factor ให้เป็นตัวเลขโดยใช้โมเดล Generalized Linear Model (GLM) เราอาจอธิบายหลักการทำงานคร่าว ๆ ได้ดังนี้

1. สร้าง glm ที่ทำนาย outcome ของโมเดลกับ predictor ที่เป็น factor 

2. ใช้ค่าสัมประสิทธิ์ของโมเดลที่ได้จาก glm เป็นคะแนนของระดับปัจจัยแต่ละระดับ

- `step_lencode_bayes()` หลักการเดียวกับ glm แต่เปลี่ยนการประมาณค่าพารามิเตอร์ในโมเดลเป็น bayesian glm ผลลัพธ์ที่ได้จะมีความแกร่งว่าการใช้ glm แบบปกติโดยเฉพาะกรณีขนาดตัวอย่างเล็ก ความแตกต่างอีกส่วนนึงที่น่าสนใจคือ `step_lencode_bayes()` จะให้ solution ที่แตกต่างกันไปในแต่ละครั้ง เนื่องจากการประมาณค่าพารามิเตอร์ใช้ Markov Chain Monte Carlo (MCMC) ที่อิงกับตัวเลขสุ่ม

- `step_lencode_mixed()` หลักการเดียวกับ glm แต่ใช้ mixed-effects model แทน เหมาะกับข้อมูลที่มีโครงสร้างแบบ hierarchical หรือ panel data 



```{r}
library(embed)
recipe(G3 ~ ., data = train) %>% 
  step_lencode_glm(Mjob, outcome = "G3") %>%  
  prep() %>% 
  juice()
```

- `step_woe()` ใช้ทำ weight of evidence (WOE) หรือ information value (IV) ในการแปลง factor ให้เป็นตัวเลข วิธีการนี้ใช้กับ binary-classification taskขั้นตอนของวิธีการนี้คือ

1. คำนวณค่า WOE ซึ่งมีค่าเท่ากับ log(odds) ของการเกิด outcome ที่สนใจจำแนกตาม level ของ factor ที่ต้องการแปลง หากใน level ไหนของ factor ไม่มี event หรือ non-event จะใช้เทคนิค Laplace Smoothing เพื่อเลี่ยงการหารด้วย 0

2. แทน factor level แต่ละตัวด้วย woe value


## 5. Transforming Numerical Variables

คือการแปลงคะแนนของตัวแปรเชิงปริมาณให้มีลักษณะการแจกแจงหรืออยู่ในรูปแบบที่เหมาะสำหรับการนำไปวิเคราะห์ด้วยโมเดลการเรียนรู้ของเครื่อง หรือในบางกรณีเป็นการสร้างตัวแปรใหม่เพื่อเพิ่มประสิทธิภาพการเรียนรู้ของโมเดล

```{r}
train  %>% summary()

recipe(G3 ~ . ,data= train) %>% 
  step_YeoJohnson(absences) %>% 
  prep() %>% 
  juice() %>% 
  ggplot(aes(x = absences))+
  geom_histogram()
```



```{r eval = F}

?step_log()
?step_sqrt()
?step_BoxCox()
?step_YeoJohnson()

?step_normalize() ## z-score = (x-mean)/sd
?step_range() ## normalization --> N = (x-min)/(max-min)
?step_scale() ## x/sd ---> ทำให้ variance  sd = 1
?step_center() ## centering x-mean
?step_percentile() ## แปลงข้อมูลเชิงปริมาณให้เป็นค่า percentile 
?step_mutate()


?step_poly()


?step_pca()
?step_pls()
```


Note: 

- `step_pls()` คือ Partial Least Squares (PLS) ซึ่งเป็นเทคนิค supervised dimensionality reduction และ feature extraction เพื่อสร้างตัวแปรใหม่ที่เป็น linear combination ของตัวแปรเดิมที่มีอยู่ (องค์ประกอบ) ที่มีความสัมพันธ์กับ outcome มากที่สุด 

จะใช้ฟังก์ชันนี้ได้จะต้องมี library-mixOmics ติดตั้งก่อน ซึ่ง library ดังกล่าวอยู่ใน Bioconductor server สามารถติดตั้งได้ดังนี้

```{r eval = F}
# install.packages("pak")
pak::pak("mixOmics")
```


```{r}
recipe(G3 ~ ., data = train) %>% 
  step_pls(all_numeric_predictors(), num_comp = 3, outcome = "G3") %>% 
  prep() %>% 
  juice()
```

- `step_ica()` เป็น feature extraction ในทำนองเดียวกับ pca เรียกชื่อเต็มว่า Independent Component Analysis (ICA) เทคนิคนี้จัดอยู่ในกลุ่ม unsupervised learning ในกลุ่ม dimensionality reduction หลักการคร่าว ๆ ของอัลกอริทึมนี้คือ สร้าง linear combination ที่เรียกว่า independent components การสร้าง ICs เหล่านี้จะมีวัตถุประสงค์คือพยายามให้แต่ละ ICs เป็นอิสระต่อกันมากที่สุด


## 6. Interaction Terms

การเพิ่ม interaction term ในโมเดลมีประโยชน์อย่างมากโดยเฉพาะใน linear model เพราะช่วยเพิ่มโอกาสการพบความสัมพันธ์แบบ non-linear ระหว่างตัวแปร ช่วยเพิ่มประสิทธิภาพการเรียนรู้ของโมเดล และยังช่วยให้สามารถอธิบายความสัมพันธ์ได้ลึกมากขึ้น

```{r eval = F}
?step_interact()
```


หลักการหา interaction terms สามารถทำได้หลายวิธีการ ตั้งแต่วิธีการที่อิงทฤษฎี (theory-driven) ไปจนถึงวิธีการที่ใช้ข้อมูลเป็นฐาน (data-driven)


1. theory-driven: ระบุ interaction term ด้วยทฤษฎี/ความรู้ความเข้าใจใน domain ของปัญหา เช่น การทำนายผลการเรียนของนักเรียน ปฏิสัมพันธ์ระหว่างเวลาที่ใช้ในการทบทวนบทเรียน กับ ความถี่ของการอภิปรายกับเพื่อนหรือครู อาจมีความสำคัญต่อการทำนายผลการเรียนดังกล่าวให้ดีขึ้น

วิธีการนี้โดยเฉพาะทางการศึกษาจำเป็นที่จะต้องมีทฤษฎีหรือเหตุผลที่แข็งแกร่ง ชัดเจน รองรับการสร้าง interaction term ดังกล่าว

2. ใช้หลักการระบุ interaction (Principle of Interaction Search ของ Wu and Hamada (2011)

concept หลักของหลักการนี้ได้แก่ 

- interaction hierarchy: interaction ที่มีระดับสูงกว่า (higher degree of interaction) มีโอกาสน้อยลงที่จะอธิบาย variation ในตัวแปรตาม

- effect sparsity: มีเพียงส่วนน้อยของ effects ที่เป็นไปได้ทั้งหมดเท่านั้นที่อธิบาย variation ในตัวแปรตามได้อย่างมีนัยสำคัญ

- effect heredity: interaction terms จะถูกพิจารณาต่อเมื่อ terms ที่มีระดับต่ำกว่า (preceding terms) ที่มีผลในการอธิบาย variation ในตัวแปรตามได้อย่างมีนัยสำคัญ

  - Strong Heredity: interaction term จะถูกนำมาพิจารณาต่อเมื่อ main effects ทั้งหมดของตัวแปรที่เกี่ยวข้องใน interaction นั้น ๆ สามารถอธิบาย variation ได้อย่างมีนัยสำคัญ ตัวอย่างเช่น interaction   จะถูกพิจารณาต่อเมื่อ main effect ของ  และ  มีนัยสำคัญทั้งคู่
  
  - Weak Heredity: interaction term จะถูกนำมาพิจารณาต่อเมื่อ main effect อย่างน้อยหนึ่งตัวของตัวแปรที่เกี่ยวข้องใน interaction นั้น ๆ สามารถอธิบาย variation ได้อย่างมีนัยสำคัญ
  
  
```{r}
df <- data.frame(
  x = c(1, 2, 3, 1.5, 2.5, 2),
  y = c(2, 2, 2, 1.6, 1.6, 1.6),  
  text = c("x1", "x2", "x3", "x1 × x2", "x1 × x3", "x2 × x3"),
  fill = c("blue", "white", "white", "lightblue", "lightblue", "pink"),
  order = c("first", "first", "first", "second", "second", "second") 
)

ggplot(df, aes(x, y)) +
  ## first order
  geom_tile(data = df[df$order == "first", ], aes(fill = fill), 
            color = "black", width = 0.6, height = 0.2) +
  # Second Order
  geom_tile(data = df[df$order == "second", ], aes(fill = fill), 
            color = "black", width = 0.4, height = 0.2) +
  geom_text(aes(label = text), size = 6,
            family = "ChulaCharasNew") +
  scale_fill_identity() +
  theme_void()+
  ggtitle("Principle of Interaction Search")
```
  


3. ใช้โมเดลที่จับ interaction โดยอัตโนมัติ เช่น tree-based model หรือ neural network โดยสามารถใช้ร่วมกับ XAI เช่น Partial Dependence Plot (PDP) หรือ SHAP value ซึ่งจะช่วยให้ผู้วิเคราะห์สามารถหาและแปลความหมาย interaction term ที่สำคัญ ได้โดยใช้ข้อมูลเป็นฐาน


4. อาจใช้ regularized regression เช่น LASSO regression เข้ามาช่วยเลือก interaction term ที่สำคัญได้ วิธีการนี้จะต้องใช้การสร้าง term interaction ทั้งหมดที่คิดว่าจะเป็นไปได้ก่อน จากนั้นใช้ penalized regression ช่วยเลือก
 

## 7. Handling Missing Data

ปัญหา missing value เป็นปัญหาที่พบบ่อยและอาจสร้างผลกระทบอย่างมากต่อโมเดลการเรียนรู้ของเครื่อง การจัดการ missing value มีหลายวิธีการ แต่ละวิธีการจะมีข้อดี ข้อจำกัดและเหมาะกับบริบทการใช้งานที่แตกต่างกัน เราอาจจำแนกบริบทการใช้งานได้เป็น 2 ลักษณะ

- การจัดการ missing ใน inferential stat

  - เน้นอธิบาย เน้นอนุมานไปยังประชากร/กลุ่มเป้าหมาย
  
  - พยายามรักษาความน่าเชื่อถือ/ความถูกต้องของการอนุมาน
  
  - เน้นวิธีการ impute missing ที่สามารถ capture ความไม่แน่นอนของการทดแทนค่าสูญหายให้ได้ เช่น MI
  
  - ปัจจัยสำคัญที่ใช้เลือกวิธีการ impute คือ กลไกการสูญหายของข้อมูล

- ในการสร้าง supevised learning model

  - เน้นสร้างโมเดลที่มีความแม่นยำ
  
  - การจัดการ missing มีความสำคัญเพียงในขั้นตอน preprocessing เพื่อสร้างโมเดลทำนายที่ดีที่สุด
  
  - เนื่องจากเน้นผลการทำนายที่ดี วิธีการที่ใช้จึงเน้นการทดแทนค่าสูญหายที่แม่นยำและทำให้โมเดลมีค่าทำนายที่แม่นยำที่สุด 
  
  - ไม่สนใจเรื่องการอนุมานหรืออธิบายความสัมพันธ์ในประชากร


```{r eval = F}
?step_naomit() ## listwise deletion
?step_impute_mean()
?step_impute_median()
?step_impute_mode()
?step_impute_linear()
?step_impute_knn()
?step_impute_bag()
```

```{r}
complete_data <- read_csv("exam.csv")
```

ลองสร้างการสูญหายใช้ตัวแปร `learning_performance`

```{r}
library(patchwork)
set.seed(123)
missing_data <- complete_data %>% 
  ## สร้างตัวแปรใหม่ที่สูญหายแบบ MAR โดยมีความสัมพันธ์กับ ach และ engage
  mutate(learning_performance_mar = case_when(
     ach < 40 | engage == "moderate engage"  ~ ifelse(runif(387,0,1)<0.8,NA, learning_performance) ,
    .default = learning_performance
  )
  )
p1<-missing_data %>% 
  pivot_longer(cols=starts_with("learning")) %>% 
  ggplot(aes(x=value))+
  geom_histogram(aes(fill = name))+
  labs(fill = "")+
  theme(legend.position = "none",
        legend.direction = "horizontal")
p2<-missing_data%>% 
  pivot_longer(cols=starts_with("learning")) %>% 
  ggplot(aes(x = value, y=name))+
  geom_boxplot(aes(fill = name))+
  ylab("")+
    theme(legend.position = "none")

p1+p2
```

```{r}
split_missing <- initial_split(missing_data, prop = 0.8, strata = ach)
train_missing <- training(split_missing)
test_missing <- testing(split_missing)
```



```{r}
recipe(ach ~ ., data = train_missing) %>% 
  step_rm(learning_performance) %>% 
  step_impute_mean(learning_performance_mar) %>%
  prep() %>% 
  juice() %>% 
  ggplot(aes(x = learning_performance_mar, y= ach))+
  geom_point()
```

```{r}
recipe(ach ~ ., data = train_missing) %>% 
  step_rm(learning_performance) %>% 
  step_impute_knn(learning_performance_mar) %>%
  prep() %>% 
  juice() %>% 
  ggplot(aes(x = learning_performance_mar, y= ach))+
  geom_point()
```

## 8. แนวทางการทำ feature engineering ที่เหมาะสม

- ป้องกัน data leakage

  - แยก train/test เสมอ
  
  - เก็บ test data ไว้สำหรับ final model เท่านั้น
  
  - ใช้ Cross-validation สำหรับปรับแต่ง hyperparameter ที่เหมาะสม
  
- ลำดับขั้นของการทำ feauture engineering ที่แตกต่างกันมีผลต่อประสิทธิภาพของโมเดล  พิจารณาการดำเนินงานต่อไปนี้ `step_interact()`, `step_lencode_glm()`, `step_normalized()` และ `step_impute_knn()` ควรดำเนินการลำดับไหนก่อนหลัง

- ตรวจสอบผลลัพธ์ด้วยวิธีการที่หลากหลาย

  - Statistical Approached
  
  - Data Visualization
  

