---
title: "Explainable AI"
format: html
editor: visual
---

## Outline

-   แนวคิดพื้นฐาน

-   เทคนิค XAI

-   กรณีศึกษาและการประยุกต์ใช้ XAI

## 1. แนวคิดพื้นฐาน

โมเดลการเรียนรู้ของเครื่องที่กล่าวมาในรายวิชานี้สามารถนำไปประยุกต์ใช้เพื่อตัดสินใจทางการศึกษาในสถานการณ์ที่หลากหลายโดยเฉพาะการให้ insight เพื่อทำนายหรือจำแนกข้อมูล อย่างไรก็ตามหลายกรณีการใช้ประโยชน์จากโมเดลดังกล่าวเพียงแค่นี้อาจไม่เพียงพอ โดยเฉพาะในกรณีที่ต้องการให้คำอธิบายหรือเหตุผลที่จะใช้สนับสนุนหรือสร้างความมั่นใจในการตัดสินใจ คำอธิบายดังกล่าวจะช่วยให้เกิด

1.  ความไว้วางใจ (trust)

2.  ความเป็นธรรม (fairness)

3.  ความรับผิดชอบ (accountability)

4.  การปรับปรุง (improvement)

5.  ความสอดคคล้องกับข้อกำหนดด้านกฎหมายหรือระเบียบต่าง ๆ

อย่างไรก็ตามโมเดลการเรียนรู้ของเครื่องเกือบทุกตัวมีปัญหาเรื่องการอธิบายคำตอบหรือการตัดสินใจของโมเดลทั้งนี้เป็นเพราะโมเดลที่มีประสิทธิภาพสูงมาก ๆ มากมีลักษณะการทำงานที่ซับซ้อนเข้าไปสังเกตหรือทำความเข้าใจโดยตรงได้ยาก นักวิทยาการข้อมูลเรียกโมเดลลักษณะดังกล่าวว่าเป็น black box จากข้อจำกัดดังกล่าวทำให้นักวิทยาการข้อมูลต้องการ insight เพิ่มเติมสำหรับอธิบายการทำงานหรือการตัดสินใจของโมเดลนี้ การแก้ปัญหาคือการพัฒนาเทคนิคที่เรียกว่า explainable AI (XAI) ขึ้นมา

## 2. เทคนิค XAI

เป็นแนวคิดและชุดของเทคนิคที่มุ่งเน้นทำให้ระบบปัญญาประดิษฐ์และการเรียนรู้ของเครื่องมีความโปร่งใสและสามารถอธิบายได้ว่าเหตุใดจึงทำนายหรือตัดสินใตเช่นนั้น

-   สามารถอธิบายเหตุผลการตัดสินใจให้กับผู้ใช้

-   ระบุจุดแข็งและจุดอ่อนของตัวเอง

-   สื่อสารให้ผู้ใช้เข้าใจถึงแนวโน้มพฤติกรรมในอนาคตของระบบ

การเรียนรู้เรื่อง XAI ผู้เรียนควรทราบความแตกต่างระหว่าง interpretability และ explainability

-   ความสามารถด้านการแปลความ (interpretability) -- ระดับที่มนุษย์สามารถเข้าใจสาเหตุของการตัดสินใจของโมเดล หรือความสามารถในการเข้าใจกลไกภายในของโมเดล เช่น การเข้าใจว่าตัวแปรอิสระแต่ละตัวมีผลต่อตัวแปรตามอย่างไรผ่านค่า regression coefficients ใน linear regression model

-   ความสามารถด้านการอธิบาย (explainability)-- ความสามารถในการอธิบายหรือนำเสนอเหตุผลที่โมเดลตัดสินใจในรูปแบบที่มนุษย์เข้าใจได้ โดยไม่จำเป็นต้องเข้าใจกลไกภายในทั้งหมด เช่นการใช้ LIME หรือ SHAP เพื่อสร้างคำอธิบายจากโมเดลที่ซับซ้อน

## 3. ประเภทของ XAI

-   Global vs Local Explanations

-   Model-specific vs Model-agnostic Explanations

### 3.1 Global vs Local Explanations

Global explanations คือ คำอธิบายพฤติกรรมโดยรวมของโมเดลทั้งหมด ตอบคำถามในลักษณะว่า โมเดลนี้ให้ความสำคัญกับปัจจัยหรือตัวแปรอิสระใดบ้าง เทคนิคที่อยู่ในกลุ่มนี้ เช่น Feature Importance, Partial Dependence Plot

Local Explanations คือ คำอธิบายการทำนายเฉพาะกรณี (single prediction) ตอบคำถามว่า ทำไมโมเดลจึงให้ผลทำนายของเคสนี้ นักเรียนคนนี้เป็นแบบนี้ เทคนิคที่อยู่ในกลุ่มนี้ เช่น LIME, SHAP

### 3.2 Model-specific vs Model-agnostic Explanations

Model-specific explanations คือ คำอธิบายที่สร้างหรือออกแบบเฉพาะสำหรับโมเดลประเภทใดประเภทหนึ่ง เช่น การอ่านค่าสัมประสิทธิ์การถดถอยจาก linear regression การแปลผล odd ratio ใน logistic regression การดู split rule ใน decision tree

Model-agnostic explanations คือ คำอธิบายที่สามารถใช้กับโมเดลทุกประเภท ไม่จำเป็นต้องรู้ว่าโมเดลเป็นประเภทใด เช่น VIP, LIME, SHAP,...

## 4. เทคนิค XAI

### 4.1 Feature importance (Variable Importance)

Feature Importance เป็นเทคนิคพื้นฐานในการทำความเข้าใจว่าตัวแปรใดมีความสำคัญต่อการทำนายของโมเดล โดยวัดว่าตัวแปรแต่ละตัวมีผลกระทบมากน้อยเพียงใดต่อผลลัพธ์ ซึ่งจัดอยู่ในกลุ่ม global explanations

หลักการทำงานของ feature importance มีหลายวิธีในการคำนวณ ขึ้นอยู่กับว่าผู้วิเคราะห์ใช้โมเดลอะไรในการทำนาย ทำให้การจัดกลุ่มในอีกประเภท feature importance สามารถเป็นไปได้ทั้ง model-specific และ model-agnostic

1.  Model-specific methods จะใช้วิธีการคำนวณเฉพาะของแต่ละโมเดลเป็น feature importance เช่น linear regression หรือ glm อาจใช้ regression coefficients ในการคำนวณ หรือ tree-based model อาจใช้ impurity

2.  Model-agnostic methods จะใช้วิธีการคำนวณที่ไม่ขึ้นกับโมเดล

-   Variance-based variable importance -- ใช้ partial dependence plot (pdp) เป็นข้อมูลในการคำนวณตัวชี้วัด vip ประเภทนี้ (จะกล่าว pdp ภายหลัง) โดยที่ pdp เป็นเทคนิคในกลุ่มทัศนภาพข้อมูลที่ใช้แสดงความสัมพันธ์ระหว่างตัวแปรอิสระกับค่าทำนายของตัวแปรตามที่ได้จาก ML โดยแผนภาพจะแสดงให้เห็นว่า เมื่อ x เปลี่ยนแปลงไป ผลการทำนายจะเปลี่ยนแปลงไปอย่างไร เมื่อควบคุมให้ตัวแปรอิสระอื่นคงที่

VIP ภายใต้แนวคิดของ Variance-based variable importance นี้จะสร้างตัวชี้วัดจาก pdp เพื่อวัดลักษณะการเปลี่ยนแปลงใน pdp กล่าวคือ pdp ที่มีการเปลี่ยนแปลงมากแสดงว่าตัวแปรอิสระนั้น ๆ มีความสำคัญมาก ดังนั้นความแปรปรวนของค่าทำนายในแต่ละ pdp จึงเป็นตัวชี้วัดสำคัญในการบอกความสำคัญของตัวแปรอิสระแต่ละตัว

-   Permutation Feature Importance -- หลักการของวิธีการนี้คือการวัดว่าประสิทธิภาพของโมเดลเปลี่ยนแปลงไปอย่างไรเมื่อสลับค่าของตัวแปรอิสระแต่ละตัวแบบสุ่ม

    -   fit model จากนั้นคำนวณประสิทธิภาพของโมเดลบนชุดข้อมูลทดสอบ

    -   สลับค่าของตัวแปรอิสระหนึ่ง ๆ แบบสุ่ม (permute)

    -   คำนวณประสิทธิภาพของโมเดลที่สลับค่าตัวแปรนี้อีกครั้ง

    -   ความสำคัญของตัวแปร = การลดลงของประสิทธิภาพ

    -   ทำซ้ำกับทุกตัวแปรอิสระ

-   SHAP Values -- ค่านี้มาจากแนวคิดในทฤษฎีเกม โดยกำหนดให้โมเดลเป็นเหมือนผลกำไร ที่ได้จากการทำงานร่วมกันของตัวแปรอิสระทั้งหมด (เรียกว่าผู้เล่น)

    -   ปัญหาคือเราต้องการทราบว่าผู้เล่นแต่ละคนมีส่วนช่วยให้โมเดลทำนายได้อย่างแม่นนำแค่ไหน

        -   วิธีการวัดความสำคัญของตัวแปรดังกล่าว Shapley เสนอว่า ให้ดำเนินการพิจารณาว่าเมื่อใส่ตัวแปรอิสระดังกล่าวเข้าไปในชุดของตัวแปรอื่น ๆ แล้ว ตัวแปรอิสระตัวนั้นจะช่วยให้โมเดลเรียนรู้ได้ดีขึ้นมากน้อยแค่ไหน ระดับความสำคัญนี้หากจะเรียกให้ถูกอาจเรียกว่า ระดับการมีส่วนร่วมในการทำนายของตัวแปรอิสระ คำนวณจากค่าเฉลี่ยการมีส่วนร่วมนี้จากทุกรูปแบบที่มีการใส่ตัวแปร A เข้าไป

            |                        |               |
            |------------------------|---------------|
            | Subset                 | ประสิทธิภาพที่ดีขึ้น |
            | {} -\> X1              | 0.4           |
            | {X2} -\> {X1,X2}       | 0.1           |
            | {X3} -\> {X1, X3}      | 0.2           |
            | {X2,X3} -\> {X1,X2,X3} | 0.3           |

        -   ค่า Shapley value ของ A จะคำนวณจากค่าเฉลี่ยของประสิทธิภาพที่ดีขึ้นข้างต้น

### 4.2 เตรียมโมเดล

ในทางปฏิบัติเราสามารถคำนวณ feature importance ผ่าน library-vip ดังนี้

```{r}
## install.packages("vip")
library(vip)
library(tidyverse)
library(tidymodels)
library(future)
library(finetune)
```

```{r}
data <- read_delim("/Users/choat/Documents/GitHub/datakruroo.github.io/MachineLearning/week04/student-mat.csv",
delim = ";")
data <- data |> 
    mutate(G3 = ifelse(G3 >= 10, "pass", "fail")) |> 
    mutate(G3 = fct_relevel(G3, "pass","fail"))
### แบ่งชุดข้อมูล 80:20
set.seed(123)
split <- initial_split(data, prop = 0.8, strata = "G3")
train_data <- training(split)
test_data <- testing(split)
```

```{r}
## สร้าง data preprocessing
rec_glm <- recipe(G3 ~. , data = train_data) |> 
    step_mutate_at(school, sex, address, famsize, Pstatus, 
                 Medu, Fedu, Mjob, Fjob, reason, guardian,
                 traveltime, studytime, schoolsup,
                 famsup, paid, activities, nursery, higher,internet,
                 romantic, famrel, freetime, goout, Dalc, Walc, health, 
                 fn = factor) |> 
    step_novel(all_nominal_predictors()) |> 
    step_dummy(all_nominal_predictors()) |> 
    step_zv(all_predictors()) %>% 
    step_nzv(all_predictors()) %>% 
    step_normalize(all_numeric_predictors())

rec_svm <- recipe(G3 ~. , data = train_data) |> 
    step_mutate_at(school, sex, address, famsize, Pstatus, 
                 Medu, Fedu, Mjob, Fjob, reason, guardian,
                 traveltime, studytime, schoolsup,
                 famsup, paid, activities, nursery, higher,internet,
                 romantic, famrel, freetime, goout, Dalc, Walc, health, 
                 fn = factor) |> 
    step_novel(all_nominal_predictors()) %>% 
    step_dummy(all_nominal_predictors()) %>% 
    step_zv(all_predictors()) %>% 
    step_nzv(all_predictors()) %>% 
    step_normalize(all_predictors()) 


## ระบุโมเดล
glm_spec <- logistic_reg(penalty = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("glmnet")

svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune("sigma")) |> 
    set_mode("classification") |> 
    set_engine("kernlab")

## สร้าง workflow
wf_glm <- workflow() |> 
    add_recipe(rec_glm) |> 
    add_model(glm_spec)

wf_svm <- workflow() |> 
    add_recipe(rec_svm) |> 
    add_model(svm_spec)
```

```{r}
## เตรียมการทำ parallel processing
plan(multisession, workers = 12)

## สร้าง validation data และ performance metrics
set.seed(123)
myfolds <- vfold_cv(train_data , v=5, repeats = 3, strata = G3)
my_metrics <- metric_set(f_meas, sens, spec, precision, accuracy, roc_auc)
```

```{r}
start <- Sys.time()
test_race_glm <- wf_glm %>% 
  tune_race_anova(
    resamples = myfolds,
    metrics = my_metrics,
    grid = 30,
    control = control_race(
        burn_in = 3,
        verbose = FALSE,
        verbose_elim = TRUE,
        save_pred = TRUE, 
        save_workflow = TRUE,
        parallel_over = "everything")
    )

test_race_svm <- wf_svm |> 
    tune_race_anova(
    resamples = myfolds,
    metrics = my_metrics,
    grid = 30,
    control = control_race(
        burn_in = 3,
        verbose = FALSE,
        verbose_elim = TRUE,
        save_pred = TRUE, 
        save_workflow = TRUE,
        parallel_over = "everything")
    )
use_race <- Sys.time() - start 
```

```{r}
test_race_glm %>% collect_metrics()
test_race_svm %>% collect_metrics()
```

```{r}
final_glm <- wf_glm %>% 
  finalize_workflow(test_race_glm %>% select_best(metric = "f_meas")) %>% 
  fit(data = train_data)

final_svm <- wf_svm %>% 
  finalize_workflow(test_race_svm %>% select_best(metric = "f_meas")) %>% 
  fit(data = train_data)
```

### 4.3 Global Explanation using model-specific feature importance

```{r}
library(vip)
vi(final_glm) %>% 
  filter(Importance > 5) %>% 
  ggplot(aes(x = Importance, y= reorder(Variable, Importance)))+
  geom_col(aes(fill = Sign))+
  ylab("")+
  xlab("\n Importance")+
  ggtitle("VIP plot")
```

```{r}
vip(final_glm, num_features = 5)
```

### 4.4 Global Explainations using DALEX

library-DALEX รองรับการคำนวณ feature importance ในกลุ่ม model-agnotic ด้วยวิธีการ permutation และ SHAP values

```{r}
library(DALEX)

## สร้าง predict function เพื่อสื่อสารว่าโมเดลของเราทำนายอะไร และให้อะไรกลับมา
## ลองก่อน

predict(final_glm, new_data = train_data, type = "prob")

predict(final_glm, new_data = train_data, type = "prob") %>% pull(1)

## เขียนจริง
predict_function <- function(model, newdata) {
  predict(model, new_data = newdata, type = "prob") %>% pull(1)
}



## สร้าง explainer
explainer_glm <- explain(
  model = final_glm ,
  data = train_data,
  y = ifelse(train_data$G3 == "pass", 1, 0),
  label = "glmnet",
  predict_function = predict_function,
  type = "classification"
)
## สร้าง explainer ของ svm
explainer_svm <- explain(
  model = final_svm,
  data = train_data,
  y = ifelse(train_data$G3 == "pass", 1, 0),
  label = "svm",
  predict_function = predict_function,
  type = "classification"
)
```

#### 4.4.1 คำนวณ model performance

ดำเนินการสร้าง VIP เพื่ออธิบายความสำคัญของตัวแปรอิสระภายในโมเดล

```{r}
explainer_glm %>%  model_performance()
glm_performance <- model_performance(explainer_glm) 
glm_performance %>% plot()
```

```{r}
svm_performance <- model_performance(explainer_svm)
svm_performance %>% plot()
```

```{r}
plot(glm_performance, svm_performance)
```

ใน DALEX มีฟังก์ชันสำคัญสำหรับอธิบายโมเดลดังนี้

-   `model_performance()` ประเมินประสิทธิภาพของโมเดลโดยรวม

-   `model_parts()` ตัวแปรไหนมีผล/มีความสำคัญต่อการทำนายบ้าง อย่างไร

-   `predict_parts()` หน่วยข้อมูลหรือนักเรียนคนนี้ได้ผลการทำนายแบบนี้เพราะอะไร

-   `model_profile()` และ `predict_profile()` ถ้าเปลี่ยนค่าของตัวแปรอิสระแล้วจะเกิดอะไรขึ้น

#### 4.4.2 คำนวณ feature importance (global explanations) ด้วย DALEX

การวัดความสำคัญของตัวแปรด้วย DALEX จะใช้เทคนิคที่เรียกว่า Permutation Feature Importance ซึ่งจัดอยู่ในกลุ่ม model-agnostic และ global explanations หลักการเหมือนที่กล่าวไปแล้ว

```{r}
glm_parts <- model_parts(explainer_glm) ## global XAI
glm_parts %>% plot()
glm_parts %>% data.frame() %>% 
  filter(variable != "_full_model_" & variable != "_baseline_") %>% 
  ggplot(aes(x = dropout_loss, y= reorder(variable, dropout_loss)))+
  geom_boxplot()
```

```{r}
# สร้าง custom loss function แบบ F1 (f_meas)
loss_f1 <- function(observed, predicted) {
  # แปลง prob เป็น class
  predicted_class <- factor(ifelse(predicted >= 0.5, "pass", "fail"), levels = c("pass", "fail"))
  observed_class <- factor(ifelse(observed == 1, "pass", "fail"), levels = c("pass", "fail"))
  
  f_meas_vec(truth = observed_class, estimate = predicted_class)
}

attr(loss_f1, "loss_accuracy") <- "F1 Score"

test <- model_parts(
  explainer_glm,
  loss_function = loss_f1
)
test %>% plot()
```

#### 4.4.3 การคำนวณ feature importance ด้วย SHAP values

โดยปกติ shapley ถูกพัฒนาเพื่อทำ local explanations แต่ก็สามารถใช้ในลักษณะ global explanations ได้ด้วย

```{r}
shap_values <- predict_parts(
  explainer_glm,
  type = "shap",
  new_observation = train_data,
  B = 20
)

shap_list <- map(.x = 1:5, 
    .f = ~ 
  predict_parts(
    explainer_glm,
    new_observation = train_data[.x,],
    type = "shap",
    B = 5
  ))

shap_list[[1]] %>% dim()

shap_list %>% 
  bind_rows() %>% 
  data.frame() %>% 
  #mutate(variable = str_remove(variable, "=.*$")) %>% 
  group_by(variable_name) %>%
  summarise(mean_abs_shap = mean(abs(contribution))) %>% 
  filter(variable_name != "G3 ") %>% 
  ggplot(aes(x = mean_abs_shap, y = reorder(variable_name, mean_abs_shap)))+
  geom_col()
```

### 4.5 Partial Dependence Plot (PDP)

-   Partial Dependence Plot (PDP) เป็นเทคนิคการสร้างแผนภาพที่แสดงว่า เมื่อเปลี่ยนค่าของตัวแปรอิสระตัวใดตัวหนึ่ง ค่า prediction ของโมเดลมีแนวโน้มจะเปลี่ยนแปลงไปอย่างไร เมื่อกำหนดให้ตัวแปรอิสระอื่น ๆ คงที่

-   การวิเคราะห์่ส่วนนี้เป็นการสร้างคำอธิบายแบบ What-if analysis ประเภทหนึ่ง

```{r}
model_profile(explainer_glm) %>% plot()
```

```{r}
# 1. สร้าง model_profile สำหรับ GLM
pdp_glm <- model_profile(
  explainer = explainer_glm,
  variables = c("G2")
)
pdp_glm  %>% plot()

# 2. สร้าง model_profile สำหรับ SVM
pdp_svm <- model_profile(
  explainer = explainer_svm,
  variables = c("G2")
)
plot(pdp_svm)

# 3. วาด plot เปรียบเทียบ
plot(pdp_glm, pdp_svm)
```

### 4.6 Local Explainer

แม้เราจะเข้าใจว่าโดยรวมแล้วโมเดลให้ความสำคัญกับตัวแปรใดบ้าง (global explanation) แต่ในหลายกรณีการตัดสินใจเฉพาะบุคคล เช่น นักเรียนแต่ละคน ได้รับการทำนายให้สอบผ่าน/ไม่ผ่าน เราอาจต้องการทราบว่า **ทำไมโมเดลจึงให้ผลทำนายเช่นนั้นกับเคสนี้**

เทคนิคในกลุ่ม Local Explanation มีเป้าหมายเพื่อตอบคำถามนี้ โดยใช้ข้อมูลของแต่ละ observation ในการอธิบายผลลัพธ์ของโมเดล

#### 4.6.1 Breakdown plot

```{r}
test_data
test_data[1,] %>% select(-G3) %>% 
  glimpse()
predict(final_glm,
        new_data = test_data[1,])
predict(final_glm,
        new_data = test_data[1,],
        type = "prob")
```

```{r}
## เลือก observation ผลการทำนายของนักเรียนคนนี้เป็นอย่างไร
test_data[1,]
predict(final_glm, new_data = test_data[1,], type = "prob") %>% pull(1)
```

```{r}
##  ผลการทำนายของนักเรียนคนนี้เป็นอย่างไร?
test_data[50,]
predict(final_glm, new_data = test_data[50,], type = "prob") %>% pull(1)
```

ทำไมนักเรียนทั้งสองคนถึงได้ผลการทำนายที่แตกต่างกัน?

-   **Break-down plot**

แนวคิดพื้นฐานของ BD plots คือการระบุว่าตัวแปรอิสระแต่ละตัวมีส่วนเพิ่มหรือลดค่าทำนาย/ค่าความน่าจะเป็นของโมเดลมากน้อยแค่ไหน เราเรียกค่านี้ว่า contribution หลักการคำนวณคือการเพิ่มตัวแปรที่สนใจเข้าไปในโมเดล โดยกำหนดให้ตัวแปรอิสระตัวอื่น ๆ คงที่ จากนั้นวัดว่าค่าทำนายของโมเดลมีการเปลี่ยนแปลงไปอย่างไร

note: breakdown plot ของ DALEX จะมีการเรียงลำดับตัวแปรอิสระตามความสำคัญให้โดยอัตโนมัติ โดยมีหลักการดังนี้

1.  เริ่มจาก baseline ก่อน (null model)

2.  วนลูปเพื่อตรวจสอบว่าการ fixed ค่าตัวแปรอิสระไหน ที่ทำให้ค่าทำนายเปลี่ยนแปลงไปจาก null model มากทีี่สุด

3.  วนลูปเพื่อเลือก feature ตัวถัดไปเรื่อย ๆ จนครบ

ลองพิจารณานักเรียนที่ถูกทำนายว่าจะสอบตก ตัวแปร G2 มีผลมากน้อยแค่ไหนต่อการสอบตกของนักเรียนคนนี้

```{r}
## student1
test_data[1,] %>% pull(G2)
```

```{r}
bd_g2 <- test_data
bd_g2$G2 <- test_data[1,] %>% pull(G2)

all_data <- predict(final_glm, 
                    new_data = test_data, 
                    type = "prob") %>% pull(1) %>% mean()

g2_6 <- predict(final_glm, 
                new_data = bd_g2, 
                type = "prob") %>% pull(1) %>% mean()

g2_6 - all_data ## contribution
```

การดำเนินการข้างต้นสามารถทำได้ด้วย `predict_parts()` ของ DALEX ดังนี้

```{r}
names(test_data)
breakdown_student1 <- explainer_glm %>% 
  predict_parts(
    new_observation = test_data[1,],
    type = "break_down"
  )
breakdown_student1 %>% data.frame()
```

```{r}
breakdown_student1 %>% plot()
```

ลองพิจารณา BD-plot ของนักเรียนอีกคนนึงที่มีความน่าจะเป็นที่จะผ่านสูง

```{r}
test_data[50,]
breakdown_student2 <- explainer_glm %>% 
  predict_parts(
    new_observation = test_data[50,],
    type = "break_down"
  )
breakdown_student2 %>% plot()
```

```{r}
bd_list <- map(.x = 1:10, 
    .f = ~ 
  predict_parts(
    explainer_glm,
    new_observation = train_data[.x,],
    type = "break_down"
  ))

bd_list %>% bind_rows() %>% data.frame() %>% 
  mutate(student_id = rep(1:10, each = 35)) %>% 
  select(student_id, variable_name, variable_value, contribution, sign) %>% 
  filter(variable_name == "G2") %>% 
  ggplot(aes(x= contribution, y=as.numeric(variable_value)))+
  geom_point()+
  xlab("contribution")+
  ylab("G2: midterm score")
```

ในทางปฏิบัติเราสามารถกำหนดลำดับของการคำนวณ contribution ใน BD-plot ได้

```{r fig.width=9, fig.height=4}
custom_order <- names(test_data)

explainer_glm %>% 
  predict_parts(
    new_observation = test_data[1,],
    type = "break_down"
  )

breakdown_student1_order2

breakdown_student1_order2 <- explainer_glm %>% 
  predict_parts(
    new_observation = test_data[1,],
    type = "break_down",
    order = custom_order
  )
library(patchwork)

breakdown_student1 %>% plot() + ## default order
  breakdown_student1_order2 %>% plot() + ## custom order
  plot_layout(ncol = 2)
```

```{r}
breakdown_student2_order2 <- explainer_glm %>% 
  predict_parts(
    new_observation = test_data[50,],
    type = "break_down",
    order = custom_order
  )
```

```{r fig.width = 12, fig.height = 8}
library(patchwork)
breakdown_student1 %>% plot() + 
  breakdown_student1_order2 %>% plot() +
  breakdown_student2 %>% plot() +
  breakdown_student2_order2 %>% plot() +
  plot_layout(ncol = 2)
```

ตัวอย่างนี้แสดงให้เห็นว่าลำดับของการคำนวณ contribution มีผลให้ค่า contribution มีความแตกต่างกันได้ และความแตกต่างดังกล่าวอาจมีมากโดยเฉพาะในกรณีที่มีอิทธิพลปฏิสัมพันธ์ในตัวแปร ลองพิจารณาตัวอย่างด้านล่างจะเห็นว่าในโมเดลที่จับอิทธิพลปฏิสัมพันธ์ได้ ค่า contribution จะมีการเปลี่ยนแปลงอย่างมาก

```{r fig.width = 12, fig.height = 8}
breakdown_student1_svm1 <- explainer_svm %>% 
  predict_parts(
    new_observation = test_data[1,],
    type = "break_down",
  )

breakdown_student1_svm2 <- explainer_svm %>% 
  predict_parts(
    new_observation = test_data[1,],
    type = "break_down",
    order = custom_order
  )
breakdown_student1_svm1 %>% plot() +
  breakdown_student1_svm2 %>% plot() +
  plot_layout(ncol = 2)
```

#### 4.6.2 Shapley values

เพื่อแก้ปัญหาความลำเอียงของ BD-plot จึงมีการพัฒนา Shapley values ที่อิงกับหลักทฤษฎีเกมแบบร่วมมือ (cooperative game theory) highlight สำคัญของ Shapley values คือ

> Feature แต่ละตัวมีส่วนช่วยเพิ่ม/ลดค่าทำนายของโมเดลมากน้อยแค่ไหน เมื่อกำหนดให้ตัวแปรอิสระอื่นคงที่ โดยจะพิจารณาทุกลำดับที่เป็นไปได้ของการเพิ่ม feature นั้นเข้าไปในโมเดล

-   พิจารณาทุก subset ที่เป็นไปได้ แสดงว่าไม่ได้พิจารณาเฉพาะลำดับใน full model แบบ BD-plot แต่เพียงอย่างเดียว

-   contribution ของ Shap เป็นการพิจารณาว่าเมื่อเพิ่มตัวแปรที่สนใจเข้าไปในโมเดล ค่าทำนายของเคสนั้น ๆ จะมีการเปลี่ยนแปลงโดยเฉลี่ยในทุกความเป็นไปได้อย่างไร

แนวคิดดังกล่าวทำให้ Shap มีจุดเด่นในด้าน ความยุติธรรมมีความลำเอียงต่ำ รองรับ interaction เพราะพิจารณาในทุกกรณีทุกความเป็นไปได้ และอธิบายได้ทั้งในระดับ global และ local

อย่างไรก็ตาม Shap มีข้อเสียคือการคำนวณที่ใช้เวลานานมาก โดยเฉพาะในกรณีที่มีตัวแปรอิสระจำนวนมาก ๆ

```{r}
shap_student1_glm <- explainer_glm %>% 
  predict_parts(
    new_observation = test_data[1,],
    type = "shap",
    B = 20
  )
shap_student1_glm 
```

```{r}
shap_student1_glm %>% plot()
```

```{r}
lm(G2 ~ G1, data= test_data) %>% summary()
```

ลองพิจารณา contribution ของ shapley value ที่ได้จาก svm (นักเรียนคนที่ 1)

```{r}
shap_student1_svm <- explainer_svm %>% 
  predict_parts(
    new_observation = test_data[1,],
    type = "shap",
    B = 20
  )

shap_student1_svm %>% plot()
```

```{r}
n <- 10
shape_student_all <- map(.x = 1:n, 
                         .f = ~explainer_glm %>% 
  predict_parts(
    new_observation = test_data[.x,],
    type = "shap",
    B = 5
  ))

shape_student_all %>% bind_rows() %>% data.frame() %>%
  mutate(student_id = rep(1:n, 6, each = 33), .before = 1) %>% 
  select(student_id, variable_name, variable_value, contribution) %>% 
  filter(variable_name %in% c("G2","G1","age")) %>% 
  
  ggplot(aes(x = contribution, y=variable_name))+
  geom_boxplot(width = 0.2)+
  geom_jitter(
    data = . %>% filter(variable_name %in% c("G2","G1")),
    aes(col = as.numeric(variable_value)>10))+
  geom_jitter(
    data = . %>% filter(variable_name == "age"),
    aes(col = as.numeric(variable_value) > 16))+
  theme_light()+
  theme(legend.position = "none")
```

```{r}
top_features <- shape_student_all %>% bind_rows() %>% data.frame() %>%
  mutate(student_id = rep(1:n, 6, each =33), .before = 1) %>% 
  select(student_id, variable_name, variable_value, contribution, sign) %>% 
  group_by(variable_name) %>%
summarise(med_abs_shap = median(abs(contribution), na.rm = TRUE)) %>%
  arrange(desc(med_abs_shap)) %>%
  slice_head(n = 5) %>%
  pull(variable_name)


## install.packages("ggbeeswarm")
library(ggbeeswarm)

shape_student_all %>% bind_rows() %>% data.frame() %>%
  mutate(student_id = rep(1:n, 33, each = 6), .before = 1) %>% 
  select(student_id, variable_name, variable_value, contribution) %>% 
  filter(variable_name %in% top_features) %>% 
  ggplot(aes(x = contribution, y=reorder(variable_name, 
                                         abs(contribution), 
                                         FUN = median)))+
  geom_quasirandom(col = "steelblue")+
  stat_summary(fun = median, geom = "point", shape = 21, size = 3, fill = "orange") +
  labs(title = "SHAP Summary Plot (DALEX)", x = "SHAP value", y = "Feature") +
  theme_minimal()

```

##### Local dependency plot using SHAP

การที่ได้ค่า Shapley value ของแต่ละหน่วยข้อมูล เราสามารถนำไปสร้าง partial dependency plot ได้ด้วย

```{r}
shape_student_all %>% bind_rows() %>% data.frame() %>%
  mutate(student_id = rep(1:n, 6, each = 33), .before = 1) %>% 
  select(student_id, variable_name, variable_value, contribution, sign) %>% 
  filter(variable_name %in% c("G2")) %>%
  mutate(
    variable_value = as.numeric(variable_value),
    contribution = as.numeric(contribution)) %>%
  ggplot(aes(x = variable_value, y = contribution))+
  geom_point(aes(col = variable_value))+
  geom_smooth(col = "black", se = F)+
  theme_light()+
  scale_color_gradient2(low = "maroon", mid = "orange", high = "steelblue",
                        midpoint = 10)+
  ggtitle("Local dependency plot using SHAP")+
  xlab("\n G2: Midterm Score")
```

#### 4.6.3 Local Interpretable Model-agnostic Explanations (LIME)

BD-plot และ Shapley ถูกพัฒนาขึ้นโดยเหมาะสำหรับโมเดลที่มี feature จำนวนไม่มากนัก อย่างไรก็ตามในกรณีที่โมเดลทำนายมีจำนวน feature จำนวนมาก

อย่างไรก็ตามเมื่อจำนวน feature มีจำนวนมาก ๆ วิธีการแบบ Shap หรือ BD-plot จะทำงานค่อนข้างช้า นอกจากนี้แม้ว่าตัวแปรอิสระบางตัวจะไม่มีผลในการทำนาย y เลย แต่ก็อาจจะถูกระบุว่ามีผลต่อค่าทำนายอยู่บ้างเล็ก ๆ น้อย ๆ อยู่ดี

ข้อจำกัดข้างต้นจะเป็ํนปัญหามาก ๆ โดยเฉพาะในกรณีที่เรามีตัวแปรอิสระหลายร้อย หลายพันตัวในโมเดล

สถานการณ์ดังกล่าว LIME จะเป็นวิธีการที่เหมาะสมกว่า แนวคิดเบื้องหลังของ LIME มีดังนี้

-   วัตถุประสงค์คือ เราต้องการเข้าใจปัจจัยที่ทำให้โมเดลสร้างค่าทำนายแบบนี้มาให้เรา ณ บริเวณรอบ ๆ ตัวอย่างหนึ่งที่เราสนใจ

-   เพื่อให้เข้าใจพฤติกรรมของโมเดล ณ บริเวณที่เราสนใจ

    -   เราจะสร้างชุดข้อมูลจำลอง (artificial data) ขึ้นมารวม ๆ ตัวอย่างที่เราสนใจ

    -   นำข้อมูลจำลองนี้ไปสร้างโมเดลทำนายด้วยโมเดลง่าย ๆ เช่น linear regression ที่มี regularization หรือ decision tree แล้วใช้ผลการวิเคราะห์จากโมเดลนี้อธิบายพฤติกรรมของ black-box model แบบเฉพาะจุด เรียกโมเดลนี้ว่า glass-box model

    -   glass-box model นี้ ตัวแปรตามจะเป็นค่าทำนายที่ได้จาก black-box model ส่วนตัวแปรอิสระคือ artificial data

```{r}
## install.packages("DALEXtra")
## install.packages("lime")

library(DALEXtra)
library(lime)


predict_model.workflow <- function(x, newdata, type, ...) {
  yhat <- predict(x, new_data = newdata, type = type, ...)
  
  if (type == "raw") {
    data.frame(Response = yhat$.pred_class)
  } else {
    as.data.frame(yhat)
  }
}

explainer <- lime::lime(
  x = train_data %>% select(-G3),        # ข้อมูลต้นฉบับ (ไม่มี target)
  model = as_classifier(final_glm),      # โมเดลที่ train แล้ว (เช่น workflow)
  bin_continuous = TRUE,          # ให้ discretize ตัวแปรต่อเนื่อง
  n_bins = 4 ,                     # จำนวน bin ที่ใช้ (optional)
  quantile_bins = F
)

new_x <- test_data[1, ] %>% select(-G3)

explanation <- lime::explain(
  x = new_x,
  explainer = explainer,
  n_permutations = 5000,
  n_features = 5,          # จำนวนตัวแปรที่จะแสดงผล
  n_labels = 1,            # จำนวนคลาสที่สนใจ
  kernel_width = 0.2      # ค่าที่ควบคุมระยะของตัวอย่างจำลอง
)

explanation %>% plot_explanations()
explanation %>% plot_features()
```

ลองเปลี่ยน glass-box model

```{r}
glassbox_model1 <- function(x, y, weights, ...) {
  rpart::rpart(y ~ ., data = x, weights = weights, ...)
}
glassbox_model2 <- function(x, y, family, ...) {
  glmnet::glmnet(x,y, family = family, lambda = 0.2, alpha = 1)
}

explainer <- lime::lime(
  x = train_data %>% select(-G3),                     # ข้อมูลต้นฉบับ (ไม่มี target)
  model = as_classifier(final_glm),            # โมเดลที่ train แล้ว (เช่น workflow)
  bin_continuous = TRUE,          # ให้ discretize ตัวแปรต่อเนื่อง
  n_bins = 4 ,                     # จำนวน bin ที่ใช้ (optional)
  quantile_bins = F,
  fit_model = glassbox_model1
)

new_x <- test_data[50, ] %>% select(-G3)

explanation <- lime::explain(
  x = new_x,
  explainer = explainer,
  n_permutations = 5000,
  n_features = 5,          # จำนวนตัวแปรที่จะแสดงผล
  n_labels = 1,            # จำนวนคลาสที่สนใจ
  kernel_width = 0.2      # ค่าที่ควบคุมระยะของตัวอย่างจำลอง
)

explanation %>% plot_explanations()
explanation %>% plot_features()
```

#### 4.6.4 What-If Analysis

นักเรียนคนนี้มีการปรับเปลี่ยนพฤติกรรม เช่น มีคะแนนสอบ midterm ที่เพิ่มขึ้น และความรู้พื้นฐานที่ดีขึ้น แล้วผลลัพธ์จะเปลี่ยนแปลงไปยังไง

```{r}
test_data[1,]

profile1 <- explainer_svm %>% 
  predict_profile(
    new_observation = test_data[1,],
    variables = c("G2","G1")
  )
profile1 %>% 
  plot(variables = c("G2","G1"))

profile2 <- explainer_svm %>% 
  predict_profile(
    new_observation = test_data[50,],
    variables = c("G2","G1")
  )

plot(profile2,
     variables = c("G2","G1")) +
  labs(title = "What-If Analysis")
```
