---
title: "Week 6: Supervised Learning 2"
author: "ผศ.ดร.สิวะโชติ ศรีสุทธิยากร"
institute: "ภาควิชาวิจัยและจิตวิทยาการศึกษา <br> คณะครุศาสตร์ จุฬาลงกรณ์มหาวิทยาลัย"
format:
  revealjs:
    slide-number: c/t
    footer: "week 2: 2758623 Machine Learning Principles and Application<br>
            ผศ.ดร.สิวะโชติ ศรีสุทธิยากร"
    logo: https://github.com/ssiwacho/picture/blob/main/datakruroo.png?raw=true
    theme: theme.scss
    css: my_css.css
    scrollable: true
    transition: fade
    background-transition: fade
    highlight-style: github
    title-slide-attributes:
      data-background-image: img/ML.jpg
      data-background-opacity: 8%
      data-background-size: full
code-link: true
execute:
  echo: true
  freeze: auto
---

## Outline

-   Decision Tree

-   C5.0 Tree

-   Linear Discrimination

-   Naive Bayes

-   Support Vector Machine

## Decision Tree {.smaller}

-   decision tree ที่เป็นอัลกอริทึมพื้นฐานตัวหนึ่งที่สามารถใช้ได้พัฒนาโมเดลทำนายทั้งที่เป็น regression และ classification model โดย decision tree จัดเป็นอัลกอริทึมที่อยู่ในกลุ่ม nonparametric ซึ่งแตกต่างจาก linear regression

-   ารเรียนรู้ของ decision tree มีลักษณะเป็นการสร้างกฎเกณฑ์ในการแบ่งข้อมูลออกเป็นส่วนย่อยที่ไม่ทับซ้อนกันภายใต้ feature space จากคุณลักษณะดังกล่าวทำให้ decision tree เป็นอัลกอริทึมที่มีความยืดหยุ่นมากกว่า linear regression และสามารถใช้เรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้นได้ดีกว่า regression

-   อย่างไรก็ตามด้วยความที่ decision tree มีความยืดหยุ่นสูง ย่อมทำให้ความเสี่ยงที่จะประสบปัญหา overfitting สูงขึ้น ซึ่งสามารถแก้ไขได้โดยการใช้เทคนิคต่างๆ เช่น pruning ผ่านการ tune hyperparameters อีกวิธีการหนึ่งคือการใช้ ensemble learning ซึ่งจะถูกกล่าวถึงในบทเรียนถัดไป

## Decision Tree: Structure {.smaller}

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-549379503.png){width="70%"}

## Classification and Regression Trees (CART) {.smaller}

-   CART เป็นการสร้างพื้นที่ปิดล้อมรูปสี่เหลี่ยมที่ไม่ทับซ้อนกันเพื่อแบ่งส่วนของข้อมูลภายใน feature space ออกเป็นส่วนย่อย ๆ โดยการแบ่งแต่ละครั้งจะทำให้เกิดส่วนย่อยใหม่ขึ้น 2 ส่วน และดำเนินการแบ่งพื้นที่ดังกล่าวทวนซ้ำไปเรื่อย ๆ จนกว่าจะถึงจุดที่หยุดกระบวนการ เรียกกระบวนการแบ่งส่วนของพื้นที่ดังกล่าวว่า binary recursive partitioning

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-2146096965.png){width="70%"}

## Classification Trees (CART) {.smaller}

Impurity ที่มักใช้ในการคำนวณคือ Gini Impurity และ Entropy

$$
\text{Gini Impurity} = \sum_{i=1}^{k}p_i(1-p_i)  = 1 - \sum_{i=1}^{k} p_i^2
$$

$$
\text{Entropy} = -\sum_{i=1}^{k}p_i\log_2(p_i)
$$

โดยที่ $p_i$ คือความน่าจะเป็นของคลาส $i$ ในพื้นที่นั้นหรือสัดส่วนของคลาส $i$ ในพื้นที่นั้น

## Classification Trees (CART) {.smaller}

สมมุติว่าผู้วิเคราะห์ต้องการทำนาย deposit โดยใช้ตัวแปรอิสระจำนวน 3 ตัวได้แก่ default, housing และ loan ขั้นแรกของการพัฒนา decision tree คือการกำหนด root node ที่เหมาะสม คำถามคือ ควรใช้ตัวแปรอิสระตัวใดเป็น root node ดีเพราะเหตุใด?

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-1262716711.png)

## Classification Trees (CART) {.smaller}

สมมุติว่าผู้วิเคราะห์ต้องการทำนาย deposit โดยใช้ตัวแปรอิสระจำนวน 3 ตัวได้แก่ default, housing และ loan ขั้นแรกของการพัฒนา decision tree คือการกำหนด root node ที่เหมาะสม คำถามคือ ควรใช้ตัวแปรอิสระตัวใดเป็น root node ดีเพราะเหตุใด?

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-1755337911.png){width="70%"}

## Classification Trees (CART) {.smaller}

รูปด้านล่างแสดงการเปรียบเทียบ impurity ของตัวแปรตาม (deposit) ที่เกิดขึ้นจากการแบ่งส่วนข้อมูลภายใต้เงื่อนไขที่ Housing = Yes จากรูปจะเห็นว่าเมื่อใช้ Default เป็นตัวแบ่งจะได้ค่า impurity เท่ากับ 0.1731 แต่ถ้าใช้ Loan เป็นตัวแบ่งจะได้ค่า impurity เป็น 0.1420 ระหว่างตัวแปรอิสระสองตัวนี้ผู้อ่านคิดว่าควรเลือกตัวแปรอิสระใดมาเป็น internal node ในตำแหน่งดังกล่าว

![](https://datakruroo.netlify.app/mlcourse/_site/documents/images/image-1494665973.png)

## Regression Trees (CART) {.smaller}

การสร้าง regression tree จะใช้วิธีการเดียวกันกับ classification tree แต่จะใช้ค่า SSE แทนค่า impurity ที่ใช้ในการคำนวณค่าของตัวแปรและจุดแบ่งที่เหมาะสม

$$
\text{SSE} = \sum_{i=1}^{n}(y_i - \hat{y})^2
$$

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-2-1.png)

## Regression Trees (CART) {.smaller}

เป้าหมายของอัลกอริทึม decision tree คือการจุดตัดที่ทำให้ค่า Total SSE ดังกล่าวมีค่าต่ำที่สุด ซึ่งจะหมายความว่าการแบ่งส่วนย่อยนั้นสามารถสร้าง decision tree ที่ทำนายค่าของตัวแปรตามได้ใกล้เคียงค่าจริงมากที่สุดเท่าที่จะเป็นไปได้

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-3-1.png)

## Regression Trees (CART) {.smaller}

การแบ่งที่มีประสิทธิภาพสูงสุดคือ x = 2.7 (ทำไมนะ) ผลลัพธ์ในขั้นตอนนี้จะได้ต้นไม้ในลักษณะดังรูป

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-4-1.png)

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-4-2.png)

## Regression Trees (CART) {.smaller}

จากนั้นทำการหาจุดแบ่งต่อจำแนกตามฝั่งซ้ายและฝั่งขวาของจุดตัดแรก

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-5-1.png)

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-5-2.png)

## Regression Trees (CART) {.smaller}

การดำเนินการตามอัลกอริทึมข้างต้นจะดำเนินการทวนซ้ำไปเรื่อย ๆ จนกระทั่งค่า information gain ที่ได้จะมีค่าน้อยลู่เข้าสู่ 0 ซึ่งหมายความว่าโมเดลไม่สามารถเรียนรู้สารสนเทศใด ๆ จากข้อมูลได้เพิ่มเติมแล้ว รูปด้านล่างแสดงโมเดลทำนายภายหลังจากอัลกอริทึมดังกล่าวหยุดการดำเนินการทวนซ้ำแล้ว จะเห็นว่าการแบ่งส่วนย่อยตามอัลกอริทึม recursive binary partitioning ดังกล่าวทำให้ได้โมเดลทำนายที่สามารถเรียนรู้ความสัมพันธ์เชิงเส้นโค้งที่พบในข้อมูลได้อย่างมีประสิทธิภาพ

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-6-1.png)

![](https://datakruroo.netlify.app/mlcourse/_site/documents/03TreeModels_files/figure-html/unnamed-chunk-6-2.png)

## Hyperparameters สำหรับ decision tree {.smaller}

1.  `tree_depth`: จำนวนระดับของ decision tree นับตั้งแต่ root note ถึง leaf node

2.  `min_n`: จำนวนตัวอย่างขั้นต่ำที่ต้องมีใน terminal node

3.  `cost_complexity`: ค่า penalty ที่ใช้ชดเชย impurity หรือ Total SSE ของ decision tree การทำ penaty ใน tree จะช่วยลดความเสี่ยงที่จะเกิด overfitting

$$
Regularized \ Error = \text{Impurity} + \alpha \times \text{Tree Depth}
$$

## Parallel Computing {.smaller}

```{r eval = F, echo = T}
library(tidymodels)
library(future)

# ตั้งค่า parallel processing
plan(multisession, workers = 10)

# สร้าง cross-validation folds
cv_folds <- vfold_cv(train_data, v = 5, repeats = 3, strata = "research_score")

# สร้างโมเดล Decision Tree พร้อมกำหนดพารามิเตอร์ให้ปรับค่าได้ (tune)
dt_model <- decision_tree(
  cost_complexity = tune(),  # ควบคุมการ pruning
  tree_depth = tune(),       # ความลึกของต้นไม้
  min_n = tune()             # จำนวนตัวอย่างขั้นต่ำต่อโหนด
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

# กำหนด recipe
dt_recipe <- recipe(research_score ~ ., data = train_data) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors())

# สร้าง workflow
dt_workflow <- workflow() %>%
  add_model(dt_model) %>%
  add_recipe(dt_recipe)

# กำหนด grid ของ hyperparameters สำหรับ tuning
dt_grid <- grid_regular(
  cost_complexity(range = c(-5, 0), trans = log10_trans()),  # ใช้ log-scale เพื่อครอบคลุมช่วงที่กว้างขึ้น
  tree_depth(range = c(1, 20)),  # ความลึกของต้นไม้ตั้งแต่ 1 ถึง 20
  min_n(range = c(2, 30)),  # จำนวนตัวอย่างขั้นต่ำต่อโหนด
  levels = 5
)

# ตั้งค่าการควบคุมการ tuning
ctrl <- control_grid(save_pred = TRUE, parallel_over = "everything")

# ปรับค่าพารามิเตอร์โดยใช้ tune_grid()
dt_tuned_results <- tune_grid(
  dt_workflow,
  resamples = cv_folds,
  grid = dt_grid,
  metrics = metric_set(roc_auc, f_meas, sens, spec, precision),
  control = ctrl
)

# ตรวจสอบผลลัพธ์
dt_tuned_results
```

## Linear Discriminant {.smaller}

-   เป็นหนึ่งในเทคนิคการจำแนกประเภทที่ใช้ในการวิเคราะห์ข้อมูลที่มีลักษณะเป็นเชิงเส้น โดยมีจุดประสงค์เพื่อแยกแยะกลุ่มหรือคลาสของข้อมูลตามคุณลักษณะของข้อมูล (features)

-   หลักพื้นฐานของ LDA คือพยายามหาเส้นแบ่งข้อมูล ที่ทำให้ความแตกต่างของกลุ่มข้อมูลมากที่สุด ภายใต้ข้อตกลงเบื้องต้นว่าข้อมูลในแต่ละ class มีการแจกแจงแบบปกติที่มีคุณสมบัติ homogeneity of covariance matrices

-   การหาเส้นแบ่งข้อมูลด้วย LDA จะใช้หลักการลดมิติข้อมูล (เหมือน PCA) แต่แตกต่างกันตรงที่พยายามสกัดองค์ประกอบที่ทำให้ distribution ของข้อมูลแตกต่างกันมากที่สุด (maximize the ratio of between-class variance to within-class variance) เรียกสเกลองค์ประกอบที่สร้างขึ้นนี้ว่า discrimination function

-   จากนั้นจะสร้างจุดแบ่ง class จาก discrimination function ที่ได้ โดยจุดแบ่งนี้จะเป็นเส้นตรงที่ทำให้ค่าเฉลี่ยของ discrimination score ระหว่างสองกลุ่มมีค่าสูงสุด

## Linear Discriminant {.smaller}

หา eigenvector ที่เป็นคำตอบของสมการ Eigenvalue ของ $S_W^{-1} S_B$ ซึ่งทำให้การแยกกลุ่มของข้อมูลสูงสุด

![](https://sebastianraschka.com/images/blog/2014/linear-discriminant-analysis/lda_1.png){width="90%"}

## Quardratic Discriminant Analysis (QDA) {.smaller}

-   Quadratic Discriminant Analysis (QDA) เป็นรูปแบบหนึ่งของ Discriminant Analysis ที่คล้ายกับ LDA แต่ ไม่สมมติว่าข้อมูลในแต่ละคลาสมี covariance matrix ที่เหมือนกัน และ QDA จะสร้างเส้นแบ่งที่มีลักษณะเป็น quadratic (ไม่เชิงเส้น) เพื่อจำแนกข้อมูลระหว่างกลุ่มต่าง ๆ

-   QDA ยังสมมุติการแจกแจงแบบปกติของข้อมูลในแต่ละ class อยู่ แต่ไม่สมมุติว่า covariance matrix ของข้อมูลในแต่ละ class เหมือนกันแล้ว

-   เมื่อการแจกแจงมีการกระจายที่แตกต่างกันเส้นแบ่งระหว่าง class จึงมีลักษณะเป็นเส้นโค้ง

## Quardratic Discriminant Analysis (QDA) {.smaller}

![](https://mathformachines.com/images/quadratic-linear.png)

## Grid Search & Imbalanced Class Problem {.smaller}

ทดลอง fit multunomial logistic regression กับข้อมูลต่อไปนี้ กำหนดให้ตัวแปรตามคือ ผลการเรียนที่มี 4 ระดับ คือ fail, poor, moderate และ good ส่วนตัวแปรที่เหลือเป็นตัวแปรอิสระ

```{r}
library(tidymodels)
library(tidyverse)
data <- read_csv("/Users/choat/Documents/GitHub/datakruroo.github.io/MachineLearning/week05/student_data_missing.csv")

data <- data %>% 
  mutate(ach = case_when(
    gpax.y2 < 1.5 ~ "fail",
    gpax.y2 >= 1.5 & gpax.y2 < 2.5 ~ "poor",
    gpax.y2 >= 2.5 & gpax.y2 < 3.5 ~ "moderate",
    gpax.y2 >= 3.5 ~ "good"
  )) %>% 
  select(-gpax.y2)

glimpse(data)
```

ดำเนินการแบ่งข้อมูลออกเป็นสองส่วน เมื่อพิจารณาแผนภาพด้านล่างจะเห็นว่าการแจกแจงของตัวแปรตามมีลักษณะที่ไม่สมดุลอย่างมาก สภาพดังกล่าวอาจส่งผลให้โมเดลการเรียนรู้ของเครื่องมีตัวอย่างในกลุ่ม good และ fail น้่อยกว่ากลุ่ม moderate และ poor ซึ่งทำให้โมเดลมีประสิทธิภาพในการจำแนกกลุ่มดังกล่าวต่ำ

```{r}
library(patchwork)
set.seed(123)
split <- initial_split(data, prop = 0.7, strata = ach)
train_data <- training(split)
test_data <- testing(split)

train_data %>% 
  ggplot(aes(x=ach))+
  geom_bar()+
  ggtitle("train_data") +
test_data %>% 
  ggplot(aes(x=ach))+
  geom_bar()+
  ggtitle("test_data")
```

## ทดลองวิเคราะห์โดยไม่แก้ปัญหา

ขั้นแรกเราจะลองดำเนินการสร้างโมเดลทำนายโดยไม่แก้ปัญหา imbalanced class ด้วย multinomial logistic regression

1.  สร้าง recipe ของการทำ data preprocessing

```{r}
rec <- recipe(ach~. , data = train_data) %>% 
  step_mutate(infect = factor(infect, levels=c(1,2), labels=c("yes","no"))) %>% 
  step_mutate(pre_read = rowMeans(across(starts_with("pre.read")))) %>% 
  step_mutate(post_read = rowMeans(across(starts_with("post.read")))) %>%
  step_mutate(stress = rowMeans(across(starts_with("stress")))) %>%
  step_mutate(stu_engage = rowMeans(across(starts_with("stu.engage")))) %>%
  step_mutate(gender = factor(gender, levels=c(1,2), labels=c("M","F"))) %>% 
  step_mutate(t_exp = factor(t.exp, levels=c(1,2,3), labels=c("low","moderate","high"))) %>%
  step_mutate(stu_itcap = factor(stu.itcap , levels=c(1,2,3), labels=c("low","moderate","high"))) %>%
  step_mutate(t_itcap = factor(t.itcap, levels=c(1,2,3), labels=c("low","moderate","high"))) %>%
  step_mutate(climate = rowMeans(across(starts_with("climate")))) %>%
  step_mutate(t_mode = factor(t.mode, levels=c(1,2,3), labels=c("online","onsite","hybrid"))) %>% 
  step_mutate(internet = factor(internet, levels=c(1,2), labels=c("yes", "no"))) %>% 
  step_rm(starts_with("pre.read"), starts_with("post.read"), 
          starts_with("stress."), starts_with("stu.engage"), 
          starts_with("climate."),
          t.exp, stu.itcap,t.itcap, t.mode)

rec %>% prep() %>% 
  juice() %>% 
  summary()
```




```{r eval = F} 
rec <- recipe(ach~. , data = train_data) %>% 
  step_mutate(infect = factor(infect, levels=c(1,2), labels=c("yes","no"))) %>% 
  step_mutate(pre_read = rowMeans(across(starts_with("pre.read")))) %>% 
  step_mutate(post_read = rowMeans(across(starts_with("post.read")))) %>%
  step_mutate(stress = rowMeans(across(starts_with("stress")))) %>%
  step_mutate(stu_engage = rowMeans(across(starts_with("stu.engage")))) %>%
  step_mutate(gender = factor(gender, levels=c(1,2), labels=c("M","F"))) %>% 
  step_mutate(t_exp = factor(t.exp, levels=c(1,2,3), labels=c("low","moderate","high"))) %>%
  step_mutate(stu_itcap = factor(stu.itcap , levels=c(1,2,3), labels=c("low","moderate","high"))) %>%
  step_mutate(t_itcap = factor(t.itcap, levels=c(1,2,3), labels=c("low","moderate","high"))) %>%
  step_mutate(climate = rowMeans(across(starts_with("climate")))) %>%
  step_mutate(t_mode = factor(t.mode, levels=c(1,2,3), labels=c("online","onsite","hybrid"))) %>% 
  step_mutate(internet = factor(internet, levels=c(1,2), labels=c("yes", "no"))) %>% 
  step_rm(starts_with("pre.read"), starts_with("post.read"), 
          starts_with("stress."), starts_with("stu.engage"), 
          starts_with("climate."),
          t.exp, stu.itcap,t.itcap, t.mode) %>%
  ### --- 
  #step_unknown(all_nominal_predictors()) %>% 
  step_impute_bag(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) 

```

2.  ระบุ model และ workflow

```{r eval = F}
multinom_spec <- multinom_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

multinom_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(multinom_spec)
```

3.  สร้าง grid ของ hyperparameters สำหรับ tuning

Grid Search เป็นวิธีการปรับจูนค่าพารามิเตอร์ของโมเดล (Hyperparameter Tuning) โดยการสร้างชุดของค่าพารามิเตอร์ที่เป็นไปได้ในรูปแบบตารางหรือกริด และทดลองฝึกโมเดลด้วยค่าพารามิเตอร์ทุกชุดในกริดนั้น จากนั้นประเมินประสิทธิภาพของโมเดลแต่ละชุดเพื่อหาค่าที่ให้ผลลัพธ์ดีที่สุด

การปรับจูนพารามิเตอร์มีความสำคัญเพราะพารามิเตอร์เหล่านี้ส่งผลต่อประสิทธิภาพและความสามารถของโมเดลในการทำนายผลลัพธ์ การใช้ Grid Search ช่วยให้เราสามารถค้นหาค่าพารามิเตอร์ที่เหมาะสมที่สุดสำหรับปัญหาของเรา

-   สร้าง grid แบบ manual --\> regular grid

-   `grid_regular()`: สร้างกริดของค่าพารามิเตอร์แบบทุกค่าที่เป็นไปได้

-   `grid_random()` : สร้างกริดของค่าพารามิเตอร์แบบสุ่ม\<-- simple random sampling

-   `grid_max_entropy()`: เหมาะกับปัญหาที่ต้องการ hyperparameters ที่มีค่าแตกต่างกันมากที่สุด กระจายไปทั่ว grid space มากที่สุด

-   `grid_latin_hypercube()`: เหมาะกับกรณีที่มี hyperparameters หลายตัว และต้องการให้กระจายอย่างสม่ำเสมอใน grid space

วิธีการอื่นนอกเหนือจาก `grid_regular()` ถูกพัฒนาขึ้นเพื่อแก้ไขข้อจำกัดของการทำ manual grid search ที่ใช้ทรัพยากรในการประมวลผลมากเกินไปในหลายกรณี โดย `grid_random()` ใช้วิธีสุ่มค่าพารามิเตอร์แบบ simple random sampling จาก grid space ซึ่งช่วยลดการใช้ทรัพยากรในการประมวลผลลงได้ อย่างไรก็ตามวิธีการอาจมีความเสี่ยงที่จะสุ่มไปยังค่าที่ไม่ได้มีประสิทธิภาพสูงสุด

```{r eval = F}
grid_space <- parameters(penalty(range=c(-10,0)), 
                mixture(range=c(0,1)))
```


```{r}
set.seed(1234)
grid_regular(penalty(range=c(-10,0)),levels=10)
regular_grid <- grid_regular(x = grid_space, levels = 10)
p1<-regular_grid %>% 
  ggplot(aes(x=penalty, y=mixture))+
  geom_point()+
  scale_x_log10()+
  theme(legend.position = "none")+
  ggtitle("Regular Grid")
```

```{r}
set.seed(1234)
random_grid <- grid_random(x = grid_space, size = 20) ## ทำ simple random sampling grid จาก grid space
p2<-random_grid %>%
  ggplot(aes(x=penalty, y=mixture))+
  geom_point()+
  scale_x_log10()+
  theme(legend.position = "none")+
  ggtitle("Random Grid")
```

```{r fig.width =8, fig.height = 8}
set.seed(1234)
maxentropy_grid <- grid_max_entropy(x = grid_space, size = 20)
p3<-maxentropy_grid %>%
  ggplot(aes(x=penalty, y=mixture))+
  geom_point()+
  scale_x_log10()+
  theme(legend.position = "none")+
  ggtitle("Max Entropy Grid")

set.seed(1234)
latin_grid <- grid_latin_hypercube(x = grid_space, size = 20)
p4<- latin_grid %>%
  ggplot(aes(x=penalty, y=mixture))+
  geom_point()+
  scale_x_log10()+
  theme(legend.position = "none")+
  ggtitle("Latin Hypercube Grid")


(p1+p2)/(p3+p4)
```


4.  hyperparameter tuning

```{r eval = F}
library(future)
availableCores() ##-- ตรวจสอบจำนวน cores
plan(multisession, workers = 13)
```


```{r}
### -------
cv_folds <- vfold_cv(train_data, v = 5, repeats = 3, strata = "ach")
ctrl <- control_grid(save_pred = TRUE, parallel_over = "everything")

regular_tune <- multinom_wf %>% 
                tune_grid(resamples = cv_folds, 
                          grid = regular_grid, 
                          control = ctrl)

random_tune <- multinom_wf %>%
                tune_grid(resamples = cv_folds, 
                          grid = random_grid, 
                          control = ctrl)

maxentropy_tune <- multinom_wf %>%
                tune_grid(resamples = cv_folds, 
                          grid = maxentropy_grid, 
                          control = ctrl)

latin_tune <- multinom_wf %>%
                tune_grid(resamples = cv_folds, 
                          grid = latin_grid, 
                          control = ctrl)
```


```{r}
p1 <- regular_tune %>% collect_metrics() %>% 
  filter(.metric == "roc_auc") %>%
  ggplot(aes(x = penalty, y = mixture))+
  geom_tile(aes(fill = mean))+
  geom_text(aes(label = round(mean,2)))+
  scale_x_log10()+
  scale_fill_viridis_c(option = "A")+
  theme_light()
p1
```


```{r}
# รวมค่าพยากรณ์จากทั้ง 4 grid search
results <- bind_rows(
  regular_tune %>% collect_metrics() %>% mutate(method = "regular"),
  random_tune %>% collect_metrics() %>% mutate(method = "random"),
  maxentropy_tune %>% collect_metrics() %>% mutate(method = "max_entropy"),
  latin_tune %>% collect_metrics() %>% mutate(method = "latin_hypercube")
)

results %>% 
  filter(.metric == "roc_auc") %>% 
  ggplot(aes(x = method, y = mean, fill = method)) +
  geom_boxplot() +
  theme_minimal()

```

ลองเอาผลการ tune จาก `latin_tune` มาวิเคราะห์เพิ่มเติม

```{r eval = F}
latin_tune %>% collect_metrics() %>% 
  filter(.metric == "roc_auc") %>%
  arrange(desc(mean))

best_latin <- latin_tune %>% select_best(metric = "roc_auc")

final_multinom <- multinom_wf %>% 
  finalize_workflow(best_latin) %>%
  fit(data = train_data)


final_multinom  %>% 
  predict(new_data = test_data) %>% 
  bind_cols(test_data %>% select(ach))  %>% 
  mutate(.pred_class = fct_relevel(.pred_class, "fail","poor","moderate","good")) %>% 
  mutate(ach = fct_relevel(ach, "fail","poor","moderate","good")) %>% 
  conf_mat(truth = ach, estimate = .pred_class)
```

```{r}
final_multinom  %>% 
  predict(new_data = test_data) %>% 
  bind_cols(test_data %>% select(ach))  %>% 
  mutate(.pred_class = fct_relevel(.pred_class, "fail","poor","moderate","good")) %>% 
  mutate(ach = fct_relevel(ach, "fail","poor","moderate","good")) %>% 
  group_by(ach) %>%
  spec(truth = ach, estimate = .pred_class)
```



```{r}
  group_by(ach) %>% 
  sens(truth = ach, estimate = .pred_class)

final_multinom  %>% 
  predict(new_data = test_data) %>% 
  bind_cols(test_data %>% select(ach))  %>% 
  mutate(.pred_class = fct_relevel(.pred_class, "fail","poor","moderate","good")) %>% 
  mutate(ach = fct_relevel(ach, "fail","poor","moderate","good")) %>% 
  group_by(ach) %>% 
  precision(truth = ach, estimate = .pred_class)
```

## Imbalance Class Problem Solving {.smaller}

ข้อมูลไม่สมดุลเกิดขึ้นเมื่อการกระจายตัวของคลาสเป้าหมายในชุดข้อมูลมีความแตกต่างกันอย่างมาก โดยคลาสส่วนน้อย (Minority Class) มักมีตัวอย่างน้อยกว่าคลาสส่วนมาก (Majority Class) เช่น 5 เท่า หรือ 10 เท่า

จากตัวอย่างข้างต้นจะเห็นว่า ปัญหาข้อมูลไม่สมดุล (imbalance class) อาจส่งผลให้การจำแนกของโมเดลในบางคลาสมีประสิทธิภาพที่ต่ำ การแก้ปัญหาดังกล่าวสามารถทำได้หลายวิธีการ เช่น

-   แบ่งคลาสใหม่ ในกรณีที่คลาสถูกสร้างขึ้นจากตัวแปรต่อเนื่อง

-   การใช้การปรับน้ำหนักของคลาสในโมเดล

-   ในเทคนิคการสุ่มซ้ำข้อมูลในคลาสที่มีจำนวนน้อย

ใน R มี library-themis ที่ช่วยแก้ปัญหาดังกล่าวและสามารถทำงานร่วมกับ tidymodels ได้

```{r eval = F}
## install.packages("themis")
library(themis)
```

ใน library มีวิธีการแก้ปัญหา 3 แนวทางหลัก ได้แก่

1.  Undersampling

2.  Oversampling

3.  Hybrid sampling


```{r}
train_data %>% 
  ggplot(aes(x=ach))+
  geom_bar()+
  ggtitle("train_data") 
```


## Undersampling {.smaller}

-   สุ่มลบตัวอย่างใน majority class จนได้อัตราส่วนที่ต้องการ `step_downsample()`

-   สุ่มลบ/เลือกตัวอย่างแบบมีกลยุทธ์ มี 2 วิธีการได้แก่

    -   `step_nearmiss()` -- เลือกลบตัวอย่างใน majority class ที่มีระยะห่างเฉลี่ยกับ minority class น้อยที่สุด

    -   `step_tomek()` -- Tomek link คือคู่ของตัวอย่างที่มีคลาสต่างกันและอยู่ใกล้กันมากที่สุด วิธีการนี้คือการลบตัวอย่างที่เป็น Tomek link ออกจาก majority class

## Oversampling {.smaller}

-   `step_smote()` -- Synthetic Minority Over-sampling Technique สร้างตัวอย่างสังเคราะห์ใน minority class โดยใช้การ interpolation ระหว่างตัวอย่างจริงที่มีอยู่ หลักการคร่าว ๆ คือ (1) เลือกตัวอย่างใน minority class แบบสุ่ม (2) หาเพื่อนบ้านที่ใกล้ที่สุด k ตัวจากตัวอย่างที่เลือกมา (3) สุ่มเลือก 1 เส้นทางระหว่างตัวอย่างที่เลือกกับเพื่อนบ้านมา (4) สร้างตัวอย่างใหม่โดยใช้ interpolation ระหว่างตัวอย่างที่เลือกกับเพื่อนบ้านที่ใกล้สุดดังกล่าว

$$
X_{syn} = X_{real} + \lambda(X_{neighbor-X_{real}}) \ \ ; \ \ \lambda \in [0,1]
$$

-   `step_bsmote()` -- Borderline SMOTE แก้ปัญหา SMOTE แบบดั้งเดิมที่อาจไปสร้างตัวอย่างใหม่ในพื้นที่ที่ไม่มีข้อมูลจริงมากเกินไป โดย BSMOTE จะเน้นสร้างตัวอย่างสังเคราะห์ที่อยู่ใกล้กับ decision boundary ระหว่าง minority class กับ majority class

-   `step_adasyn()` -- Adaptive Synthetic Sampling ปรับปรุง SMOTE โดย เพิ่มข้อมูลเฉพาะจุดที่มีปัญหาความไม่สมดุลสูงที่สุด หมายความว่า จุดที่อยู่ใกล้ majority class มากเกินไป จะได้รับการ oversampling มากขึ้น


-   `step_rose()` -- Random Over-Sampling Examples สร้างตัวอย่างใหม่ทั้งใน minority และ majority class โดยการสุ่มจาก kernal density estimation (KDE) โดยปกติใช้ normal distribution ผลลัพธ์ที่ได้จะได้ตัวอย่างใหม่รอบ ๆ ตัวอย่างที่เลือกมาเดิม ทั้งนี้จะมีการกำหนดอัตราส่วนระหว่างคลาส และการกระจายของการสุ่มตัวอย่างใหม่ เป็น hyperparameter ของอัลกอริทึม


```{r fig.width=8, fig.height=6}
library(themis)

## no smote
p1<-rec %>% 
  prep() %>% 
  juice() %>% 
  ggplot(aes(x=gpax.y1, y=learn.per2, col =ach))+
  geom_point()

## smote
p2<-rec %>% 
  step_smote(ach, over_ratio = 1,
             neighbors = 5) %>% 
  prep() %>% 
  juice() %>% 
  ggplot(aes(x=gpax.y1, y=learn.per2, col =ach))+
  geom_point()


(p1+p2)
```

## Hybrid Sampling {.smaller}

การผสมผสานระหว่าง undersampling และ oversampling ในบางกรณีช่วยให้การแก้ปัญหา imbalance class มีประสิทธิภาพมากขึ้นได้ แนวทาง hybrid มีหลายวิธีการดังนี้

**SMOTE + Tomek Links**

-   ใช้ SMOTE เพิ่มตัวอย่างใหม่ใน minority class

-   ใช้ Tomek link ลบตัวอย่างใน majority class ที่อยู่ใกล้กับ minority มากเกินไป

```{r}
rec %>% 
  step_smote(ach, over_ratio = 0.7,
             neighbors = 5) %>% 
  step_tomek(ach) %>% 
  prep() %>% 
  juice() %>% 
  ggplot(aes(x = ach))+
  geom_bar()
```


**ANDASYN + NearMiss**

-   ใช้ ADASYN เพิ่มตัวอย่างใหม่ใน minority class โดยเน้นจุดที่มีความไม่สมดุลสูง

-   ใช้ Nearmiss เพื่อลบตัวอย่าง majority class ที่อยู่ใกล้ minority class มากที่สุดty

```{r}
rec %>% 
  step_adasyn(ach, over_ratio = 0.7,
             neighbors = 5) %>% 
  step_nearmiss(ach) %>% 
  prep() %>% 
  juice() %>% 
  ggplot(aes(x = ach))+
  geom_bar()
```



## กิจกรรม

สร้างโมเดลจำแนกผลการเรียนดังนี้

1. โมเดล baseline ไม่ต้องแก้ปัญหา imbalance class

2. โมเดลที่แก้ปัญหา imbalance class ด้วย  undersampling 1 โมเดล

3. โมเดลที่แก้ปัญหา imbalance class ด้วย  oversampling 2 โมเดล

4. โมเดลที่แก้ปัญหา imbalance class ด้วย  hybrid sampling 1 โมเดล



## Support Vector Machine (SVM) {.smaller}

-   Support Vector Machine (SVM) เป็น supervised learning algorithm ที่ใช้สำหรับการจำแนกประเภท (classification) และการวิเคราะห์การถดถอย (regression) หลักการทำงานคร่าว ๆ ของ SVM คือการสร้างเส้นแบ่งแยก (hyperplane) ที่สามารถแยกข้อมูลออกเป็นกลุ่ม/คลาสต่าง ๆ ในพื้นที่ P มิติให้ได้ดีที่สุด โดยที่ P คือจำนวน Features

-   อย่างไรก็ตามการสร้างเส้นแบ่งดังกล่าวเป็นสิ่งที่โมเดลการเรียนรู้ต่าง ๆ ก็ทำเหมือนกัน ดังรูปด้านขวา จะเห็นเส้นแบ่งที่ดีที่สุดจากแต่ละอัลกอริทึม

![](img/HMC.png)

## Support Vector Machine (SVM) {.smaller}

support vector machine มีหลักการเบื้องต้นคล้ายกับ classifier แบบ linear อย่างไรก็ตามการหาเส้นแบ่งของ SVM จะพิจารณาจาก support vector ที่เป็นข้อมูลที่อยู่ใกล้ขอบเขตการจำแนกข้อมูลมากที่สุดด้วย (มี contraint เพิ่มเติม) กล่าวคือ SVM จะพยายามหาเส้นแบ่งที่ทำให้ระยะห่างระหว่าง support vector กับเส้นแบ่งมีค่ามากที่สุด ดังรูป

![](img/SVM1.png){width="60%"}

## Support Vector Machine (SVM) {.smaller}

การจำแนกข้อมูลด้วยอัลกอริทึมตามหลักข้างต้นจะเรียกว่า hard margin classification อย่างไรก็ตามอัลกอริทึมประเภทนี้มีข้อจำกัดในบางกรณี เช่น

![](img/outlier.png){width="60%"}

## Support Vector Machine (SVM) {.smaller}

-   การแก้ปัญหาดังกล่าวคือการปรับให้โมเดลมีความยืดหยุ่นมากขึ้น โดยใช้ soft margin classification ซึ่งจะให้ความผิดพลาดได้มากขึ้น แต่ก็จะลดความเสี่ยงที่เกิดจากข้อมูลผิดปกติ

-   แนวคิดดังกล่าวคือการเพิ่ม hyperparameter อีกตัวหนึ่งขึ้นมาเขียนแทนด้วย C (penalty term) ที่ใช้ควบคุมระยะห่างระหว่าง hyperplan กับข้อมูลที่ใกล้ที่สุดจากทั้งสองกลุ่ม (support vector)

![](img/SMC.png)

## Support Vector Machine (SVM): Kernel Trick {.smaller}

หลายกรณีความสัมพันธ์ระหว่างตัวแปรอาจมีความซับซ้อนทำให้เส้นแบ่งที่เป็นเส้นตรงไม่สามารถจำแนกข้อมูลได้ดี กรณีเช่นนี้ linear classifier ทั้งหลายอาจจะให้ประสิทธิภาพที่ต่ำ

SVM แก้ปัญหานี้ด้วยการใช้ kernel trick ซึ่งเป็นการแปลงข้อมูลเป็นมิติสูงขึ้นเพื่อที่จะสามารถใช้เส้นตรง (hyperplane) แบ่งข้อมูลในมิติสูงอย่างมีประสิทธิภาพ

![](img/nonlinearSVM.png)

## Support Vector Machine (SVM): Kernel Trick {.smaller}

-   Linear Kernel

-   Polynomial Kernel

-   Radial Basis Function (RBF) Kernel (Gaussian Kernel)

-   Sigmoid Kernel

## Naive Bayes {.smaller}

-- ดูเนื้อหาคลิปวิดิโอ —

```{r eval = F}
nb_spec <- naive_Bayes(smoothness = tune(), Laplace = tune()) %>%  
  set_engine("klaR") %>%  
  set_mode("classification")
```
