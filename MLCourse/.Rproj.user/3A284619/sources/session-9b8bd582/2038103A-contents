---
title: "Untitled"
---

# TidyText Format

Tidytext format เป็นวิธีการจัดการข้อมูลประเภทข้อความ (text data) ที่อยู่ภายใต้หลัก tidy data ซึ่งช่วยให้ผู้วิเคราะห์ข้อมูลสามารถใช้เครื่องมือต่าง ๆ ภายใต้ tidyverse ในการดำเนินการวิเคราะห์ข้อมูลประเภทข้อความได้

ชุดข้อมูลข้อความแบบ tidytext ถูกนิยามให้มีเป็นตารางข้อมูลแบบ one-token-per-row กล่าวคือภายในแต่ละแถวของตาราง จะประกอบด้วย token ซึ่งเป็นคำ (word) หรือ n-gram หรือประโยค หรือย่อหน้า ที่มีความหมาย

นอกจากรูปแบบ tidytext ที่จะกล่าวถึงในเอกสารนี้ ยังมีวิธีการจัดเก็บและจัดการข้อมูลข้อความในลักษณะอื่น ๆ ที่มีความแตกต่างไปจาก tidytext ดังนี้

-   String format: เป็นการเก็บข้อความไว้ในเวกเตอร์แบบตัวอักษร

-   Corpus: เป็นฐานข้อมูลประเภทหนึ่งสำหรับข้อมูลประเภทข้อความที่ใช้เป็นฐานในการวิเคราะห์ ซึ่งข้อมูลดังกล่าวเป็นไปได้ทั้ง หนังสือ เอกสาร บทความ บทพูด เป็นต้น

-   Document-term matrix: เป็น sparse matrix ที่แถวของเมทริกซ์เป็นประโยคหรือเอกสาร ข้อมูลภายในเมทริกซ์จะเป็นความถี่หรือ tf-idf ของคำแต่ละคำภายในประโยคแต่ละประโยค

กรณีศึกษานี้มีวัตถุประสงค์คือเพื่อพัฒนาโมเดลทำนายความรู้สึก (positive, negative) โดยใช้ชุดข้อมูล `tcas61.csv` ขั้นตอนแรกจะนำข้อมูลเข้าและดำเนินการวิเคราะห์ข้อมูลเบื้องต้นก่อน

```{r}
library(tidyverse)
library(textrecipes)
library(tidymodels)
## importing data
dat <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/review_shopping.csv", header=F, sep="\t",col.names = c("text","sentiment"))
glimpse(dat, width=60)
```

จากชุดข้อมูลข้างต้นจะเห็นว่ามีข้อมูลอยู่ 2 คอลัมน์ คอลัมน์แรกเป็นข้อความ (`text`) ส่วนคอลัมน์ที่สองเป็นความรู้สึกของข้อความ (`sentiment`)

ผู้วิเคราะห์ดำเนินการสำรวจข้อมูลข้างต้นพบว่า ชุดข้อมูลนี้มีข้อความทั้งหมด 124 ข้อความ ส่วนใหญ่จำนวน 80 ข้อความ (ร้อยละ `r round(80*100/124,2)`) เป็นข้อความเชิงลบ

```{r}
table(dat$sentiment)
```

## data splitting

ขั้นตอนถัดมาจะแบ่งข้อมูลออกเป็น training กับ test dataset ดังนี้

```{r}
split<-initial_split(dat, prop = 0.7, strata = sentiment)
train<-training(split)
test<-testing(split)
dim(train)
glimpse(train, width=60)
```

## Preprocessing data

วัตถุประสงค์ในขั้นตอนนี้คือสร้างชุดข้อมูลของข้อความที่มีลักษณะเป็น document term matrix และใช้เมทริกซ์ดังกล่าวเป็นข้อมูลนำเข้าสำหรับพัฒนาโมเดลทำนาย DTM มีลักษณะคือเป็นชุดข้อมูลที่แถวจะเก็บข้อมูลของหน่วยข้อมูลที่เป็นข้อความ/ประโยค หรือเอกสารของกลุ่มเป้าหมาย ส่วนคอลัมน์จะเก็บข้อมูลความถี่ของคำแต่ละคำภายในแต่ละข้อความ

ขั้นแรกของการสร้าง DTM คือการตัดคำในแต่ละข้อความให้แยกออกจากกัน จากนั้นเป็นการทำความสะอาดข้อมูลเพื่อลบคำที่ไม่มีความหมาย หรืออักขระพิเศษต่าง ๆ ออกไปจากเมทริกซ์ เพื่อเพิ่มประสิทธิภาพการเรียนรู้ให้กับโมเดล การตัดคำในภาษาไทย แนะนำให้ใช้ engine ของ library pythainlp ของภาษา Python <https://pythainlp.github.io/projects/>

### Tokenization using pythainlp

ตัวอย่างนี้ผู้วิเคราะห์จะตัดคำในประโยคด้วยฟังก์ชัน `word_tokenize()` ภายใต้ library pythainlp โดยใช้ engine deepcut ในภาษา python แต่จะสั่งงานกระบวนการดังกล่าวจาก R Studio ผ่าน package reticulate ซึ่งสามารถทำได้หลายวิธีการ

วิธีการแรกคือการเขียนฟังก์ชันของภาษา python เอาไว้ใน script file แล้วเรียกใช้ด้วย R ซึ่งมีขั้นตอนการดำเนินงาน 2 ขั้นตอน ขั้นแรกเป็นการเขียนฟังก์ชันสำหรับตัดคำใน python โดยเขียนไว้ใน script file ของ python `token.py` ดังนี้

```{python eval=F}
# define custom tokenlization function
def tokenize_thai_sentence(text):
  import pythainlp
  import deepcut

  tokens = pythainlp.word_tokenize(text, keep_whitespace=False, engine="deepcut")
  return tokens
```

ขั้นตอนที่สองเป็นการเรียกใช้ tokennization function ที่เขียนใน `token.py` มาใช้ใน R ซึ่งสามารถทำได้โดยเขียนคำสั่งดังนี้

```{r message=F}
library(reticulate)
# load tokenize_thai_sentence function from python script
source_python("token.py")
## defined custom tokenization function
tokenize_thai <- function(text){
tokens <- lapply(text, tokenize_thai_sentence)
tokens_list <- lapply(tokens, function(x) {
                              paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
train<-train %>%
  mutate(token = tokenize_thai(text))
```

ผลลัพธ์ที่ได้จากการดำเนินการข้างต้น จะได้ประโยคที่มีการแบ่งคำไว้เรียบร้อยด้วยเครื่องหมายเว้นวรรค ดังตัวอย่างต่อไปนี้

```{r}
train[1,"token"]
train[2,"token"]
```

อีกวิธีการหนึ่งคือการเรียกใช้ tokennization function จาก pythainlp ผ่าน reticulate ของ R โดยตรงดังนี้

```{r}
# Import pythainlp library
pythainlp <- import("pythainlp")

# Define a custom tokenization function for textrecipes
tokenize_thai2 <- function(text) {
  tokens <- lapply(text, pythainlp$word_tokenize, engine="longest")
  tokens_list <- lapply(tokens, function(x) {
                              paste(x, collapse = " ")})
  tokens_list <- unlist(tokens_list)
return(tokens_list)
}

## tokenized via mutate function
train<-train %>%
  mutate(token = tokenize_thai2(text))
```

เมื่อได้ชุดข้อมูลข้างต้นแล้ว จะใช้ฟังก์ชัน `unnest_token()` เพื่อสร้างชุดข้อมูล tidytext ดังนี้

```{r}
train <- train %>%
  unnest_tokens(token = "words", input=token, output=word)
```

ลองทำสำรวจคำที่ตัดได้ในเบื้องต้น จะเห็นว่ามีคำที่ไม่ให้ความหมายอยู่เป็นจำนวนมาก

```{r}
train %>%
  count(word) %>%
  arrange(desc(n)) %>%
  filter(n>10) %>%
  ggplot(aes(x = n, y= reorder(word,n)))+
  geom_col()+
  theme(text=element_text(family="ChulaCharasNew"))
```

### Remove Stopwords

Stopwords คือคำที่ไม่ให้ความหมายหรือไม่มีประโยชน์ในการตีความหมายของประโยคหรือข้อความเป้าหมาย เช่น a, and, the, และ หรือ อะ ซึ่งในชุดข้อมูลหากมีคำประเภทนี้อยู่มาก ๆ จะกลายเป็น noise ที่รบกวนการวิเคราะห์ ใน library pythainlp มีการรวบรวม stopwords สำหรับภาษาไทยเอาไว้พอสมควร ผู้วิเคราะห์สามารถนำมาใช้ได้ การเรียก stopword จาก pythinlp มาใช้สามารถทำได้ในทำนองเดียวกับการตัดคำดังนี้

```{python eval=F}
def stop word():
  import pythainlp
  
  stopword = pythainlp.corpus.common.thai_stopwords()
  stopword = list(stopword)
  return stopword
```

การเรียก stopword จาก pythainlp ออกมาใช้สามารถทำได้ดังนี้ จะเห็นว่าชุดข้อมูล stopword ดังกล่าวรวบรวมคำไว้จำนวน 1,030 คำ

```{r}
source_python("stopword.py")
length(stopword())
head(stopword(),10)
tail(stopword(),10)
train<-train %>%
  filter(!word %in% stopword())
```

เมื่อคัดกรอง stopword ออกไปและดำเนินการสำรวจข้อมูลเบื้องต้นพบว่าได้ผลการวิเคราะห์เป็นดังนี้

```{r fig.height=6}
train %>%
  count(sentiment,word) %>%
  arrange(desc(n)) %>%
  filter(n>5) %>%
  ggplot(aes(x = n, y= reorder(word,n), 
             fill=sentiment))+
  geom_col()+
  theme(text=element_text(family="ChulaCharasNew"))

train<-train %>%
  filter(!word %in% c("ๆ","ก็","ครับ","ค่ะ","เลย","กับ","มี"))
```

## modelling using tidymodels

การสร้างโมเดลทำนายอารมณ์ความรู้สึก (sentiment) ด้วยข้อความจะใช้คำ/วลี/ประโยคภายในข้อความที่ผ่านการจัดระเบียบและทำความสะอาดแล้วมาสร้างเป็น feature matrix สำหรับทำนาย feature matrix ของข้อความดังกล่าวจะมีลักษณะคือ แถวเป็นข้อความเต็มที่มีความหมายบวกหรือลบหรืออื่น ๆ ส่วนคอลัมน์เป็นความถี่ของคำแต่ละคำที่ปรากฎอยู่ภายในแต่ละข้อความ เมทริกซ์ดังกล่าวมีสามประเภท ได้แก่ Term Frequency matrix (TF matrix), Inverse Document Frequency matrix (IDF matrix) และ TF-IDF matrix รายละเอียดมีดังนี้

-   TF matrix เป็นเมทริกซ์ที่ใช้แสดงความถี่ของการเกิดคำแต่ละคำภายในข้อความ/เอกสารเป้าหมาย โดยส่วนใหญ่ค่าความถี่ดังกล่าวจะรายงานเป็นร้อยละการพบคำดังกล่าวเทียบกับข้อความทั้งหมด หากมีแนวโน้มที่จะพบคำที่แตกต่างกันภายในข้อความ/เอกสารที่แตกต่างประเภทกัน แสดงว่ามีความสัมพันธ์กันระหว่างคำที่ใช้กับข้อความ ผู้วิเคราะห์สามารถใช้ความสัมพันธ์ดังกล่าวมาสร้างเป็นโมเดลทำนายได้
-   IDF matrix เป็นเมทริกซ์ที่ให้ค่าน้ำหนัก/ความสำคัญของคำแต่ละคำภายในข้อความ/เอกสารเป้าหมายทั้งหมด (เรียกว่า corpus) ค่าน้ำหนักดังกล่าวมีค่าเท่ากับ $log(n)/n_j$ เมื่อ $n$ คือจำนวนข้อความทั้งหมด และ $n_j$ คือจำนวนข้อความที่พบคำที่ $j$ จากค่าน้ำหนักดังกล่าวจะเห็นว่า หากคำ ๆ ใดพบมากพบบ่อยในหลาย ๆ ข้อความ แสดงว่าคำนั้นเป็นคำทั่วไปและมีแนวโน้มที่จะมีความสามารถในการจำแนกข้อความที่มีความแตกต่างกันได้ยาก ในทางกลับกันคำ ๆ ใดที่พบน้อยแสดงว่าเป็นคำเฉพาะสำหรับข้อความบางประเภท ดังนั้นคำดังกล่าวจึงมีแนวโน้มที่จะใช้จำแนกข้อความที่มีความแตกต่างกันได้ดีกว่า
-   TF-IDF matrix เป็นเมทริกซ์ที่รวมสูตรการคำนวณระหว่าง TF และ IDF matrix โดย TF-IDF จะคำนวณน้ำหนักของคำโดยใช้ ผลคูณระหว่าง TF กับ IDF วิธีการนี้จึงทำให้สามารถระบุน้ำหนักของคำต่าง ๆ ภายใต้ข้อความเป้าหมายได้ครอบคลุมมิติทั้งในด้านความบ่อยและความสำคัญไปได้พร้อมกัน โดยปกติ TF-IDF matrix เป็นเทคนิคที่ใช้มากกว่าสองเมทริกซ์ข้างต้น

```{r}
# create recipe object
rec<-recipe(sentiment~., data=train) %>%
  step_tokenize(word) %>%
  step_tokenfilter(word, max_tokens = 100) %>%
  step_tfidf(word) %>%
  step_normalize(all_numeric_predictors())

# create model
logit <- logistic_reg(penalty=tune(),
                      mixture=tune()) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

# create workflow
workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(logit)

# tune grid
fold <- vfold_cv(train, v=10, repeats=3, strata=sentiment)

```

```{r}
stopword
```

```{r}

thai_stopwords <- py_to_r(py_eval("list(pythainlp.corpus.common.thai_stopwords())"))

thai_stopwords_py <- pythainlp$corpus$common$thai_stopwords()
py_eval("list(thai_stopwords_py)")
thai_stopwords <- py_to_r(thai_stopwords_py)

# P
train %>%
  filter(token )

library(reticulate)

# Assuming you have a frozenset object in Python
py_set <- py_eval("frozenset(['apple', 'banana', 'cherry'])")
# Convert the frozenset to a list in Python using a custom function
py_list <- py_run_string("def frozenset_to_list(py_set): 
                         return list(py_set)")$frozenset_to_list(py_set)

# Convert the Python list to an R vector
r_vector <- py_to_r(py_list)

# Print the R vector
print(r_vector)


```

```{r}
rec <- recipe(~text, data=train) %>%
  step_tokenize(text, token  = tokenize_thai2
                ) %>%
  step_tokenfilter(text, min_times=1) %>%
  prep(NULL) %>%
  juice()
```

## `unnest_tokens()`

ฟังก์ชัน `unnest_tokens()` มีหน้าที่สำหรับ

```{r}
temp<-read.csv("https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/tcas61.csv", header=F, sep="\t",
               col.names = c("text","sentiment"))
temp<-tibble(temp)
head(temp)
table(temp$sentiment)

library(recipes)

library(tidytext)

```

```{python}
import pythainlp
stopword=list(pythainlp.corpus.common.thai_stopwords())
```
