# Import pythainlp library
pythainlp <- import("pythainlp")

# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
  tokens <- lapply(text, pythainlp$word_tokenize)
  tokens_list <- lapply(tokens, function(token) {
    paste(token, collapse = " ")
  })
  unlist(tokens_list)
}
temp<-tokenize_thai(train$text)

# Prepare the data (replace this with your own data)
train <- tibble(text = c("ข้อความภาษาไทย 1", "ข้อความภาษาไทย 2"))

# Prepare the data (replace this with your own data)
train <- tibble(text = c("ข้อความภาษาไทย 1", "ข้อความภาษาไทย 2"))

# Create a recipe
recipe <- recipe(~ text, data = train) %>%
  step_tokenize(text, token = tokenize_thai) %>%
  step_tokenfilter(text, min_times = 1) %>%
  step_tfidf(text)

# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train)



# Import pythainlp library
pythainlp <- import("pythainlp")

# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
  tokens <- lapply(text, pythainlp$word_tokenize)
  tokens <- tokenlist(tokens)
  tokens
  }

tokenize_thai(train$text) %>% tibble()

# Prepare the data (replace this with your own data)
train <- tibble(text = c("ข้อความภาษาไทย 1", "ข้อความภาษาไทย 2"))

# Create a recipe
recipe <- recipe(~ text, data = train) %>%
  step_tokenize(text) %>%
  step_tokenfilter(text, min_times = 1) %>%
  step_tfidf(text)

# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train)


recipe(~ text, data = train) %>%
  step_tokenize(text) %>%
  prep(NULL) %>%
  juice()



split<-initial_split(tcas, prop = 0.7, strata = sentiment)
train<-training(split)
test<-testing(split)
dim(train)
glimpse(train, width=80)


# load tokenize_thai_sentence function from python script
source_python("token.py")

## defined custom tokenization function
tokenize_thai <- function(text) {
  tokens <- lapply(text, pythainlp$word_tokenize, engine = "deepcut")
  tokens_list <- lapply(tokens, function(token) {
        paste(token, collapse = " ")
  })
  unlist(tokens_list)
}
train_token <- train %>%
  mutate(text2 = tokenize_thai(text))
# Create a recipe
recipe <- recipe(sentiment~ text2, data = train_token) %>%
  step_tokenize(text2) %>%
 # step_tokenfilter(text2, min_times = 0) %>%
  step_tfidf(text2)

# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
glimpse(processed_data)


logit<-logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

 fit<- logistic_reg(mixture=1, penalty=0.1) %>%
  set_engine("glm") %>%
  set_mode("classification") %>%
  fit(sentiment~. , data=processed_data)
plot(fit %>% extract_fit_engine())
pred<-predict(fit,processed_data)





# Load pythainlp and other required modules
pythainlp <- import("pythainlp")
string <- import("string")

# Custom function to simplify Thai words
normalize_thai_word <- function(word) {
  # Replace your custom normalization rules here
  word <- pythainlp$util$normalize(word)
  return(word)
}

# Example Thai text
thai_text <- "ใช้ไพทอนสำหรับประมวลผลข้อความภาษาไทย"

# Tokenize Thai text
tokens <- pythainlp$tokenize$word_tokenize(thai_text, engine = "newmm")

# Normalize Thai tokens
normalized_tokens <- lapply(tokens, normalize_thai_word)

# Print normalized tokens
unlist(normalized_tokens)
