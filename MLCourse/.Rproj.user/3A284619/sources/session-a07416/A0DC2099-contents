---
title: "Polynomial Regression and MARs"
author: "ผศ.ดร.สิวะโชติ ศรีสุทธิยากร"
toc: true
toc-depth: 3
toc-title: สารบัญ
theme: default
---

Multiple regression และ regression โมเดลที่เกี่ยวข้องในบทเรียนก่อนหน้านี้เป็นโมเดลที่อยู่ภายใต้กลุ่มของโมเดลเชิงเส้น (linear model) ที่มีวัตถุประสงค์หลักคือเพื่อเรียนรู้ความสัมพันธ์ภายในข้อมูลที่มีรูปแบบเชิงเส้นตรงเป็นหลัก อย่างไรก็ตามก็อาจมีบางโมเดลที่สามารถใช้เรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้นได้ คือ regression model with interaction อย่างไรก็ตามโมเดล regression ดังกล่าวก็ยังมีความยืดหยุ่นที่ไม่มากเมื่อเปรียบเทียบกับอัลกอริทึมอื่น ๆ ที่มีในปัจจุบัน โดยบทเรียนนี้จะกล่าวถึงอัลกอริทึม multivariate adaptive regression splines (MARs) ที่กล่าวได้ว่าเป็น linear model ประเภทหนึ่งแต่มีความสามารถสูงในการเรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้น นอกจากนี้ยังสามารถใช้ได้ทั้งในปัญหาแบบ regression และ classification

# Polynomial Regression

MARs เป็นโมเดลที่พัฒนาขึ้นโดยมีพื้นฐานมาจาก polynomial regression หัวข้อนี้จึงจะกล่าวถึงมโนทัศน์ของ polynomial regression ก่อนเพื่อเป็นพื้นฐานในการทำความเข้าใจ MARs ในหัวข้อถัดไป

## Basic concept

โมเดลการถดถอยพหุนาม (polynomial regression models) เป็นโมเดลการถดถอยแบบเชิงเส้นที่ใช้วิเคราะห์หรือเรียนรู้ความสัมพันธ์ในข้อมูลแบบที่ไม่ใช่เชิงเส้นตรง โดยใช้ฟังก์ชันพหุนาม (polynomial function) เป็นส่วน systematic part ของโมเดลแทนการใช้ฟังก์ชันเชิงเส้นตรงธรรมดา ลองพิจารณาความสัมพันธ์

```{r message = F}
library(ggplot2)
library(dplyr)
set.seed(123)
x<-runif(1000,0,8)
y<-rnorm(1000, sin(x), 0.5) 
data <- data.frame(x,y)
data %>% ggplot()+
  geom_point(aes(x=x, y=y),col="steelblue", alpha=0.6)+
  theme_light()
```

จะเห็นว่าความสัมพันธ์ข้างต้นมีลักษณะเป็นเส้นโค้ง การ fit ความสัมพันธ์ดังกล่าวด้วย regression model สามารถทำได้หลายวิธีการ วิธีการแรกคือการใช้ linear model เหมือนเดิมแต่มีการเพิ่ม term พหุนามที่มีดีกรีต่าง ๆ สมการถดถอยพหุนามมีสมการทั่วไปดังนี้

$$
y_i = \beta_0+\beta_1 x_i + \beta_2 x^2_i + \beta_3 x^3_i + ...+ \beta_p x^p_i + \epsilon_i
$$

คำสั่งต่อไปนี้แสดงการ fit สมการถดถอยพหุนาม degree 2,3 และ 4 กับข้อมูลข้างต้น (<https://en.wikipedia.org/wiki/Degree_of_a_polynomial>)

```{r}
library(tidyr)
linear.fit <- lm(y~x, data = data)
poly2.fit <- lm(y~x + I(x^2), data = data)
poly3.fit <- lm(y~x+I(x^2)+I(x^3), data=data)
poly4.fit <- lm(y~x+I(x^2)+I(x^3)+I(x^4), data=data)
summary(poly2.fit)
summary(poly3.fit)
summary(poly4.fit)

data %>% bind_cols(linear = predict(linear.fit),
                   quadratic = predict(poly2.fit),
                   cubic = predict(poly3.fit),
                   quartic = predict(poly4.fit)) %>%
  gather(linear:quartic, key = "model", value = "pred") %>%
  mutate(model = factor(model, levels=c("linear",
                                        "quadratic",
                                        "cubic",
                                        "quartic"))) %>%
  ggplot(aes(x=x, y=y))+
  geom_point(col = "steelblue", alpha=0.7)+
  geom_line(aes(y=pred))+
  facet_wrap(vars(model))
```

ปัญหาของ polynomial regression คือ multicollinearity

```{r message=F}
library(car)
vif(poly2.fit)
vif(poly3.fit)
```

การแก้ปัญหา multicollinearity สามารถทำได้หลายวิธีการ วิธีการแรกคือการ centering ตัวแปรอิสระ ดังนี้

```{r message=F}
data %>%bind_cols(
  pred2 = data %>%
  mutate(x_c = x-mean(x)) %>%
  lm(y~x_c+I(x_c^2),data=.) %>%
   predict(.)
  )%>%
    ggplot(aes(x=x,y=y))+
    geom_point(col="steelblue",alpha=0.6)+
    geom_line(aes(y=pred2))

data %>%
  mutate(x_c = x-mean(x)) %>%
  lm(y~x_c+I(x_c^2),data=.) %>% 
  vif()

data %>%
  mutate(x_c = x-mean(x)) %>%
  lm(y~x_c+I(x_c^2)+I(x_c^3),data=.) %>% 
  vif()
```

อีกวิธีการหนึ่งคือการแปลงตัวแปรอิสระที่มี degree ของโมเดลให้เป็นด้วยพหุนามเชิงตั้งฉาก (orthogonal polynomial) <http://home.iitk.ac.in/~shalab/regression/Chapter12-Regression-PolynomialRegression.pdf>

การสร้างเทอมพหุนามเชิงตั้งฉากใน R สามารถทำได้โดยใช้ฟังก์ชัน `poly()` ดังนี้

```{r}
poly2.fit <- lm(y~poly(x,2), data=data)
summary(poly2.fit)
data %>% bind_cols(pred = predict(poly2.fit)) %>%
  ggplot(aes(x=x, y=y))+
  geom_point()+
  geom_line(aes(y=pred), col="orange", linewidth = 1.5)
```

## Polynomial Regression in tidymodels

```{r}
library(tidymodels)
dat <- read.csv("https://raw.githubusercontent.com/ssiwacho/2758688_ML/main/week%201/TeacherSalaryData.csv")
head(dat)
set.seed(1234)
split <- initial_split(dat, prop = 0.8)
train <- training(split)
test <- testing(split)
train<-train %>%
  mutate(salary = log(salary))
```

### pre-processing

```{r eval=F}
preproc <- recipe(salary ~ ., data= train) %>%
  step_select(-X)%>%
  step_poly(yrs.service, yrs.since.phd, degree = tune()) %>%
  step_dummy(rank, discipline, sex)
```

### model specification

```{r}
poly_mod <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
```

### set workflow and tuning

```{r message=F, echo=F, eval=F}
poly_workflow <- workflow() %>%
  add_recipe(preproc) %>%
  add_model(poly_mod)

folds <- vfold_cv(train, v=5, repeats=2)
params = parameters(degree(range=c(1,4)),
                    penalty(),
                    mixture())
my_grid <- grid_max_entropy(params, size=30)
library(doMC)
registerDoMC(cores = parallel::detectCores())
tune_results <- poly_workflow %>% 
  tune_grid(resamples = folds,
            grid = my_grid,
            control = control_grid(verbose = T,
                                   save_pred = T)
            )
tune_results %>% autoplot()
mybest<-show_best(tune_results, n=1, metric = "rsq")
```

```{r echo=F, eval=F}
lastfit <- poly_workflow %>%
  finalize_workflow(mybest) %>%
  last_fit(split)


lastfit %>% collect_metrics()
lastfit %>% collect_predictions() %>%
  ggplot(aes(x=salary, y=.pred))+
  geom_point()
```

### Syntax ในคาบเรียน

```{r}
library(ggplot2)
library(dplyr)
set.seed(123)
x<-runif(1000,0,8)
y<-rnorm(1000, sin(x), 0.5) 
data <- data.frame(x,y)
data %>% ggplot()+
  geom_point(aes(x=x, y=y),col="steelblue", alpha=0.6)+
  theme_light()

## method 1
data<-data %>%
  mutate(x2 = x^2,
         x3 = x^3)
head(data)

fit<-lm(y~x+x2+x3, data=data)
summary(fit)
pred.poly<-predict(fit)
head(pred.poly)
data$pred3<-pred.poly
head(data)
data %>% ggplot(aes(x=x))+
  geom_point(aes(y=y),col="steelblue", alpha=0.6)+
  geom_line(aes(y=pred3),col="orange",linewidth=1.5)+
  theme_light()

## method 2
fit<-lm(y~x+I(x^2)+I(x^3), data=data)
summary(fit)
pred.poly<-predict(fit)
head(pred.poly)
data$pred3<-pred.poly
head(data)
data %>% ggplot(aes(x=x))+
  geom_point(aes(y=y),col="steelblue", alpha=0.6)+
  geom_line(aes(y=pred3),col="orange",linewidth=1.5)+
  theme_light()

data %>% ggplot(aes(x = x, y= x2))+
  geom_point()+
  geom_smooth(method="lm")
library(car)
vif(fit)


## method 3
data<-data %>% 
  mutate(x_c = x-mean(x),#centering
         x_c2 = x_c^2,
         x_c3 = x_c^3
         ) 

fit<-lm(y~x_c+x_c2+x_c3, data=data)
summary(fit)
vif(fit)

## method 4: orthoginal polynomial

fit<-lm(y~poly(x,3), data=data)
pred.poly<-predict(fit)
head(pred.poly)
data$pred_orthogonal<-pred.poly
head(data)
data %>% ggplot(aes(x=x))+
  geom_point(aes(y=y),col="steelblue", alpha=0.6)+
  geom_line(aes(y=pred3),col="orange",linewidth=1.5)+
  geom_line(aes(y=pred_orthogonal),col="maroon",linewidth=1.5)+
  theme_light()
```

## Multivariate Adaptive Regression Spline (MARs)

Multivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinear relationships in the data by assessing cutpoints (*knots*) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). (XXX, 2020)

ฟังก์ชันทำนายของ MARs มีความแตกต่างจาก regression model ทั่วไป กล่าวคือจะมีการแบ่งส่วน domain ของ feature เป็นช่วง ๆ จุดที่ใช้แบ่งส่วนเรียกว่า knot จากนั้นทำการ fit regression model ให้กับแต่ละส่วนด้วยโมเดลที่แตกต่างกัน

ลองพิจารณารูป A อัลกอริทึมจะเริ่มจากการหา knot ที่เหมาะสมที่สุดจาก domain ของ feature จากนั้นประมาณค่าพารามิเตอร์ในโมเดลดังนี้

![](images/image-1750132179.png){width="60%"}

เมื่อได้ knot แรกแล้วอัลกอริทึมจะหา knot ต่อไปดังรูป B

![](images/image-31836073.png){width="70%"}

![](images/image-1029173891.png)

### MARs using tidymodels
