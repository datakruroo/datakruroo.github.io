# Install the required packages
install.packages("tm")
install.packages("text2vec")
install.packages("e1071")

# Load the required libraries
library(tm)
library(text2vec)
library(e1071)

# Read positive and negative sentiment files
pos_lines <- readLines("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/pos.txt", encoding = "UTF-8")
neg_lines <- readLines("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/neg.txt", encoding = "UTF-8")

# Create labels for positive and negative sentiment
pos_labels <- rep("pos", length(pos_lines))
neg_labels <- rep("neg", length(neg_lines))

# Combine the sentiment data and labels
sentiment_data <- c(pos_lines, neg_lines)
sentiment_labels <- c(pos_labels, neg_labels)

# Create a text corpus
corpus <- VCorpus(VectorSource(sentiment_data))

# Preprocess the text data
corpus_clean <- tm_map(corpus, content_transformer(tolower))
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)

# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus_clean)

# Split the data into training and test sets
set.seed(123)
train_indices <- sample(1:nrow(dtm), 0.8 * nrow(dtm))
test_indices <- setdiff(1:nrow(dtm), train_indices)

train_dtm <- dtm[train_indices, ]
test_dtm <- dtm[test_indices, ]
train_labels <- sentiment_labels[train_indices]
test_labels <- sentiment_labels[test_indices]

train_dtm_df <- as.data.frame(as.matrix(train_dtm))
test_dtm_df <- as.data.frame(as.matrix(test_dtm))
# Train the Naive Bayes Classifier
classifier <- naiveBayes(train_dtm_df, as.factor(train_labels))

# Test the classifier
predicted_labels <- predict(classifier, newdata = test_dtm_df)

# Calculate the accuracy
accuracy <- mean(predicted_labels == test_labels)
print(paste("Accuracy:", accuracy))



{r}
split <- initial_split(lexicon, prop = 0.8)
train <- training(split)
test <- testing(split)

{r}
rec <- recipe(sentiment~word, data=train) %>%
  step_string2factor(all_outcomes()) %>%
  step_tokenize(word) %>%
  step_tokenfilter(word, min_times = 5) %>%
  step_tfidf(word) %>%
  step_normalize(all_numeric_predictors())

logistic_mod <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

sentiment_workflow <- workflow() %>%
  add_recipe(rec) %>%
  add_model(logistic_mod)

sentiment_fit <- fit(sentiment_workflow, data = train)


{r}
test_rec <- rec %>%
  prep(test) %>%
  bake(test)
test_rec %>% glimpse()

{r}

sentiment_preds <- predict(sentiment_fit, new_data = test_rec) %>%
  bind_cols(test_rec)
sentiment_preds

{r}

sentiment_preds %>%
  conf_mat(truth = factor(sentiment), 
           estimate = .pred_class)


user_reviews <- readr::read_tsv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv")

user_reviews %>%
  count(grade) %>%
  ggplot(aes(grade, n)) +
  geom_col(fill = "midnightblue", alpha = 0.7)


## not run here
user_reviews %>%
  filter(grade > 8) %>%
  sample_n(5) %>%
  pull(text)


reviews_parsed <- user_reviews %>%
  mutate(text = str_remove(text, "Expand$")) %>%
  mutate(rating = case_when(
    grade > 7 ~ "good",
    TRUE ~ "bad"
  ))

words_per_review <- reviews_parsed %>%
  unnest_tokens(word, text) %>%
  count(user_name, name = "total_words")

words_per_review %>%
  ggplot(aes(total_words)) +
  geom_histogram(fill = "midnightblue", alpha = 0.8)


set.seed(123)
review_split <- initial_split(reviews_parsed, strata = rating)
review_train <- training(review_split)
review_test <- testing(review_split)


library(textrecipes)

review_rec <- recipe(rating ~ text, data = review_train) %>%
  step_tokenize(text) %>%
  step_stopwords(text) %>%
  step_tokenfilter(text, max_tokens = 500) %>%
  step_tfidf(text) %>%
  step_normalize(all_predictors())

review_prep <- prep(review_rec)


lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")

lasso_wf <- workflow() %>%
  add_recipe(review_rec) %>%
  add_model(lasso_spec)

lasso_wf

lambda_grid <- grid_regular(penalty(), levels = 40)

set.seed(123)
review_folds <- bootstraps(review_train, strata = rating)
review_folds


doParallel::registerDoParallel()

set.seed(2020)
lasso_grid <- tune_grid(
  lasso_wf,
  resamples = review_folds,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, ppv, npv)
)

lasso_grid %>%
  collect_metrics()

lasso_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5, show.legend = FALSE) +
  facet_wrap(~.metric) +
  scale_x_log10()

best_auc <- lasso_grid %>%
  select_best("roc_auc")

best_auc

final_lasso <- finalize_workflow(lasso_wf, best_auc)

final_lasso
library(vip)
final_lasso %>%
  last_fit(review_split) %>%
  extract_fit_engine() %>%
  vi(lambda = best_auc$penalty) %>%
  group_by(Sign) %>%
  top_n(20, wt = abs(Importance)) %>%
  ungroup() %>%
  mutate(
    Importance = abs(Importance),
    Variable = str_remove(Variable, "tfidf_text_"),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Sign, scales = "free_y") +
  labs(y = NULL)
