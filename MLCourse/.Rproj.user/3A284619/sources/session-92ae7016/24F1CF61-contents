---
title: "Hyperparameter Tuning using Keras Tuner"
---

การเรียนรู้เชิงลึก (Deep Learning) เป็นแขนงย่อยของศาสตร์ด้านการเรียนรู้ของเครื่อง (machine learning) ที่พัฒนาขึ้นโดยใช้การทำงานของเซลล์ประสาทในสมองที่เรียกว่า neuron เป็นต้นแบบ เรียกโมเดลการเรียนรู้ดังกล่าวว่า โมเดลโครงข่ายประสาทเทียม (artificial neuron network models: ANNs)

![ความแตกต่างระหว่าง ML กับ DL](https://editor.analyticsvidhya.com/uploads/21745d3.png)

# ชุดข้อมูลที่ใช้เป็นตัวอย่าง

ชุดข้อมูลที่ใช้เป็นตัวอย่างจะใช้ชุดข้อมูล [**Sign Language Digits Dataset**](https://github.com/ardamavi/Sign-Language-Digits-Dataset)

### Details of datasets:

-   Image size: 100 x 100 pixels

-   Color space: RGB

-   Number of classes: 10 (Digits: 0-9)

-   Number of participant students: 218

-   Number of samples per student: 10

```{r eval=F}
system("git clone https://github.com/ardamavi/Sign-Language-Digits-Dataset.git")
```

```{r eval=F}
dir("~/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/Deeplearning/sign")
```

การนำเข้าไฟล์ข้อมูลทั้งสองสามารถทำได้โดยใช้ library `numpy` ซึ่งสามารถทำบน R ผ่าน reticulate library ดังนี้

```{r eval=F}
library(reticulate)
np <- import("numpy")
x_array <- np$load("sign/X.npy")
y_array <- np$load("sign/Y.npy")
```

ชุดข้อมูลดังกล่าวประกอบด้วยภาพภาษามือของตัวเลข 10 ตัว ตั้งแต่ 0, 1, 2, ..., 9 จำนวน 2,062 ภาพ โดยที่แต่ละภาพมีขนาด 64 x 64 pixels

```{r eval=F}
dim(x_array)
dim(y_array)

```

ในเบื้องต้นจะลอง train โมเดลเพื่อจำแนกภาษามือสำหรับตัวเลข 0 กับ 1 ก่อน เมื่อพิจารณาใน `y_array`

```{r eval=F}
y_array %>% head()
```

บทเรียนนี้จะกล่าวถึงการปรับแต่งค่า hyperparameters ในโมเดลโครงข่ายประสาทเทียม (neural network models) ซึ่งเป็นขั้นตอนสำคัญของการพัฒนาโมเดลการเรียนรู้เชิงลึก hyperparameters ของ neural network models มีหลายตัวได้แก่ จำนวน hidden layer จำนวน node ภายในแต่ละ layer ระดับของ learning rate และ activation function ที่เลือกใช้

```{r eval=F}
library(reshape2)
# Helper function to convert image data to a data frame for ggplot2
prepare_image_data <- function(image_data) {
  df <- melt(image_data)
  colnames(df) <- c("x", "y", "value")
  return(df)
}

# Prepare image data for plotting
image_1 <- prepare_image_data(x_array[206,,])
image_2 <- prepare_image_data(x_array[4,,])

# Plot images using ggplot2
p1 <- ggplot(image_1, aes(x = y, y = -x, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  coord_fixed() +
  theme_minimal() +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank())

p1
p2 <- ggplot(image_2, aes(x = y, y = -x, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black") +
  coord_fixed() +
  theme_minimal() +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank())

# Display the plots
gridExtra::grid.arrange(p1, p2, ncol = 2)

```

# 1. Setup

ก่อนการดำเนินการใด ๆ ผู้วิเคราะห์จำเป็นต้องติดตั้งและเตรียมสภาพแวดล้อมการทำงานให้พร้อมก่อน ได้แก่ การติดตั้ง **`tensorflow`**, **`keras`** และ **`keras-tuner`** ดังนี้

```{r eval=F}
pip install tensorflow
pip install keras
pip install keras-tuner
```

จากนั้นติดตั้งและเรียก reticulate

```{r echo=F, eval=F}
#install.packages("reticulate")
library(reticulate)
```

# 2. Import Python library

```{r eval=F}
np <- import("numpy")
tf <- import("tensorflow")
keras_tuner <- import("keras_tuner")
keras <- import("keras")
layers <- keras$layers
RandomSearch <- import("kerastuner.tuners")$RandomSearch
HyperParameters <- import("kerastuner.engine.hyperparameters")$HyperParameters
```

# 3. Load the data

นำข้อมูลเข้าและจัดกระทำข้อมูลก่อนการวิเคราะห์

```{r message=F, eval=F}
library(tidymodels)
dat <- read.csv("criminal.csv")
dat <- dat %>% drop_na()
dat <- recipe(TheifperPop ~., data = dat) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep(NULL) %>%
  juice()
split <- initial_split(dat)
train <- training(split)
test <- testing(split)
## convert training dataset to train_x and train_y matrix
train_x <- train %>% select(-TheifperPop) %>% as.matrix()
train_y <- train %>% select(TheifperPop) %>% as.matrix()
test_x <- test %>% select(-TheifperPop) %>% as.matrix()
test_y <- test %>% select(TheifperPop) %>% as.matrix()
```

# 4. Model Specification

```{r, eval=F}
library(keras)
# Build the model function
build_model <- function(hp) {
  model <- keras_model_sequential() %>%
    layer_dense(units = hp$Int("units_1", min_value = 32L, max_value = 512L, step = 32L), 
                activation = "relu", input_shape = 17L) %>%
    layer_dense(units = hp$Int("units_2", min_value = 32L, max_value = 512L, step = 32L), 
                activation = "relu") %>%
    layer_dense(units = 1L)

  model %>% compile(
    optimizer = optimizer_adam(
      learning_rate = hp$Float("learning_rate", min_value = 1e-4, max_value = 1e-2, sampling = "LOG")
    ),
    loss = "mse",
    metrics = "mse"
  )

  return(model)
}

# Configure the tuner
RandomSearch <- keras_tuner$tuners$RandomSearch

tuner <- RandomSearch(
  build_model,
  objective = "val_mse",
  max_trials = 20L,
  executions_per_trial = 2L,
  directory = "tuning_results",
  project_name = "test_keras_tuner"
)

# Prepare the data
# Replace the following lines with your own dataset
#train_x <- matrix(runif(1107 * 17), nrow = 1107) %>% class()
#train_y <- matrix(runif(1107), nrow = 1107)
#test_x <- matrix(runif(369 * 17), nrow = 369)
#test_y <- matrix(runif(369), nrow = 369)

# Set early stopping
early_stopping <- keras$callbacks$EarlyStopping(monitor = "val_loss", patience = 3L)

# Search for the best hyperparameters
result<-tuner$search(
  train_x,
  train_y,
  validation_data = list(test_x, test_y),
  epochs = 10L,
  batch_size = 256L,
  callbacks = list(early_stopping)
)
```
