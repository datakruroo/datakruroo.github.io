"0","# defined model"
"0","def build_model(hp):"
"0","    model = keras.Sequential()"
"0","    model.add(layers.Flatten(input_shape=(64, 64)))"
"0","    "
"0","    # Tune the number of hidden layers"
"0","    hp_num_layers = hp.Int('num_layers', min_value=1, max_value=2, step=1)"
"0","    for i in range(hp_num_layers):"
"0","        # Tune the number of units in the Dense layer"
"0","        hp_units = hp.Int('units_' + str(i), min_value=600, max_value=3000, step=200)"
"0","        "
"0","        # Tune the L1 regularization factor"
"0","        hp_l1 = hp.Float('l1_' + str(i), min_value=0, max_value=0.1, step=0.01)"
"0","        # Tune the L2 regularization factor"
"0","       # hp_l2 = hp.Float('l2_' + str(i), min_value=0, max_value=0.1, step=0.01)"
"0","        model.add(layers.Dense(units=hp_units, kernel_regularizer=keras.regularizers.l1(l1=hp_l1)))"
"0","        model.add(layers.Activation('relu'))"
"0","        "
"0","        # Tune the dropout rate"
"0","        #hp_dropout = hp.Float('dropout_' + str(i), 0, 0.5, step=0.1)"
"0","        #model.add(layers.Dropout(hp_dropout))"
"0","       "
"0","    model.add(layers.Dense(10, activation='softmax'))"
"0","    "
"0","    # Tune the learning rate"
"0","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])"
"0","    "
"0","    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),"
"0","                  loss='categorical_crossentropy',"
"0","                  metrics=['accuracy'])"
"0","    "
"0","    return model"
"0",""
