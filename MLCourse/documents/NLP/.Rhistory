### --- model specification
lasso_spec <- multinom_reg(penalty = tune(),
mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
wf<-workflow() %>%
add_recipe(recipe_obj) %>%
add_model(lasso_spec)
library(tidyverse)
library(reticulate)
pythainlp <- import("pythainlp")
# python stopword script
stopword_fn <-"
def stopword():
import pythainlp
stopword = pythainlp.corpus.common.thai_stopwords()
stopword = list(stopword)
return stopword
"
py_run_string(stopword_fn)
py$stopword() %>% head(20)
pythainlp$corpus$thai_negations
pythainlp$corpus$thai_syllables
# negations dictionary
negations_fn <-"
def negations():
import pythainlp
stopword = pythainlp.corpus.common.thai_negations()
stopword = list(stopword)
return stopword
"
# syllables dictionary
syllables_fn<-"
def syllables():
import pythainlp
syllable = pythainlp.corpus.common.thai_syllables()
syllable = list(syllable)
return(syllable)
"
py_run_string(negations_fn)
py_run_string(syllables_fn)
py$negations() %>% head(20)
py$syllables() %>% head(20)
library(tidyverse)
library(readxl)
# importing data
dat <- read_excel("traindataset_hw.xlsx")
glimpse(dat,60)
dat$result3 %>% table()
hw3 <- dat %>% select(`รหัสนิสิต`,result3, text3)
head(hw3)
# define tokenization function
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
head(temp$token,10)
trie <- pythainlp$tokenize$Trie
## custom word
custom_word <- readLines("mytext.txt", skipNul = TRUE)
## load thai dictionary from pythainlp
dict_default <- pythainlp$corpus$thai_words()
extended_dict <- trie(pythainlp$corpus$thai_words())
for (word in custom_word) {
extended_dict$add(word)
}
trie <- pythainlp$tokenize$Trie
## custom word
custom_word <- readLines("mytext.txt", skipNul = TRUE)
## load thai dictionary from pythainlp
dict_default <- pythainlp$corpus$thai_words()
extended_dict <- trie(pythainlp$corpus$thai_words())
for (word in custom_word) {
extended_dict$add(word)
}
# define tokenization function
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm",
custom_dict = extended_dict)
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
head(temp$token,10)
library(tidytext)
temp<-temp %>%
unnest_tokens(input = token, output = word, token = "ptb") %>%
filter(!word %in% py$stopword()) %>%
#filter(!word %in% py$negations()) %>%
filter(!word %in% py$syllables())
temp %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".",""))%>%
group_by(result3,word) %>%
count() %>%
arrange(desc(n)) %>%
head(20) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col(aes(fill = factor(result3,
levels=c(0,1,2),
labels=c("ผิด","ถูกบางส่วน","ถูก"))))+
theme(text=element_text(family="ChulaCharasNew"))+
labs(fill = "ผลการตอบ")
#install.packages("wordcloud2")
library(wordcloud2)
word_count<-temp %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".",""))%>%
group_by(result3,word) %>%
count()
col<-ifelse(word_count[,1]==0,"maroon",
ifelse(word_count[,1]==1,"orange","steelblue"))
wordcloud2(data = word_count[,-1], size = 3, fontFamily = "ChulaCharasNew",
color = col, backgroundColor = "white",
shape = "square")
### ----
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm",
custom_dict = extended_dict)
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".","")) %>%
filter(!word %in% py$stopword()) %>%
#filter(!word %in% py$negations()) %>%
filter(!word %in% py$syllables()) %>%
mutate(result3 = factor(result3, levels=c(0,1,2),
labels=c("ผิด","ถูก","ถูก")))
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_adasyn(result3)
library(themis)
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_adasyn(result3)
library(textrecipes)
### ----
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm",
custom_dict = extended_dict)
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".","")) %>%
filter(!word %in% py$stopword()) %>%
#filter(!word %in% py$negations()) %>%
filter(!word %in% py$syllables()) %>%
mutate(result3 = factor(result3, levels=c(0,1,2),
labels=c("ผิด","ถูก","ถูก")))
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_adasyn(result3)
library(tidymodels)
### --- model specification
lasso_spec <- multinom_reg(penalty = tune(),
mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
fold<-vfold_cv(temp, v=10)
?tune_grid
eval_metric <- metric_set(accuracy, sens, spec)
library(doMC)
registerDoMC(cores=15)
result <- wf %>%
tune_grid(resamples = fold,
grid = 30,
metrics = eval_metric
)
wf<-workflow() %>%
add_recipe(recipe_obj) %>%
add_model(lasso_spec)
registerDoMC(cores=15)
result <- wf %>%
tune_grid(resamples = fold,
grid = 30,
metrics = eval_metric
)
autoplot(result)
knn_spec <- nearest_neighbor(neighbors = tune(),
weight_func = tune())%>%
set_engine("kknn") %>%
set_mode("classification")
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_normalize(all_numeric_predictors()) %>%
step_adasyn(result3)
### --- model specification
lasso_spec <- multinom_reg(penalty = tune(),
mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
knn_spec <- nearest_neighbor(neighbors = tune(),
weight_func = tune())%>%
set_engine("kknn") %>%
set_mode("classification")
wf<- workflow_set(
preproc = list(recipe_obj),
models = list(lasso_spec, knn_spec)
)
result <- wf %>%
workflow_map(
resamples = fold,
grid = 30,
metrics = eval_metric
)
autoplot(result)
autoplot(result) %>% collect_metrics()
result %>% collect_metrics()
result %>% collect_metrics() %>%
filter(.metric == "sens") %>%
arrange(desc(mean))
result %>% collect_metrics() %>%
filter(.metric == "spec") %>%
arrange(desc(mean))
result %>% autoplot()
result %>% autoplot() +
theme(legend.position = "top")
result %>% autoplot() +
theme(legend.position = "top")+
scale_y_continuous(breaks=seq(0,1,0.1))
wf
wf[2,1]
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(neighbors, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(weight, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(factor(weight), mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(factor(weight), mean, color = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(factor(weight), mean, color = .metric))
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(factor(weight_func), mean, color = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(factor(weight_func), mean, color = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(factor(weight_func), mean, fill = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(factor(weight_func), mean, fill = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)+
coord_flip()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(x=factor(weight_func),y= mean, fill = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)+
coord_flip()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(neighbors, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(x=factor(weight_func),y= mean, fill = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)+
coord_flip()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +
geom_col(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +
geom_boxplot(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric)
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +
geom_boxplot(size = 1, show.legend = FALSE) +
facet_wrap(~.metric)
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +
geom_boxplot(size = 0.5, show.legend = FALSE) +
facet_wrap(~.metric)
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(neighbors, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(neighbors, mean, color = .metric)) +
geom_boxplot(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
e
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(neighbors, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(neighbors, mean, color = .metric)) +
geom_line(size = 1, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
collect_metrics() %>%
ggplot(aes(neighbors, mean, color = .metric)) +
geom_line(size = 1, show.legend = FALSE) +
geom_point()+
facet_wrap(~.metric) +
scale_x_log10()
dat <- read_excel("traindataset_hw.xlsx")
glimpse(dat,50)
ืnames(dat)
ืnames(dat)
names(dat)
names(dat)[3]<-"student_id"
names(dat)[4]<-"student_name"
names(dat)[5]<-"section"
install.packages("crfsuite")
library(crfsuite)
crf_spec <- parsnip::set_engine(
parsnip::linear_reg(),
"crfsuite",
pkg = "crfsuite",
mode = "classification"
)
setwd("~/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/NLP")
pattern <- "ตัวอย่าง"
text <- "นี่คือข้อความตัวอย่าง"
match_position <- regexpr(pattern, text)
match_position
pattern <- "^ตัวอย่าง"
text <- "นี่คือข้อความตัวอย่าง"
match_position <- regexpr(pattern, text)
match_position
str_match(pattern, text)
?str_match
str_match(text, pattern)
strings <- c(" 219 733 8965", "329-293-8753 ", "banana", "595 794 7569",
"387 287 6718", "apple", "233.398.9187  ", "482 952 3315",
"239 923 8115 and 842 566 4692", "Work: 579-499-7527", "$1000",
"Home: 543.355.3679")
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
str_match(strings, phone)
text <- "นี่คือข้อความ ตัวอย่าง"
str_match(text, pattern)
text <- c("นี่คือข้อความ ตัวอย่าง","ตัวอย่าง มันเป็นอย่างงี้")
match_position <- regexpr(pattern, text)
match_position
pattern <- "ตัวอย่าง"
text <- c("นี่คือข้อความ ตัวอย่าง","ตัวอย่าง มันเป็นอย่างงี้")
match_position <- regexpr(pattern, text)
match_position
str_match(text, pattern)
text <- c("นี่คือข้อควาตัวอย่าง","ตัวอย่าง มันเป็นอย่างงี้")
match_position <- regexpr(pattern, text)
match_position
str_match(text, pattern)
pattern <- "^ตัวอย่าง"
text <- c("นี่คือข้อควาตัวอย่าง","ตัวอย่าง มันเป็นอย่างงี้")
match_position <- regexpr(pattern, text)
match_position
str_match(text, pattern)
text <- c("นี่คือข้อควา ตัวอย่าง","ตัวอย่าง มันเป็นอย่างงี้")
str_match(text, pattern)
reticulate::repl_python()
