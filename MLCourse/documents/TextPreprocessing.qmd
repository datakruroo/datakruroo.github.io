---
title: "Text PreProcessing"
---

# Prerequired

เนื้อหาในบทความนี้จะกล่าวถึงการจัดระเบียบและจัดกระทำข้อมูลแบบข้อความด้วย R และ library PyThaiNLP ของ Python ก่อนการดำเนินการผู้วิเคราะห์จำเป็นต้องดำเนินการติดตั้งโปรแกรมและ library ที่จำเป็นก่อนดังนี้

1.  ติดตั้ง R และ RStudio
2.  ติดตั้ง Python และดำเนินการตั้งค่าตามขั้นตอนในลิงค์ <https://support.posit.co/hc/en-us/articles/360023654474-Installing-and-Configuring-Python-with-RStudio>
3.  ติดตั้ง package reticulate บน R โดยพิมคำสั่ง `install.packages("reticulate")` บน R console
4.  ติดตั้ง PyThaiNLP โดยเข้าไปที่หน้า Terminal ใน RStudio จากนั้นพิมพ์คำสั่ง `pip install pythainlp`

เมื่อดำเนินการในขั้นตอนทั้ง 4 เรียบร้อยแล้ว ขั้นตอนต่อมาคือการเรียกใช้ library pythainlp บน R โดยพิมพ์คำสั่งดังนี้

```{r message = F}
library(tidyverse)
library(reticulate)
pythainlp <- import("pythainlp")
```

**เอกสารอ้างอิง**

-   [คู่มือการใช้งาน PyThaiNLP](https://pythainlp.readthedocs.io/en/latest/#pythainlp)
-   <https://towardsdatascience.com/analysis-of-presidential-speeches-throughout-american-history-bb088d36d7dd>

# Introduction

การทำ text preprocessing เป็นขั้นตอนสำคัญสำหรับการดำเนินการวิเคราะห์ข้อความ วัตถุประสงค์ของการดำเนินงานในขั้นตอนนี้คือเพื่อ (1) จัดระเบียบและทำความสะอาดข้อความแบบข้อความ เพื่อสร้างชุดข้อมูลที่มีโครงสร้างจากข้อความดิบ และ (2) จัดกระทำข้อมูลเพื่อให้อยู่ในรูปแบบที่มีความสอดคล้องกับการวิเคราะห์ตามวัตถุประสงค์

การทำ text preprocessing มีเทคนิคเฉพาะหลายตัวซึ่งผู้เรียนจำเป็นต้องรู้จักไว้ การเลือกใช้เทคนิคต่าง ๆ ขึ้นอยู่กับความต้องการของเทคนิคการวิเคราะห์ที่เลือกใช้ หรือสภาพ/ปัญหาที่พบในข้อมูลแต่ละชุด เทคนิคดังกล่าวมีดังนี้

1.  **Lowercasing:** คือการแปลงตัวอักษรตัวใหญ่ให้เป็นตัวเล็ก (ใช้ในภาษาอังกฤษ)
2.  **Tokenization:** คือการแปลงข้อความออกเป็นส่วนย่อย ๆ เช่น คำ/วลี หรือประโยค ที่สามารถนำไปวิเคราะห์ได้อย่างมีความหมาย ส่วนย่อยของข้อความดังกล่าวเรียกว่า tokens
3.  **Stopword removal:** stopword คือคำที่ไม่ให้ความหมายหรือไม่ได้มีประโยชน์ในการตีความหมายของข้อความ โดยมากมักเป็นคำเชื่อมหรือคำลงท้ายที่เขียนเพื่อให้ประโยคมีความสมบูรณ์มากขึ้น เช่น a, is, are, and, the หรือในภาษาไทย เช่น ที่ ซึ่ง อัน และ
4.  **Stemming:** เป็นเทคนิคการแปลงประเภทหนึ่ง มีวัตถุประสงค์เพื่อลดรูปคำที่พบในข้อความให้กลับไปเป็นรากศัพท์ (root words) ของคำ ๆ นั้น เช่น running, runs หรือ ran ทั้งหมดนี้มาจาก stem เดียวกันคือ run ดังนั้นการทำ stemming จะช่วยลด noise ที่เกิดขึ้นโดยจะจัดกลุ่มคำที่มีความหมายเหมือนกันดังกล่าวให้กลายเป็นคำเดียวกัน
5.  **Lemmatization:** เป็นเทคนิคที่คล้ายกับ stemming แต่มีความซับซ้อนกว่า กล่าวคือเป็นการแปลง/ลดรูปคำที่พบโดยใช้บริบทของข้อความและการวิเคราะห์ morphological (morphological analysis) ซึ่งช่วยให้คำที่ผ่านกระบวนการดังกล่าวจะมีความหมาย และสอดคล้องกับบริบทของข้อความ
6.  **Removing special character and punctuation:** เป็นการคัดกรองสัญลักษณ์หรืออักขระที่ไม่จำเป็นออกไปจากการวิเคราะห์ เป็นเทคนิคที่ดำเนินการเพื่อลด noise ออกจากข้อมูล
7.  **Removing HTML tags and URLs:** ในกรณีที่ข้อมูลถูก scrapping จาก website การวิเคราะห์ข้อความดังกล่าวอาจจะต้องมีการลบ HTML tags หรือ URL รวมทั้ง syntax อื่น ๆ ที่ติดมากับข้อความด้วยก่อนดำเนินการวิเคราะห์
8.  **Spell checking and correction:** เนื่องจากข้อความอาจมีการพิมพ์ผิดพลาดหรือคลาดเคลื่อนมาจากต้นทางของข้อมูล การแก้ไขคำผิดดังกล่าวจะช่วยให้การวิเคราะห์/นำเสนอความหมายสามารถทำได้อย่างถูกต้องและคงเส้นคงวามากขึ้น
9.  **n-grams:** คือคำภายในข้อความที่ต่อเนื่องกันเป็นลำดับจำนวน n คำ การดึงคำแบบ n-gram ขึ้นมาจะช่วยให้ได้ข้อมูลเกี่ยวกับความหมายและบริบทของข้อความนั้นได้ดีขึ้น

ในเชิงปฏิบัติผู้วิเคราะห์จะต้องเป็นผู้พิจารณาว่าจะเลือกใช้เทคนิคใดบ้างมาทำ text preprocessing ในงานของตนเอง ต้องอย่าลืมว่ากระบวนการ text preprocessing เป็นกระบวนการทวนซ้ำที่อาจจะต้องใช้เวลาและลองผิดลองถูกหลายรอบจนกว่าจะได้ผลการวิเคราะห์ที่มีความสมบูรณ์ นอกจากนี้เทคนิคดังกล่าวยังอาจมีการใช้งานที่แตกต่างกันในภาษาต่าง ๆ ที่ผู้วิเคราะห์ทำงานด้วย ส่วนที่เหลือของบทความนี้จะกล่าวถึงการใช้เทคนิคข้างต้นนการทำ text preprocessing โดยเน้นการใช้งานสำหรับภาษาไทย รายละเอียดมีดังนี้

# Tokenization

การตัดคำ (tokenization) เป็นเทคนิคพื้นฐานที่สำคัญสำหรับการทำ text preprocessing ในการประมวลผลภาษาธรรมชาติ (NLP) กระบวนการนี้เป็นส่วนสำคัญที่สุดส่วนหนึ่งเพราะเป็นการแปลงข้อมูลข้อความที่ไม่มีโครงสร้าง (unstructure) ให้เป็นข้อมูลที่มีโครงสร้าง (structure) การแบ่งส่วนของข้อความออกเป็นส่วนย่อย เช่น คำ (words) วลี (phrases) หรือประโยค (sentences) ที่มีความหมาย ทั้งนี้การเลือกว่าควรใช้การแบ่งคำลักษณะไหนขึ้นอยู่กับบริบทของข้อความ และความละเอียดของผลการวิเคราะห์ที่ต้องการ tokenization อาจจำแนกได้เป็น 3 ประเภท ได้แก่

1.  Word tokenization
2.  Sentence tokenization
3.  Subword tokenization

## Word tokenization

work tokenization หรือการตัดคำเป็นเทคนิคการแบ่งข้อความออกเป็นคำ ซึ่งสามารถทำได้หลายลักษณะ ทั้งการตัดคำด้วยเครื่องหมายวรรคตอนหรืออักขระที่กำหนด หรือการตัดคำด้วยอัลกอริทึม การตัดคำเป็นเทคนิคที่ใช้อย่างมากในกระบวนการวิเคราะห์ข้อความ เช่น การวิเคราะห์ sentiment หรือการจำแนกข้อความ

การตัดคำภาษาไทยสามารถทำได้อย่างมีประสิทธิภาพมากขึ้นด้วย libraryPyThaiNLP ที่ถูกพัฒนาขึ้นบนภาษา Python และสามารถเรียกใช้ได้บนภาษา R ผ่าน package reticulate โดยฟังก์ชันที่ใช้สำหรับตัดคำคือ `word_tokenize()`

ฟังก์ชัน `word_tokenize()` มีพารามิเตอร์ที่สามารถกำหนดเพื่อปรับเปลี่ยนการทำงานได้หลายตัว พารามิเตอร์จำเป็นได้แก่ `text` เป็นข้อความนำเข้าสำหรับฟังก์ชัน ส่วนพารามิเตอร์ `engine` ใช้สำหรับกำหนดอัลกอริทึมเพื่อตัดคำภายในประโยคที่กำหนด อัลกอริทึมที่แตกต่างกันมีผลให้การตัดคำที่ได้อาจมีความแตกต่างกัน ค่าเริ่มต้นของพารามิเตอร์นี้คือ `engine = "newmm"` เป็นวิธีการตัดคำโดยพยายามให้คำที่ตัดมีความสอดคล้องกับคำภายในพจนานุกรมภาษาไทยมากที่สุด ส่วนวิธีการอื่น ๆ ที่สามารถกำหนดได้ เช่น `longest`, `icu` หรือ `deepcut` เป็นต้น อีกพารามิเตอร์หนึ่งที่มีประโยชน์มากคือ `keep_whitespace` โดยมีค่าเริ่มต้นเท่ากับ `True` ซึ่งหมายถึงการเก็บเว้นวรรคไว้ในผลการตัดคำ ในกรณีที่ผู้วิเคราะห์ต้องการให้นำเครื่องหมายเว้นวรรคทั้งหมดออกจากผลการตัดคำให้กำหนดพารามิเตอร์นี้เป็น `False`

ตัวอย่างต่อไปนี้แสดงการใช้ฟังก์ชันดังกล่าวเพื่อตัดคำในประโยคที่กำหนด

```{r}
text <- "ส่วนเบี่ยงเบนมาตรฐานให้วัดการกระจายสัมบูรณ์ แต่สัมประสิทธิ์การแปรผันใช้วัดการกระจายสัมพัทธ์"

tokennized_text <- pythainlp$word_tokenize(text)
tokennized_text
```

ตัวอย่างต่อไปนี้แสดงผลการตัดคำด้วยฟังก์ชัน `word_tokenize()` ที่มีการกำหนดพารามิเตอร์ต่าง ๆ

```{r}
#default
pythainlp$word_tokenize(text)
# longest engine
pythainlp$word_tokenize(text, engine = "longest")
# keep whitespace = False
pythainlp$word_tokenize(text, engine = "longest", 
                        keep_whitespace = F)
# use deepcut and keep whitespace = False
pythainlp$word_tokenize(text, engine = "deepcut", 
                        keep_whitespace = F)
```

ผลการตัดคำข้างต้นจะเห็นว่าการใช้อัลกอริทึม `newmm` และ `deepcut` มีแนวโน้มที่จะได้คำที่มีความหมายถูกต้องมากที่สุด อย่างไรก็ตามจะเห็นว่ามีคำศัพท์เฉพาะที่มีการตัดคำที่ยังไม่ถูกต้อง ได้แก่ ส่วนเบี่ยงเบนมาตรฐาน การกระจายสัมบูรณ์ สัมประสิทธิ์การแปรผัน และการกระจายสัมพัทธ์ ในกรณีนี้ผู้วิเคราะห์สามารถเพิ่มคำศัพท์เฉพาะที่ใช้สำหรับประกอบการตัดคำได้ดังนี้

```{r}
trie <- pythainlp$tokenize$Trie
## custom word
custom_word <- c("ส่วนเบี่ยงเบนมาตรฐาน",
                 "การกระจายสัมบูรณ์",
                 "สัมประสิทธิ์การแปรผัน",
                 "การกระจายสัมพัทธ์")
## load thai dictionary from pythainlp
dict_default <- pythainlp$corpus$thai_words()

extended_dict <- trie(pythainlp$corpus$thai_words())
for (word in custom_word) {
  extended_dict$add(word)
}

```

```{r}
pythainlp$word_tokenize(text, custom_dict = extended_dict)
```

จากตัวอย่างข้างต้นจะเห็นว่า การกำหนด dictionary เพิ่มเติมทำให้ผู้วิเคราะห์สามารถตัดคำ/ดึงคำจากข้อความที่มีความหมายสอดคล้องกับบริบทของการวิเคราะห์ได้มากขึ้น

## Sentences tokenization

เป็นเทคนิคการแบ่งข้อความออกเป็นประโยคหรือวลีย่อย โดยอาจใช้เว้นวรรคหรือเครื่องหมายจบประโยคต่าง ๆ เป็นตัวแบ่ง การทำ tokenization ลักษณะนี้เหมาะสำหรับงานวิเคราะห์ที่ต้องการสรุปความ (text summarization) ที่โครงสร้างของประโยคมีความสำคัญ

การทำ sentences tokenization ด้วย PyThaiNLP สามารถทำได้ด้วยฟังก์ชัน `sent_tokenize()` ทั้งนี้ก่อนการใช้ฟังก์ชันดังกล่าวผู้วิเคราะห์จำเป็นต้องติดตั้ง library `python-crfsuite` ก่อนโดยพิมพ์คำสั่งต่อไปนี้ใน terminal ของ R Studio

```{terminal eval=F}
pip install python-crfsuite
```

ฟังก์ชัน `sent_tokenize()` มีพารามิเตอร์ที่สำคัญ 2 ตัว ได้แก่ `text` เป็นข้อความนำเข้าสำหรับฟังก์ชัน ส่วนพารามิเตอร์ `engine` ใช้สำหรับกำหนดอัลกอริทึมเพื่อแบ่งประโยคจากข้อความที่กำหนด โดยมี 2 อัลกอริทึมให้เลือกได้แก่ 'whitespace' กับ 'whitespace+newline' ซึ่งเป็นการแบ่งประโยคด้วยช่องว่าง และการเว้นบรรทัด

```{r}
pythainlp$sent_tokenize(text, engine = "whitespace")

mytext <- readLines("mytext.txt")
temp<-pythainlp$sent_tokenize(text = mytext[1],engine = "whitespace+newline")
temp
```

# Stopword removal

Stopwords คือคำที่ไม่ให้ความหมายหรือไม่มีประโยชน์ในการตีความหมายของประโยคหรือข้อความเป้าหมาย เช่น a, and, the, และ หรือ อะ ซึ่งในชุดข้อมูลหากมีคำประเภทนี้อยู่มาก ๆ จะกลายเป็น noise ที่รบกวนการวิเคราะห์ ใน library pythainlp มีการรวบรวม stopwords สำหรับภาษาไทยเอาไว้พอสมควร ผู้วิเคราะห์สามารถนำมาใช้ได้ การเรียก stopword จาก pythinlp มาใช้สามารถทำได้ในทำนองเดียวกับการตัดคำดังนี้

```{r}
stopword <- pythainlp$corpus$thai_stopwords
stopword
```

อย่างไรก็ตาม stopword ข้างต้นอยู่ใน format แบบ frozenset ซึ่งไม่สามารถนำมาใช้บน R ได้โดยตรง การนำ stopword ดังกล่าวมาใช้บน R สามารถเขียนคำสั่งเพิ่มเติมได้ดังนี้

```{r}
# python stopword script
stopword_fn <-"
def stopword():
  import pythainlp
  stopword = pythainlp.corpus.common.thai_stopwords()
  stopword = list(stopword)
  return stopword
"
py_run_string(stopword_fn)
```

ผลลัพธ์ที่ได้จากการเรียก stopword จากฟังก์ชันใน python script ด้วย `py_run_string()` จะเก็บอยู่ใน object ชื่อ `py` ซึ่งจะเก็บในชื่อเดียวกับฟังก์ชันดังนี้

```{r}
py$stopword() %>% head(20)
pythainlp$corpus$thai_negations
pythainlp$corpus$thai_syllables
```

นอกจาก stopword แล้ว pythainlp ยังมี dictionary ตัวอื่น ๆ ที่สามารถนำมาใช้ประโยชน์ได้ ได้แก่ พจนานุกรมคำปฏิเสธ `thai_negations` และพจนานุกรมพยางค์ `thai_syllables` ดังตัวอย่างต่อไปนี้

```{r}
# negations dictionary
negations_fn <-"
def negations():
  import pythainlp
  stopword = pythainlp.corpus.common.thai_negations()
  stopword = list(stopword)
  return stopword
"
# syllables dictionary
syllables_fn<-"
def syllables():
  import pythainlp
  syllable = pythainlp.corpus.common.thai_syllables()
  syllable = list(syllable)
  return(syllable)
"
py_run_string(negations_fn)
py_run_string(syllables_fn)
py$negations() %>% head(20)
py$syllables() %>% head(20)
```

# วิเคราะห์คำตอบอัตนัยของผู้เรียน

ตัวอย่างส่วนนี้ผู้วิเคราะห์จะพัฒนาโมเดลตรวจคำตอบการบ้านของนิสิต ข้อมูลฝึกหัดได้จากผลการตอบการบ้านของนิสิตจำนวน 74 คน ในรายวิชา 2758501 ภาคปลาย ปีการศึกษา 2565 เรื่องการวิเคราะห์การถดถอย

```{r message=F}
library(tidyverse)
library(readxl)
# importing data
dat <- read_excel("traindataset_hw.xlsx")
glimpse(dat,60)
```

คำถามที่จะนำมาทดลองพัฒนาโมเดลคือ "สมการถดถอยที่ประมาณได้มีความเหมาะสมที่จะใช้ทำนาย/อธิบายความสัมพันธ์ที่พบในข้อมูลหรือไม่ เพราะเหตุใด" แนวคำตอบของคำถามนี้คือ "เหมาะสม ด้วยเหตุผล 2 ประการ ประการแรก คือไม่พบหลักฐานว่ามีการละเมิดข้อตกลงเบื้องต้นของการวิเคราะห์การถดถอย และประการที่สอง ค่าสัมประสิทธิ์การตัดสินใจของสมการถดถอยที่ประมาณได้มีค่าเท่ากับ 0.697 แสดงว่าสมการสามารถอธิบายความผันแปรใน mathach ได้คิดเป็นร้อยละ 69.7 ซึ่งอยู่ในระดับที่สูง"

การให้คะแนนมีการให้คะแนน 3 ระดับ คือ 0, 1, 2 หมายถึงตอบผิด ตอบถูกบางส่วน และตอบถูกทั้งหมด การวิเคราะห์ด้านล่างแสดงการแจกแจงความถี่ของคะแนนที่ได้ซึ่งพบว่า ส่วนใหญ่สามารถตอบถูกได้บางส่วน และมีส่วนน้อยที่สามารถตอบถูกได้ทั้งหมด

```{r}
dat$result3 %>% table()
```

## data preprocessing

ผู้วิเคราะห์สร้างชุดข้อมูลใหม่สำหรับพัฒนาโมเดลก่อนดังนี้

```{r}
hw3 <- dat %>% select(`รหัสนิสิต`,result3, text3)
head(hw3)
```

ขั้นตอนต่อมาจะทำ tokenization เพื่อแบ่งคำจากข้อความที่นิสิตตอบมา

```{r}
# define tokenization function
tokenize_thai <- function(text) {
  tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
  tokens_list <- lapply(tokens, function(x) {
                              paste(x, collapse = " ")})
  tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-hw3 %>%
  mutate(token = tokenize_thai(text3))
head(temp$token,10)
```

จะสังเกตเห็นว่าการตัดคำด้วย dictionary ของ pythainlp สามารถทำได้ดีพอสมควร แต่คำศัพท์ทางสถิติที่แปลเป็นภาษาไทยยังไม่สามารถตัดคำได้อย่างเหมาะสม ดังนั้นเพื่อให้การตัดคำทำได้อย่างมีประสิทธิภาพมากขึ้น ผู้วิเคราะห์จึงจะเพิ่มคำศัพท์ทางสถิติที่เกี่ยวข้องไว้ใน dictionary โดยเก็บคำศัพท์ดังกล่าวไว้ในไฟล์ `mytext.txt`

```{r}
trie <- pythainlp$tokenize$Trie
## custom word
custom_word <- readLines("mytext.txt", skipNul = TRUE)
## load thai dictionary from pythainlp
dict_default <- pythainlp$corpus$thai_words()

extended_dict <- trie(pythainlp$corpus$thai_words())
for (word in custom_word) {
  extended_dict$add(word)
}
```

จากนั้นลองดำเนินการตัดคำใหม่ด้วย dictionary ข้างต้น

```{r}
# define tokenization function
tokenize_thai <- function(text) {
  tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm",
                   custom_dict = extended_dict)
  tokens_list <- lapply(tokens, function(x) {
                              paste(x, collapse = " ")})
  tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-hw3 %>%
  mutate(token = tokenize_thai(text3))
head(temp$token,10)
```

สำรวจข้อมูลเบื้องต้น

```{r}
library(tidytext)
temp<-temp %>%
  unnest_tokens(input = token, output = word, token = "ptb") %>%
  filter(!word %in% py$stopword()) %>%
  #filter(!word %in% py$negations()) %>%
  filter(!word %in% py$syllables())

temp %>% 
  # remove all numbers
  mutate(word = str_replace_all(word, "\\d+", "")) %>%
  # remove all english words
  mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
  # correct word
  mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
  #mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
  #mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
  # remove punctuation
  mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
  mutate(word = str_remove(word," ")) %>%
  # remove unnescessary words
  filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
                      "บางประการ","ทั้งสอง")) %>%
  filter(!word %in% c(".",""))%>%
  group_by(result3,word) %>%
  count() %>%
  arrange(desc(n)) %>%
  head(20) %>%
  ggplot(aes(x = n, y= reorder(word,n)))+
  geom_col(aes(fill = factor(result3,
                             levels=c(0,1,2),
                             labels=c("ผิด","ถูกบางส่วน","ถูก"))))+
  theme(text=element_text(family="ChulaCharasNew"))+
  labs(fill = "ผลการตอบ")
```

```{r}
#install.packages("wordcloud2")
library(wordcloud2)
word_count<-temp %>% 
  # remove all numbers
  mutate(word = str_replace_all(word, "\\d+", "")) %>%
  # remove all english words
  mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
  # correct word
  mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
  #mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
  #mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
  # remove punctuation
  mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
  mutate(word = str_remove(word," ")) %>%
  # remove unnescessary words
  filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
                      "บางประการ","ทั้งสอง")) %>%
  filter(!word %in% c(".",""))%>%
  group_by(result3,word) %>%
  count() 

col<-ifelse(word_count[,1]==0,"maroon",
            ifelse(word_count[,1]==1,"orange","steelblue"))
wordcloud2(data = word_count[,-1], size = 3, fontFamily = "ChulaCharasNew",
           color = col, backgroundColor = "white",
           shape = "square")
```

</br>

จะเห็นว่าคำตอบของผู้เรียนมีแนวโน้มที่จะกล่าวถึงข้อตกลงเบื้องต้นของการวิเคราะห์การถดถอยว่ามีความเหมาะสมอย่างไร

ผลการวิเคราะห์ข้างต้นยังทำให้ผู้วิเคราะห์ทำความสะอาดข้อมูลไปได้ระดับหนึ่ง ขั้นตอนต่อไปคือจะนำข้อมูลที่จัดระเบียบและทำความสะอาดนี้ไปพัฒนาโมเดลตรวจคำตอบต่อไป

## Training Classification models

การวิเคราะห์ในส่วนนี้เป็นการพัฒนาโมเดลทำนาย เพื่อตรวจคำตอบแบบข้อเขียนของนักเรียน การพัฒนาโมเดลประกอบด้วยขั้นตอนได้แก่ (1) การนำเข้าและจัดการข้อมูลก่อนการวิเคราะห์​ (2) การแบ่งส่วนชุดข้อมูล (3) การกำหนดและระบุโมเดลทำนาย (4) การปรับแต่งค่า hyperparameters ด้วยเทคนิค cross-validation (5) การประเมินประสิทธิภาพของโมเดล และ (6) การนำโมเดลไปใช้ รายละเอียดมีดังนี้

### 1.  การนำเข้าและจัดการข้อมูลก่อนการวิเคราะห์

ชุดข้อมูลที่ใช้พัฒนาโมเดลคือ `traindataset_hw.xlsx` สามารถนำเข้าได้ดังนี้

```{r}
dat <- read_excel("traindataset_hw.xlsx")
names(dat)
```

เปลี่ยนชื่อคอลัมน์บางตัวเพื่อให้สะดวกในการเรียกใช้งาน

```{r}
names(dat)[3]<-"student_id"
names(dat)[4]<-"student_name"
names(dat)[5]<-"section"
```

ผู้วิเคราะห์จะดำเนินการจัดการข้อมูลทั้งชุดก่อนตามขั้นตอนที่ได้ทำในการสำรวจข้อมูลเบื้องต้น รายละเอียดมีดังนี้

```{r}
### ---- define tokenization function for Thai language
tokenize_thai <- function(text) {
  tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm",
                   custom_dict = extended_dict)
  tokens_list <- lapply(tokens, function(x) {
                              paste(x, collapse = " ")})
  tokens_list <- unlist(tokens_list)
return(tokens_list)
}
### ---- python stopword script
stopword_fn <-"
def stopword():
  import pythainlp
  stopword = pythainlp.corpus.common.thai_stopwords()
  stopword = list(stopword)
  return stopword
"
### ---- syllables dictionary
syllables_fn<-"
def syllables():
  import pythainlp
  syllable = pythainlp.corpus.common.thai_syllables()
  syllable = list(syllable)
  return(syllable)
"
```

คำสั่งต่อไปนี้ใช้จัดเตรียมข้อมูล training dataset ทั้งหมด ทั้งนี้ผู้วิเคราะห์ได้ตรวจคำตอบตามแนวคำตอบที่กำหนดไว้แล้ว ผลการตรวจอยู่ใน column `result3`

```{r}
q3_dat <- dat %>% select(student_id, result3, text3)

```

```{r message=F}
library(tidymodels)
library(themis)
library(textrecipes)

## tokenized via mutate function
temp<-hw3 %>%
  mutate(word = tokenize_thai(text3)) %>%
  # remove all numbers
  mutate(word = str_replace_all(word, "\\d+", "")) %>%
  # remove all english words
  mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
  # correct word
  mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
  #mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
  #mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
  # remove punctuation
  mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
  mutate(word = str_remove(word," ")) %>%
  # remove unnescessary words
  filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
                      "บางประการ","ทั้งสอง")) %>%
  filter(!word %in% c(".","")) %>%
    filter(!word %in% py$stopword()) %>%
  #filter(!word %in% py$negations()) %>%
  filter(!word %in% py$syllables()) %>%
  mutate(result3 = factor(result3, levels=c(0,1,2),
                          labels=c("ผิด","ถูก","ถูก")))
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
  step_tokenize(word, token = "ptb") %>%
  step_tfidf(word) %>%  
  step_normalize(all_numeric_predictors()) %>%
  step_adasyn(result3)

### --- model specification
lasso_spec <- multinom_reg(penalty = tune(),
                           mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")
knn_spec <- nearest_neighbor(neighbors = tune(),
                             weight_func = tune())%>%
  set_engine("kknn") %>%
  set_mode("classification")


wf<- workflow_set(
  preproc = list(recipe_obj),
  models = list(lasso_spec, knn_spec)
)
  

fold<-vfold_cv(temp, v=10)
eval_metric <- metric_set(accuracy, sens, spec)
library(doMC)
registerDoMC(cores=15)
result <- wf %>%
  workflow_map(
            resamples = fold,
            grid = 30,
            metrics = eval_metric
            )
result %>% autoplot() +
  theme(legend.position = "top")+
  scale_y_continuous(breaks=seq(0,1,0.1))
```

```{r}
result %>%
  extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
  collect_metrics() %>%
  ggplot(aes(neighbors, mean, color = .metric)) +
  geom_line(size = 1, show.legend = FALSE) +
  geom_point()+
  facet_wrap(~.metric) +
  scale_x_log10()

result %>%
  extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
  collect_metrics() %>%
  ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +
  geom_boxplot(size = 0.5, show.legend = FALSE) +
  facet_wrap(~.metric)


```

last_fit

```{r}

```
