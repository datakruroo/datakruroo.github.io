paste(token, collapse = " ")
})
unlist(tokens_list)
}
train_token <- train %>%
mutate(text2 = tokenized_thai(text))
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize)
tokens_list <- lapply(tokens, function(token) {
paste(token, collapse = " ")
})
unlist(tokens_list)
}
train_token <- train %>%
mutate(text2 = tokenize_thai(text))
train_token %>% head()
# Create a recipe
recipe <- recipe(~ text2, data = train_tokenized) %>%
step_tokenize(text2) %>%
step_tokenfilter(text2, min_times = 1) %>%
step_tfidf(text2)
# Create a recipe
recipe <- recipe(~ text2, data = train_token) %>%
step_tokenize(text2) %>%
step_tokenfilter(text2, min_times = 1) %>%
step_tfidf(text2)
# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
# Preprocess the data
processed_data
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize)
# tokens_list <- lapply(tokens, function(token) {
#    paste(token, collapse = " ")
# })
unlist(tokens_list)
}
train_token <- train %>%
mutate(text2 = tokenize_thai(text))
train_token
# Create a recipe
recipe <- recipe(~ text2, data = train_token) %>%
step_tokenize(text2) %>%
step_tokenfilter(text2, min_times = 1) %>%
step_tfidf(text2)
train_token <- train %>%
mutate(text2 = tokenize_thai(text))
head(train_token)
?paste
tokens <- lapply(train$text, pythainlp$word_tokenize)
tokens
lapply(tokens, function(token) {
paste(token, collapse = " ")
)
lapply(tokens, function(token) {
paste(token, collapse = " ")})
## defined custom tokenization function
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize)
tokens_list <- lapply(tokens, function(token) {
paste(token, collapse = " ")
})
unlist(tokens_list)
}
train_token <- train %>%
mutate(text2 = tokenize_thai(text))
# Create a recipe
recipe <- recipe(~ text2, data = train_token) %>%
step_tokenize(text2) %>%
step_tokenfilter(text2, min_times = 1) %>%
step_tfidf(text2)
# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
processed_data
glimpse(processed_data)
# Create a recipe
recipe <- recipe(~ text2, data = train_token) %>%
step_tokenize(text2) %>%
step_tokenfilter(text2, min_times = 0) %>%
step_tfidf(text2)
# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
glimpse(processed_data)
glimpse(train_token)
# Create a recipe
recipe <- recipe(sentiment~ text2, data = train_token) %>%
step_tokenize(text2) %>%
step_tokenfilter(text2, min_times = 0) %>%
step_tfidf(text2)
# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
glimpse(processed_data)
logit<-logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
fit<- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(sentiment~. , data=processed_data)
fit
summary(fit)
pred<-predict(train_token)
pred<-predict(fit,train_token)
pred<-predict(fit,processed_data)
pred
table(pred$.pred_class, processed_data$sentiment)
table(processed_data$sentiment)
fit<- logistic_reg(mixture=1) %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(sentiment~. , data=processed_data)
pred<-predict(fit,processed_data)
pred
table(pred$.pred_class)
processed_data$sentiment
train%>%filter(sentiment=="pos")
train_token <- train %>%
mutate(text2 = tokenize_thai(text))
# Create a recipe
recipe <- recipe(sentiment~ text2, data = train_token) %>%
step_tokenize(text2) %>%
# step_tokenfilter(text2, min_times = 0) %>%
step_tfidf(text2)
## defined custom tokenization function
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine = "deepcut")
tokens_list <- lapply(tokens, function(token) {
paste(token, collapse = " ")
})
unlist(tokens_list)
}
train_token <- train %>%
mutate(text2 = tokenize_thai(text))
# Create a recipe
recipe <- recipe(sentiment~ text2, data = train_token) %>%
step_tokenize(text2) %>%
# step_tokenfilter(text2, min_times = 0) %>%
step_tfidf(text2)
# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
glimpse(processed_data)
fit<- logistic_reg(mixture=1) %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(sentiment~. , data=processed_data)
pred<-predict(fit,processed_data)
pred
pred$.pred_class%>%table()
fit<- logistic_reg(mixture=1, penalty=0.01) %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(sentiment~. , data=processed_data)
plot(fit)
plot(fit %>% extract_fit_engine())
plot(fit %>% extract_fit_engine())
pred<-predict(fit,processed_data)
pred
table(pred$.pred_class)
fit<- logistic_reg(mixture=1, penalty=0.1) %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(sentiment~. , data=processed_data)
fit
# Create a recipe
recipe <- recipe(sentiment~ text2, data = train_token) %>%
step_tokenize(text2)
# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
processed_data
glimpse(processed_data)
?step_tfidf
# Create a recipe
recipe <- recipe(sentiment~ text2, data = train_token) %>%
step_tokenize(text2)
# Create a recipe
recipe <- recipe(sentiment~ text2, data = train_token) %>%
step_tokenize(text2) %>%
# step_tokenfilter(text2, min_times = 0) %>%
step_tfidf(text2)
# Preprocess the data
processed_data <- recipe %>% prep() %>% bake(train_token)
glimpse(processed_data)
train
glimpse(train)
train %>%
mutate(token = tokenize_thai(text))
train[1,"token"]
train[1,]
train<-train %>%
mutate(token = tokenize_thai(text))
train[1,"token"]
## defined custom tokenization function
tokenize_thai <- function(text){
tokens_list <- paste(lapply(text, tokenize_thai_sentence),
collapse = " ")
return(tokens_list)
}
## tokenized via mutate function
train<-train %>%
mutate(token = tokenize_thai(text))
train
head(train)
glimpse(train)
## defined custom tokenization function
tokenize_thai <- function(text){
tokens_list <- paste(lapply(text, tokenize_thai_sentence),
collapse = " ")
token_list <- unlist(token_list)
return(tokens_list)
}
train<-train %>%
mutate(token = tokenize_thai(text))
library(reticulate)
## importing data
tcas <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/tcas61.csv", header=F, sep="\t",col.names = c("text","sentiment"))
glimpse(tcas, width=80)
library(tidyverse)
library(textrecipes)
library(tidymodels)
split<-initial_split(tcas, prop = 0.7, strata = sentiment)
train<-training(split)
test<-testing(split)
dim(train)
glimpse(train, width=80)
glimpse(train, width=60)
library(reticulate)
# load tokenize_thai_sentence function from python script
source_python("token.py")
## defined custom tokenization function
tokenize_thai <- function(text){
tokens_list <- paste(lapply(text, tokenize_thai_sentence),
collapse = " ")
token_list <- unlist(token_list)
return(tokens_list)
}
## tokenized via mutate function
train<-train %>%
mutate(token = tokenize_thai(text))
tokenize_thai <- function(text){
tokens_list <- paste(lapply(text, tokenize_thai_sentence),
collapse = " ")
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
train<-train %>%
mutate(token = tokenize_thai(text))
train[1,"token"]
train[2,"token"]
## defined custom tokenization function
tokenize_thai <- function(text){
tokens <- lapply(text, tokenize_thai_sentence)
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
train<-train %>%
mutate(token = tokenize_thai(text))
glimpse(train)
train[1,"token"]
train[2,"token"]
## tokenized via mutate function
train<-train %>%
mutate(token = tokenize_thai2(text))
# Import pythainlp library
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai2 <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize)
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
train<-train %>%
mutate(token = tokenize_thai2(text))
glimpse(train)
pythainlp$corpus$common$thai_stopwords %>% head()
pythainlp$corpus$common$thai_stopwords()
pythainlp$corpus$common$thai_stopwords() %>% class()
pythainlp$corpus$common$thai_stopwords() %>% list()
pythainlp$corpus$common$thai_stopwords() %>% list() %>% class()
pythainlp$corpus$common$thai_stopwords() %>% unlist()
pythainlp$corpus$common$thai_stopwords() ->temp
temp[1]
glimpse(temp)
glimpse(temp%>%unlist())
py_to_r(temp)
thai_stopwords_py <- pythainlp$corpus$common$thai_stopwords()
thai_stopwords_r <- py_to_r(thai_stopwords_py)
thai_stopwords_r
thai_stopwords_r[1]
thai_stopwords_py <- pythainlp$corpus$common$thai_stopwords()
thai_stopwords_py
class(thai_stopwords_py)
list(thai_stopwords_py)
thai_stopwords_py <- pythainlp$corpus$common$thai_stopwords()
thai_stopwords_r <- py_to_r(thai_stopwords_py)
thai_stopwords_r
?unnest_tokens
train %>%
unnest_tokens(token = "words", input=token, output=word)
library(tidytext)
train %>%
unnest_tokens(token = "words", input=token, output=word)
train <- train %>%
unnest_tokens(token = "words", input=token, output=word)
recipe(~ text, data = train) %>%
step_tokenize(text) %>%
prep(NULL) %>%
juice()
train <- train %>%
unnest_tokens(token = "words", input=token, output=word)
glimpse(train)
train %>%
filter(!word %in% thai_stopwords_py)
thai_stopwords_py <- pythainlp$corpus$common$thai_stopwords()
# Create a Python frozenset
py_set <- py_eval("frozenset(['apple', 'banana', 'cherry'])")
py_set
# Convert frozenset to R vector
r_vector <- py_to_r(py_set)
r_vector
# Print the R vector
print(r_vector)
py_list <- py_eval("list(py_set)", py_set = py_set)
# Create a Python frozenset
py_set <- py_eval("frozenset(['apple', 'banana', 'cherry'])")
py_list <- py_eval("list(py_set)", py_set = py_set)
py_list
py_list <- py_eval("list(py_set)")
# Create a Python frozenset
py_set <- py_eval("frozenset(['apple', 'banana', 'cherry'])")
py_list <- py_eval("list(py_set)")
# Assuming you have a frozenset object in Python
py_set <- py_eval("frozenset(['apple', 'banana', 'cherry'])")
# Convert the frozenset to a list in Python using a custom function
py_list <- py_run_string("def frozenset_to_list(py_set): return list(py_set)")$frozenset_to_list(py_set)
# Convert the Python list to an R vector
r_vector <- py_to_r(py_list)
py_list
py_set$list()
# Assuming you have a frozenset object in Python
py_set <- py_eval("frozenset(['apple', 'banana', 'cherry'])")
py_set$list()
py_list <- py_set$list()
py_eval("pythainlp.corpus.common.thai_stopword")
py_eval("pythainlp.corpus.common.thai_stopword()")
py_eval("list(pythainlp.corpus.common.thai_stopwords())")
py_eval("import(pythainlp) list(pythainlp.corpus.common.thai_stopwords())")
thai_stopwords <- py_to_r(py_eval("list(pythainlp.corpus.common.thai_stopwords())"))
thai_stopwords_py <- pythainlp$corpus$common$thai_stopwords()
thai_stopwords <- py_to_r(thai_stopwords_py)
thai_stopwords
thai_stopwords_py <- pythainlp$corpus$common$thai_stopwords()
py_eval("list(thai_stopwords_py")
?py_eval
py_eval("list(thai_stopwords_py)")
# load tokenize_thai_sentence function from python script
source_python("token.py")
reticulate::repl_python()
# load tokenize_thai_sentence function from python script
source_python("token.py")
stopword
stopword
stopword()
head(stopword())
head(stopword(),10)
glimpse(train)
train %>%
filter(!word %in% stopword())
train
tibble(train)
train %>%
count(word) %>%
arrange(desc(n))
train %>%
count(word) %>%
arrange(desc(n))
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>20)
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>20) %>%
ggplot(aes(x = n, y= word))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>20) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
dim(stopword())
source_python("token.py")
dim(stopword())
length(stopword())
train<-train %>%
filter(!word %in% stopword())
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>20) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train %>%
count(word) %>%
arrange(desc(n)) %>%
#filter(n>20) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train$word
table(train$word)
train<-train %>%
filter(!word %in% c("ง","กก"))
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train<-train %>%
filter(!word %in% c("ง","กก","ล่ะ"))
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col(aes(col=sentiment))+
theme(text=element_text(family="ChulaCharasNew"))
glimpse(train)
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col(aes(col=factor(sentiment)))+
theme(text=element_text(family="ChulaCharasNew"))
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n),
col=sentiment))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train %>%
count(sentiment,word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n),
col=sentiment))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
train %>%
count(sentiment,word) %>%
arrange(desc(n)) %>%
filter(n>5) %>%
ggplot(aes(x = n, y= reorder(word,n),
fill=sentiment))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
# Define a custom tokenization function for textrecipes
tokenize_thai2 <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="longest")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
split<-initial_split(tcas, prop = 0.7, strata = sentiment)
train<-training(split)
test<-testing(split)
dim(train)
glimpse(train, width=60)
## tokenized via mutate function
train<-train %>%
mutate(token = tokenize_thai2(text))
train <- train %>%
unnest_tokens(token = "words", input=token, output=word)
train %>%
count(word) %>%
arrange(desc(n)) %>%
filter(n>20) %>%
ggplot(aes(x = n, y= reorder(word,n)))+
geom_col()+
theme(text=element_text(family="ChulaCharasNew"))
reticulate::repl_python()
