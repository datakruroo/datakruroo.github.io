{
  "hash": "71084f4e8de6311d07b2d1aa0687535a",
  "result": {
    "markdown": "---\ntitle: \"Text PreProcessing\"\n---\n\n\n# Prerequired\n\nเนื้อหาในบทความนี้จะกล่าวถึงการจัดระเบียบและจัดกระทำข้อมูลแบบข้อความด้วย R และ library PyThaiNLP ของ Python ก่อนการดำเนินการผู้วิเคราะห์จำเป็นต้องดำเนินการติดตั้งโปรแกรมและ library ที่จำเป็นก่อนดังนี้\n\n1.  ติดตั้ง R และ RStudio\n2.  ติดตั้ง Python และดำเนินการตั้งค่าตามขั้นตอนในลิงค์ <https://support.posit.co/hc/en-us/articles/360023654474-Installing-and-Configuring-Python-with-RStudio>\n3.  ติดตั้ง package reticulate บน R โดยพิมคำสั่ง `install.packages(\"reticulate\")` บน R console\n4.  ติดตั้ง PyThaiNLP โดยเข้าไปที่หน้า Terminal ใน RStudio จากนั้นพิมพ์คำสั่ง `pip install pythainlp`\n\nเมื่อดำเนินการในขั้นตอนทั้ง 4 เรียบร้อยแล้ว ขั้นตอนต่อมาคือการเรียกใช้ library pythainlp บน R โดยพิมพ์คำสั่งดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(reticulate)\npythainlp <- import(\"pythainlp\")\n```\n:::\n\n\n**เอกสารอ้างอิง**\n\n-   [คู่มือการใช้งาน PyThaiNLP](https://pythainlp.readthedocs.io/en/latest/#pythainlp)\n-   <https://towardsdatascience.com/analysis-of-presidential-speeches-throughout-american-history-bb088d36d7dd>\n\n# Introduction\n\nการทำ text preprocessing เป็นขั้นตอนสำคัญสำหรับการดำเนินการวิเคราะห์ข้อความ วัตถุประสงค์ของการดำเนินงานในขั้นตอนนี้คือเพื่อ (1) จัดระเบียบและทำความสะอาดข้อความแบบข้อความ เพื่อสร้างชุดข้อมูลที่มีโครงสร้างจากข้อความดิบ และ (2) จัดกระทำข้อมูลเพื่อให้อยู่ในรูปแบบที่มีความสอดคล้องกับการวิเคราะห์ตามวัตถุประสงค์\n\nการทำ text preprocessing มีเทคนิคเฉพาะหลายตัวซึ่งผู้เรียนจำเป็นต้องรู้จักไว้ การเลือกใช้เทคนิคต่าง ๆ ขึ้นอยู่กับความต้องการของเทคนิคการวิเคราะห์ที่เลือกใช้ หรือสภาพ/ปัญหาที่พบในข้อมูลแต่ละชุด เทคนิคดังกล่าวมีดังนี้\n\n1.  **Lowercasing:** คือการแปลงตัวอักษรตัวใหญ่ให้เป็นตัวเล็ก (ใช้ในภาษาอังกฤษ)\n2.  **Tokenization:** คือการแปลงข้อความออกเป็นส่วนย่อย ๆ เช่น คำ/วลี หรือประโยค ที่สามารถนำไปวิเคราะห์ได้อย่างมีความหมาย ส่วนย่อยของข้อความดังกล่าวเรียกว่า tokens\n3.  **Stopword removal:** stopword คือคำที่ไม่ให้ความหมายหรือไม่ได้มีประโยชน์ในการตีความหมายของข้อความ โดยมากมักเป็นคำเชื่อมหรือคำลงท้ายที่เขียนเพื่อให้ประโยคมีความสมบูรณ์มากขึ้น เช่น a, is, are, and, the หรือในภาษาไทย เช่น ที่ ซึ่ง อัน และ\n4.  **Stemming:** เป็นเทคนิคการแปลงประเภทหนึ่ง มีวัตถุประสงค์เพื่อลดรูปคำที่พบในข้อความให้กลับไปเป็นรากศัพท์ (root words) ของคำ ๆ นั้น เช่น running, runs หรือ ran ทั้งหมดนี้มาจาก stem เดียวกันคือ run ดังนั้นการทำ stemming จะช่วยลด noise ที่เกิดขึ้นโดยจะจัดกลุ่มคำที่มีความหมายเหมือนกันดังกล่าวให้กลายเป็นคำเดียวกัน\n5.  **Lemmatization:** เป็นเทคนิคที่คล้ายกับ stemming แต่มีความซับซ้อนกว่า กล่าวคือเป็นการแปลง/ลดรูปคำที่พบโดยใช้บริบทของข้อความและการวิเคราะห์ morphological (morphological analysis) ซึ่งช่วยให้คำที่ผ่านกระบวนการดังกล่าวจะมีความหมาย และสอดคล้องกับบริบทของข้อความ\n6.  **Removing special character and punctuation:** เป็นการคัดกรองสัญลักษณ์หรืออักขระที่ไม่จำเป็นออกไปจากการวิเคราะห์ เป็นเทคนิคที่ดำเนินการเพื่อลด noise ออกจากข้อมูล\n7.  **Removing HTML tags and URLs:** ในกรณีที่ข้อมูลถูก scrapping จาก website การวิเคราะห์ข้อความดังกล่าวอาจจะต้องมีการลบ HTML tags หรือ URL รวมทั้ง syntax อื่น ๆ ที่ติดมากับข้อความด้วยก่อนดำเนินการวิเคราะห์\n8.  **Spell checking and correction:** เนื่องจากข้อความอาจมีการพิมพ์ผิดพลาดหรือคลาดเคลื่อนมาจากต้นทางของข้อมูล การแก้ไขคำผิดดังกล่าวจะช่วยให้การวิเคราะห์/นำเสนอความหมายสามารถทำได้อย่างถูกต้องและคงเส้นคงวามากขึ้น\n9.  **n-grams:** คือคำภายในข้อความที่ต่อเนื่องกันเป็นลำดับจำนวน n คำ การดึงคำแบบ n-gram ขึ้นมาจะช่วยให้ได้ข้อมูลเกี่ยวกับความหมายและบริบทของข้อความนั้นได้ดีขึ้น\n\nในเชิงปฏิบัติผู้วิเคราะห์จะต้องเป็นผู้พิจารณาว่าจะเลือกใช้เทคนิคใดบ้างมาทำ text preprocessing ในงานของตนเอง ต้องอย่าลืมว่ากระบวนการ text preprocessing เป็นกระบวนการทวนซ้ำที่อาจจะต้องใช้เวลาและลองผิดลองถูกหลายรอบจนกว่าจะได้ผลการวิเคราะห์ที่มีความสมบูรณ์ นอกจากนี้เทคนิคดังกล่าวยังอาจมีการใช้งานที่แตกต่างกันในภาษาต่าง ๆ ที่ผู้วิเคราะห์ทำงานด้วย ส่วนที่เหลือของบทความนี้จะกล่าวถึงการใช้เทคนิคข้างต้นนการทำ text preprocessing โดยเน้นการใช้งานสำหรับภาษาไทย รายละเอียดมีดังนี้\n\n# Tokenization\n\nการตัดคำ (tokenization) เป็นเทคนิคพื้นฐานที่สำคัญสำหรับการทำ text preprocessing ในการประมวลผลภาษาธรรมชาติ (NLP) กระบวนการนี้เป็นส่วนสำคัญที่สุดส่วนหนึ่งเพราะเป็นการแปลงข้อมูลข้อความที่ไม่มีโครงสร้าง (unstructure) ให้เป็นข้อมูลที่มีโครงสร้าง (structure) การแบ่งส่วนของข้อความออกเป็นส่วนย่อย เช่น คำ (words) วลี (phrases) หรือประโยค (sentences) ที่มีความหมาย ทั้งนี้การเลือกว่าควรใช้การแบ่งคำลักษณะไหนขึ้นอยู่กับบริบทของข้อความ และความละเอียดของผลการวิเคราะห์ที่ต้องการ tokenization อาจจำแนกได้เป็น 3 ประเภท ได้แก่\n\n1.  Word tokenization\n2.  Sentence tokenization\n3.  Subword tokenization\n\n## Word tokenization\n\nwork tokenization หรือการตัดคำเป็นเทคนิคการแบ่งข้อความออกเป็นคำ ซึ่งสามารถทำได้หลายลักษณะ ทั้งการตัดคำด้วยเครื่องหมายวรรคตอนหรืออักขระที่กำหนด หรือการตัดคำด้วยอัลกอริทึม การตัดคำเป็นเทคนิคที่ใช้อย่างมากในกระบวนการวิเคราะห์ข้อความ เช่น การวิเคราะห์ sentiment หรือการจำแนกข้อความ\n\nการตัดคำภาษาไทยสามารถทำได้อย่างมีประสิทธิภาพมากขึ้นด้วย libraryPyThaiNLP ที่ถูกพัฒนาขึ้นบนภาษา Python และสามารถเรียกใช้ได้บนภาษา R ผ่าน package reticulate โดยฟังก์ชันที่ใช้สำหรับตัดคำคือ `word_tokenize()`\n\nฟังก์ชัน `word_tokenize()` มีพารามิเตอร์ที่สามารถกำหนดเพื่อปรับเปลี่ยนการทำงานได้หลายตัว พารามิเตอร์จำเป็นได้แก่ `text` เป็นข้อความนำเข้าสำหรับฟังก์ชัน ส่วนพารามิเตอร์ `engine` ใช้สำหรับกำหนดอัลกอริทึมเพื่อตัดคำภายในประโยคที่กำหนด อัลกอริทึมที่แตกต่างกันมีผลให้การตัดคำที่ได้อาจมีความแตกต่างกัน ค่าเริ่มต้นของพารามิเตอร์นี้คือ `engine = \"newmm\"` เป็นวิธีการตัดคำโดยพยายามให้คำที่ตัดมีความสอดคล้องกับคำภายในพจนานุกรมภาษาไทยมากที่สุด ส่วนวิธีการอื่น ๆ ที่สามารถกำหนดได้ เช่น `longest`, `icu` หรือ `deepcut` เป็นต้น อีกพารามิเตอร์หนึ่งที่มีประโยชน์มากคือ `keep_whitespace` โดยมีค่าเริ่มต้นเท่ากับ `True` ซึ่งหมายถึงการเก็บเว้นวรรคไว้ในผลการตัดคำ ในกรณีที่ผู้วิเคราะห์ต้องการให้นำเครื่องหมายเว้นวรรคทั้งหมดออกจากผลการตัดคำให้กำหนดพารามิเตอร์นี้เป็น `False`\n\nตัวอย่างต่อไปนี้แสดงการใช้ฟังก์ชันดังกล่าวเพื่อตัดคำในประโยคที่กำหนด\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- \"ส่วนเบี่ยงเบนมาตรฐานให้วัดการกระจายสัมบูรณ์ แต่สัมประสิทธิ์การแปรผันใช้วัดการกระจายสัมพัทธ์\"\n\ntokennized_text <- pythainlp$word_tokenize(text)\ntokennized_text\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"ส่วน\"      \"เบี่ยงเบน\"  \"มาตรฐาน\"  \"ให้\"       \"วัด\"       \"การ\"     \n [7] \"กระจาย\"   \"สัมบูรณ์\"    \" \"        \"แต่\"       \"สัมประสิทธิ์\" \"การ\"     \n[13] \"แปรผัน\"    \"ใช้\"       \"วัด\"       \"การ\"      \"กระจาย\"   \"สัมพัทธ์\"   \n```\n:::\n:::\n\n\nตัวอย่างต่อไปนี้แสดงผลการตัดคำด้วยฟังก์ชัน `word_tokenize()` ที่มีการกำหนดพารามิเตอร์ต่าง ๆ\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#default\npythainlp$word_tokenize(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"ส่วน\"      \"เบี่ยงเบน\"  \"มาตรฐาน\"  \"ให้\"       \"วัด\"       \"การ\"     \n [7] \"กระจาย\"   \"สัมบูรณ์\"    \" \"        \"แต่\"       \"สัมประสิทธิ์\" \"การ\"     \n[13] \"แปรผัน\"    \"ใช้\"       \"วัด\"       \"การ\"      \"กระจาย\"   \"สัมพัทธ์\"   \n```\n:::\n\n```{.r .cell-code}\n# longest engine\npythainlp$word_tokenize(text, engine = \"longest\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"ส่วน\"      \"เบี่ยงเบน\"  \"มาตรฐาน\"  \"ให้\"       \"วัด\"       \"การก\"    \n [7] \"ระ\"       \"จา\"       \"ย\"        \"สัมบูรณ์\"    \" \"        \"แต่\"      \n[13] \"สัมประสิทธิ์\" \"การแปร\"   \"ผัน\"       \"ใช้\"       \"วัด\"       \"การก\"    \n[19] \"ระ\"       \"จา\"       \"ย\"        \"สัมพัทธ์\"   \n```\n:::\n\n```{.r .cell-code}\n# keep whitespace = False\npythainlp$word_tokenize(text, engine = \"longest\", \n                        keep_whitespace = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"ส่วน\"      \"เบี่ยงเบน\"  \"มาตรฐาน\"  \"ให้\"       \"วัด\"       \"การก\"    \n [7] \"ระ\"       \"จา\"       \"ย\"        \"สัมบูรณ์\"    \"แต่\"       \"สัมประสิทธิ์\"\n[13] \"การแปร\"   \"ผัน\"       \"ใช้\"       \"วัด\"       \"การก\"     \"ระ\"      \n[19] \"จา\"       \"ย\"        \"สัมพัทธ์\"   \n```\n:::\n\n```{.r .cell-code}\n# use deepcut and keep whitespace = False\npythainlp$word_tokenize(text, engine = \"deepcut\", \n                        keep_whitespace = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"ส่วน\"      \"เบี่ยงเบน\"  \"มาตรฐาน\"  \"ให้\"       \"วัด\"       \"การ\"     \n [7] \"กระจาย\"   \"สัมบูรณ์\"    \"แต่\"       \"สัมประสิทธิ์\" \"การ\"      \"แปร\"     \n[13] \"ผัน\"       \"ใช้\"       \"วัด\"       \"การ\"      \"กระจาย\"   \"สัมพัทธ์\"   \n```\n:::\n:::\n\n\nผลการตัดคำข้างต้นจะเห็นว่าการใช้อัลกอริทึม `newmm` และ `deepcut` มีแนวโน้มที่จะได้คำที่มีความหมายถูกต้องมากที่สุด อย่างไรก็ตามจะเห็นว่ามีคำศัพท์เฉพาะที่มีการตัดคำที่ยังไม่ถูกต้อง ได้แก่ ส่วนเบี่ยงเบนมาตรฐาน การกระจายสัมบูรณ์ สัมประสิทธิ์การแปรผัน และการกระจายสัมพัทธ์ ในกรณีนี้ผู้วิเคราะห์สามารถเพิ่มคำศัพท์เฉพาะที่ใช้สำหรับประกอบการตัดคำได้ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrie <- pythainlp$tokenize$Trie\n## custom word\ncustom_word <- c(\"ส่วนเบี่ยงเบนมาตรฐาน\",\n                 \"การกระจายสัมบูรณ์\",\n                 \"สัมประสิทธิ์การแปรผัน\",\n                 \"การกระจายสัมพัทธ์\")\n## load thai dictionary from pythainlp\ndict_default <- pythainlp$corpus$thai_words()\n\nextended_dict <- trie(pythainlp$corpus$thai_words())\nfor (word in custom_word) {\n  extended_dict$add(word)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npythainlp$word_tokenize(text, custom_dict = extended_dict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"ส่วนเบี่ยงเบนมาตรฐาน\" \"ให้\"                \"วัด\"               \n [4] \"การกระจายสัมบูรณ์\"    \" \"                 \"แต่\"               \n [7] \"สัมประสิทธิ์การแปรผัน\"  \"ใช้\"                \"วัด\"               \n[10] \"การกระจายสัมพัทธ์\"   \n```\n:::\n:::\n\n\nจากตัวอย่างข้างต้นจะเห็นว่า การกำหนด dictionary เพิ่มเติมทำให้ผู้วิเคราะห์สามารถตัดคำ/ดึงคำจากข้อความที่มีความหมายสอดคล้องกับบริบทของการวิเคราะห์ได้มากขึ้น\n\n## Sentences tokenization\n\nเป็นเทคนิคการแบ่งข้อความออกเป็นประโยคหรือวลีย่อย โดยอาจใช้เว้นวรรคหรือเครื่องหมายจบประโยคต่าง ๆ เป็นตัวแบ่ง การทำ tokenization ลักษณะนี้เหมาะสำหรับงานวิเคราะห์ที่ต้องการสรุปความ (text summarization) ที่โครงสร้างของประโยคมีความสำคัญ\n\nการทำ sentences tokenization ด้วย PyThaiNLP สามารถทำได้ด้วยฟังก์ชัน `sent_tokenize()` ทั้งนี้ก่อนการใช้ฟังก์ชันดังกล่าวผู้วิเคราะห์จำเป็นต้องติดตั้ง library `python-crfsuite` ก่อนโดยพิมพ์คำสั่งต่อไปนี้ใน terminal ของ R Studio\n\n\n::: {.cell}\n\n```{.terminal .cell-code}\npip install python-crfsuite\n```\n:::\n\n\nฟังก์ชัน `sent_tokenize()` มีพารามิเตอร์ที่สำคัญ 2 ตัว ได้แก่ `text` เป็นข้อความนำเข้าสำหรับฟังก์ชัน ส่วนพารามิเตอร์ `engine` ใช้สำหรับกำหนดอัลกอริทึมเพื่อแบ่งประโยคจากข้อความที่กำหนด โดยมี 2 อัลกอริทึมให้เลือกได้แก่ 'whitespace' กับ 'whitespace+newline' ซึ่งเป็นการแบ่งประโยคด้วยช่องว่าง และการเว้นบรรทัด\n\n\n::: {.cell}\n\n```{.r .cell-code}\npythainlp$sent_tokenize(text, engine = \"whitespace\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ส่วนเบี่ยงเบนมาตรฐานให้วัดการกระจายสัมบูรณ์\" \n[2] \"แต่สัมประสิทธิ์การแปรผันใช้วัดการกระจายสัมพัทธ์\"\n```\n:::\n\n```{.r .cell-code}\nmytext <- readLines(\"mytext.txt\")\ntemp<-pythainlp$sent_tokenize(text = mytext[1],engine = \"whitespace+newline\")\ntemp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"การวิเคราะห์การถดถอย\"\n```\n:::\n:::\n\n\n# Stopword removal\n\nStopwords คือคำที่ไม่ให้ความหมายหรือไม่มีประโยชน์ในการตีความหมายของประโยคหรือข้อความเป้าหมาย เช่น a, and, the, และ หรือ อะ ซึ่งในชุดข้อมูลหากมีคำประเภทนี้อยู่มาก ๆ จะกลายเป็น noise ที่รบกวนการวิเคราะห์ ใน library pythainlp มีการรวบรวม stopwords สำหรับภาษาไทยเอาไว้พอสมควร ผู้วิเคราะห์สามารถนำมาใช้ได้ การเรียก stopword จาก pythinlp มาใช้สามารถทำได้ในทำนองเดียวกับการตัดคำดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstopword <- pythainlp$corpus$thai_stopwords\nstopword\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<function thai_stopwords at 0x10d333130>\n```\n:::\n:::\n\n\nอย่างไรก็ตาม stopword ข้างต้นอยู่ใน format แบบ frozenset ซึ่งไม่สามารถนำมาใช้บน R ได้โดยตรง การนำ stopword ดังกล่าวมาใช้บน R สามารถเขียนคำสั่งเพิ่มเติมได้ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# python stopword script\nstopword_fn <-\"\ndef stopword():\n  import pythainlp\n  stopword = pythainlp.corpus.common.thai_stopwords()\n  stopword = list(stopword)\n  return stopword\n\"\npy_run_string(stopword_fn)\n```\n:::\n\n\nผลลัพธ์ที่ได้จากการเรียก stopword จากฟังก์ชันใน python script ด้วย `py_run_string()` จะเก็บอยู่ใน object ชื่อ `py` ซึ่งจะเก็บในชื่อเดียวกับฟังก์ชันดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy$stopword() %>% head(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"คุณๆ\"        \"ขาด\"        \"กระนั้น\"      \"เพิ่ง\"        \"ส่วน\"       \n [6] \"นู่น\"         \"เท่าใด\"      \"เพียงใด\"     \"คราวหลัง\"    \"ตลอดวัน\"    \n[11] \"ยาก\"        \"เห็น\"        \"นอกเหนือจาก\" \"สมัยนั้น\"      \"ประการ\"    \n[16] \"ๆ\"          \"จึง\"         \"เถอะ\"       \"หน่อย\"       \"เพิ่งจะ\"     \n```\n:::\n\n```{.r .cell-code}\npythainlp$corpus$thai_negations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<function thai_negations at 0x10d3331c0>\n```\n:::\n\n```{.r .cell-code}\npythainlp$corpus$thai_syllables\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<function thai_syllables at 0x10d333010>\n```\n:::\n:::\n\n\nนอกจาก stopword แล้ว pythainlp ยังมี dictionary ตัวอื่น ๆ ที่สามารถนำมาใช้ประโยชน์ได้ ได้แก่ พจนานุกรมคำปฏิเสธ `thai_negations` และพจนานุกรมพยางค์ `thai_syllables` ดังตัวอย่างต่อไปนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# negations dictionary\nnegations_fn <-\"\ndef negations():\n  import pythainlp\n  stopword = pythainlp.corpus.common.thai_negations()\n  stopword = list(stopword)\n  return stopword\n\"\n# syllables dictionary\nsyllables_fn<-\"\ndef syllables():\n  import pythainlp\n  syllable = pythainlp.corpus.common.thai_syllables()\n  syllable = list(syllable)\n  return(syllable)\n\"\npy_run_string(negations_fn)\npy_run_string(syllables_fn)\npy$negations() %>% head(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ไม่\" \"แต่\"\n```\n:::\n\n```{.r .cell-code}\npy$syllables() %>% head(20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"เพิ่ง\"  \"คดี\"   \"จี้\"    \"สข.\"  \"บอย.\" \"พัด\"   \"ทอ\"   \"ไบ\"   \"เทสก์\" \"ตึ่ง\"  \n[11] \"เรือก\" \"หวิว\"  \"เก\"   \"ทารณ์\" \"ษิต\"   \"ใช้\"   \"คล่อง\" \"มณ\"   \"ไมล์\"  \"ฝรั่ง\" \n```\n:::\n:::\n\n\n# วิเคราะห์คำตอบอัตนัยของผู้เรียน\n\nตัวอย่างส่วนนี้ผู้วิเคราะห์จะพัฒนาโมเดลตรวจคำตอบการบ้านของนิสิต ข้อมูลฝึกหัดได้จากผลการตอบการบ้านของนิสิตจำนวน 74 คน ในรายวิชา 2758501 ภาคปลาย ปีการศึกษา 2565 เรื่องการวิเคราะห์การถดถอย\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(readxl)\n# importing data\ndat <- read_excel(\"traindataset_hw.xlsx\")\nglimpse(dat,60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 74\nColumns: 29\n$ Timestamp     <dttm> 2023-04-19 14:36:46, 2023-04-19 15:…\n$ Score         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ `รหัสนิสิต`      <dbl> 6441127027, 6441153327, 6442505827, …\n$ `ช่ือ นามสกุล`   <chr> \"ชลิต เชี่ยวเชิงงาน\", \"นายถิรนัย ธนัครสมบัติ\"…\n$ `ตอนเรียนที่`    <dbl> 4, 4, 7, 4, 4, 7, 7, 8, 7, 7, 7, 9, …\n$ text1         <chr> \"mathach=13.9090-0.8267(stressclim)+…\n$ result1       <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ remark1       <chr> \"normal\", \"normal\", \"normal\", \"norma…\n$ residual      <chr> \"https://drive.google.com/open?id=1V…\n$ pic1          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ hetoro        <chr> \"https://drive.google.com/open?id=1M…\n$ pic2          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ norm          <chr> \"https://drive.google.com/open?id=1Q…\n$ pic3          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ coll          <chr> \"https://drive.google.com/open?id=1q…\n$ pic4          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ text2         <chr> \"ไม่มีปัญหาใด ๆ\", \"Linearity\", \"ไม่มีปัญหา…\n$ result2       <chr> \"1\", \"0\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ text3         <chr> \"สมการถดถอยที่คำนวณมาได้นั้น นิสิตคิดว่ายังมีตัว…\n$ result3       <dbl> 0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, …\n$ text4         <chr> \"ได้ จากการทดสอบสมมุติฐานพบว่าค่า p value…\n$ result4       <dbl> 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0…\n$ text5         <chr> \"เมื่อกำหนดให้บรรยากาศความเครียดในการเรีย…\n$ result5       <dbl> 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1…\n$ text6         <chr> \"จากสมการถดถอยข้างต้นนั้น mathach=13.909…\n$ result6       <dbl> 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, …\n$ text7         <chr> \"มี ได้แก่ 1. stressclim (บรรยากาศความเ…\n$ result7       <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ `total score` <dbl> 11.0, 12.0, 13.0, 11.5, 12.5, 11.0, …\n```\n:::\n:::\n\n\nคำถามที่จะนำมาทดลองพัฒนาโมเดลคือ \"สมการถดถอยที่ประมาณได้มีความเหมาะสมที่จะใช้ทำนาย/อธิบายความสัมพันธ์ที่พบในข้อมูลหรือไม่ เพราะเหตุใด\" แนวคำตอบของคำถามนี้คือ \"เหมาะสม ด้วยเหตุผล 2 ประการ ประการแรก คือไม่พบหลักฐานว่ามีการละเมิดข้อตกลงเบื้องต้นของการวิเคราะห์การถดถอย และประการที่สอง ค่าสัมประสิทธิ์การตัดสินใจของสมการถดถอยที่ประมาณได้มีค่าเท่ากับ 0.697 แสดงว่าสมการสามารถอธิบายความผันแปรใน mathach ได้คิดเป็นร้อยละ 69.7 ซึ่งอยู่ในระดับที่สูง\"\n\nการให้คะแนนมีการให้คะแนน 3 ระดับ คือ 0, 1, 2 หมายถึงตอบผิด ตอบถูกบางส่วน และตอบถูกทั้งหมด การวิเคราะห์ด้านล่างแสดงการแจกแจงความถี่ของคะแนนที่ได้ซึ่งพบว่า ส่วนใหญ่สามารถตอบถูกได้บางส่วน และมีส่วนน้อยที่สามารถตอบถูกได้ทั้งหมด\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat$result3 %>% table()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n.\n 0  1  2 \n12 57  5 \n```\n:::\n:::\n\n\n## data preprocessing\n\nผู้วิเคราะห์สร้างชุดข้อมูลใหม่สำหรับพัฒนาโมเดลก่อนดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhw3 <- dat %>% select(`รหัสนิสิต`,result3, text3)\nhead(hw3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n    `รหัสนิสิต` result3 text3                                                      \n       <dbl>   <dbl> <chr>                                                      \n1 6441127027       0 สมการถดถอยที่คำนวณมาได้นั้น นิสิตคิดว่ายังมีตัวแปรบางประการที่ไม่มีความเหม…\n2 6441153327       1 มีความเหมาะสม เพราะ แม้ว่าเมื่อใช้การวิเคราะห์สหสัมพันธ์แบบเพียร์สัน พบว่…\n3 6442505827       2 เหมาะสมเพราะไม่ละเมิดข้อตกลงเบื้องต้นใดๆเลย และมีRscore อยู่ที่69.7%  \n4 6441193427       1 มีความเหมาะสมที่จะใช้ทำนาย/อธิบายความสัมพันธ์ที่พบในข้อมูล เพราะเศษเหลื…\n5 6441140127       2 เหมาะสม เพราะจากการวิเคราะห์เศษเหลือ เมื่อพิจารณา Linearity, Nor…\n6 6441037227       1 เหมาะสม เพราะ เมื่อพิจารณาสัมประสิทธิ์การตัดสินใจ (R2) พบว่ามีค่า 0.69…\n```\n:::\n:::\n\n\nขั้นตอนต่อมาจะทำ tokenization เพื่อแบ่งคำจากข้อความที่นิสิตตอบมา\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define tokenization function\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\")\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n## tokenized via mutate function\ntemp<-hw3 %>%\n  mutate(token = tokenize_thai(text3))\nhead(temp$token,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"สมการ ถดถอย ที่ คำนวณ มา ได้ นั้น   นิสิต คิด ว่า ยังมี ตัวแปร บางประการ ที่ ไม่ มี ความเหมาะสม กับ การ วิเคราะห์   เช่น จำนวน ร้อยละ ของ นักเรียน เพศหญิง ใน โรง เร ีน   ซึ่ง อาจจะ ไม่ ถือว่า เป็น ดัชนี ชี้ วัด ว่า   ค่าเฉลี่ย นั้น จะ เพิ่มขึ้น หรือ ลดลง อย่างไร ได้ จาก การ มี จำนวน นักเรียน หญิง ที่ เพิ่มขึ้น   เป็นต้น\"                                                                                                                                                                                                \n [2] \"มี ความเหมาะสม   เพราะ   แม้ว่า เมื่อ ใช้การ วิเคราะห์ สห สัมพันธ์ แบบ เพียร์ สัน   พบ ว่า   ตัวแปร สัดส่วน นักเรียน หญิง ภายใน โรงเรียน มี ความสัมพันธ์ เชิงเส้น กับ ตัวแปร ตาม ใน ระดับ ต่ำ   ( r   =   - 2.70 )   ทำให้ ละเมิด ข้อตกลง   แต่ เมื่อ ดู จาก แผนภาพ   residual   พบ ว่า มี การ กระจาย ของ เศษเหลือ ที่ มีแนวโน้ม เป็น เส้นตรง   มี ความ เป็น   Homoskedasticity   (ฺ Breusch-Pagan   =   1.04 ,   p   =   0.792 )   และ ไม่ มีปัญหา   Multicollinearity   เนื่องจาก   VIF   ของ ทุก ตัวแปร อิสระ มีค่า น้อยกว่า   4\"\n [3] \"เหมาะสม เพราะ ไม่ ละเมิด ข้อตกลง เบื้องต้น ใดๆ เลย   และ มี Rscore   อยู่ ที่ 69.7 %\"                                                                                                                                                                                                                                                                                                                                                                                           \n [4] \"มี ความเหมาะสม ที่จะ ใช้ ทำนาย / อธิบาย ความสัมพันธ์ ที่ พบ ใน ข้อมูล   เพราะ เศษเหลือ มี การ เ เจ กเ เจ ง แบบ ปกติ   เศษเหลือ มี การ กระจาย ตัว รอบ เส้น สมการ ถดถอย อย่าง สม่ำเสมอ ใกล้เคียง กัน   และ ไม่ มีค่า ผิดปกติ ที่ มีอิทธิพล ต่อ การประมาณ สมการ ถดถอย\"                                                                                                                                                                                                                                     \n [5] \"เหมาะสม   เพราะ จาก การ วิเคราะห์ เศษเหลือ   เมื่อ พิจารณา   Linearity ,   Normality   และ   Homoscedasticity   ไม่ พบ ปัญหา ละเมิด ข้อตกลง เบื้องต้น   และ เมื่อ พิจารณา สัมประสิทธิ์ การตัดสินใจ   บ่งบอก ว่า สมการ ถดถอย สามารถ อธิบาย ความแปรผัน ที่ เกิดขึ้น ใน ตัวแปร ตาม คิด เป็น ถึง ร้อยละ   70\"                                                                                                                                                                                                \n [6] \"เหมาะสม   เพราะ   เมื่อ พิจารณา สัมประสิทธิ์ การตัดสินใจ   ( R 2 )   พบ ว่า มีค่า   0.697   ซึ่ง มีค่า เข้าใกล้   1   สรุป ได้ ว่า   สมการ ถดถอย ที่ ประมาณ ได้ สามารถ ทำนาย ตัวแปร ตาม ได้ อย่าง แม่นยำ   จึง เหมาะสม ที่จะ ใช้ ทำนาย / อธิบาย ความสัมพันธ์ ที่ พบ ใน ข้อมูล\"                                                                                                                                                                                                                               \n [7] \"เหมาะสม   เพราะ   เมื่อ พิจารณา สัมประสิทธิ์ การตัดสินใจ   ( R ^ 2 )   พบ ว่า มีค่า   0.697   ซึ่ง มีค่า เข้าใกล้   1              สรุป ได้ ว่า   สมการ ถดถอย ที่ ประมาณ ได้   สามารถ ทำนาย ตัวแปร ตาม ได้ แม่นยำ   จึง เหมาะสม ที่จะ ใช้ ทำนาย ความสัมพันธ์ ที่ พบ ใน ข้อมูล\"                                                                                                                                                                                                                             \n [8] \"เหมาะสม   เพราะว่า พิจารณา จาก สัมประสิทธิ์ การตัดสินใจ   ( R ^ 2 )   พบ ว่า มีค่า   0.697   ซึ่ง มีค่า เข้าใกล้   1   ดังนั้น จะ ได้ ว่า สมการ ถดถอย ที่ ประมาณ ได้ สามารถ ทำนาย ตัวแปร ตาม ได้ อย่าง แม่นยำ   สรุป จึง เหมาะสม ที่จะ ใช้ ทำนาย / อธิบาย ความสัมพันธ์ ที่ พบ ใน ข้อมูล\"                                                                                                                                                                                                                       \n [9] \"R ^ 2   =   0.697   ดังนั้น สมการ ถดถอย ที่ ประมาณ ได้ มีประสิทธิภาพ ใน การ ทำนาย มาก   เพราะ   R ^ 2   มีค่า ใกล้เคียง   1\"                                                                                                                                                                                                                                                                                                                                                      \n[10] \"R ^ 2 = 0.697   นั่น คือ สมการ ถดถอย ที่ ประมาณ ได้ มีประสิทธิภาพ ใน การ ทำนาย ตัวแปร ตาม ได้ แม่น มาก   เพราะ มีค่า ใกล้เคียง 1\"                                                                                                                                                                                                                                                                                                                                                    \n```\n:::\n:::\n\n\nจะสังเกตเห็นว่าการตัดคำด้วย dictionary ของ pythainlp สามารถทำได้ดีพอสมควร แต่คำศัพท์ทางสถิติที่แปลเป็นภาษาไทยยังไม่สามารถตัดคำได้อย่างเหมาะสม ดังนั้นเพื่อให้การตัดคำทำได้อย่างมีประสิทธิภาพมากขึ้น ผู้วิเคราะห์จึงจะเพิ่มคำศัพท์ทางสถิติที่เกี่ยวข้องไว้ใน dictionary โดยเก็บคำศัพท์ดังกล่าวไว้ในไฟล์ `mytext.txt`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrie <- pythainlp$tokenize$Trie\n## custom word\ncustom_word <- readLines(\"mytext.txt\", skipNul = TRUE)\n## load thai dictionary from pythainlp\ndict_default <- pythainlp$corpus$thai_words()\n\nextended_dict <- trie(pythainlp$corpus$thai_words())\nfor (word in custom_word) {\n  extended_dict$add(word)\n}\n```\n:::\n\n\nจากนั้นลองดำเนินการตัดคำใหม่ด้วย dictionary ข้างต้น\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define tokenization function\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\",\n                   custom_dict = extended_dict)\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n## tokenized via mutate function\ntemp<-hw3 %>%\n  mutate(token = tokenize_thai(text3))\nhead(temp$token,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"สมการถดถอย ที่ คำนวณ มา ได้ นั้น   นิสิต คิด ว่า ยังมี ตัวแปร บางประการ ที่ ไม่ มี ความเหมาะสม กับ การ วิเคราะห์   เช่น จำนวน ร้อยละ ของ นักเรียน เพศหญิง ใน โรง เร ีน   ซึ่ง อาจจะ ไม่ ถือว่า เป็น ดัชนี ชี้ วัด ว่า   ค่าเฉลี่ย นั้น จะ เพิ่มขึ้น หรือ ลดลง อย่างไร ได้ จาก การ มี จำนวน นักเรียน หญิง ที่ เพิ่มขึ้น   เป็นต้น\"                                                                                                                                                                                           \n [2] \"มี ความเหมาะสม   เพราะ   แม้ว่า เมื่อ ใช้การ วิเคราะห์ สหสัมพันธ์ แบบ เพียร์สัน   พบ ว่า   ตัวแปร สัดส่วน นักเรียน หญิง ภายใน โรงเรียน มี ความสัมพันธ์เชิงเส้น กับ ตัวแปรตาม ใน ระดับ ต่ำ   ( r   =   - 2.70 )   ทำให้ ละเมิด ข้อตกลง   แต่ เมื่อ ดู จาก แผนภาพ   residual   พบ ว่า มี การกระจาย ของ เศษเหลือ ที่ มีแนวโน้ม เป็น เส้นตรง   มี ความ เป็น   Homoskedasticity   (ฺ Breusch-Pagan   =   1.04 ,   p   =   0.792 )   และ ไม่ มีปัญหา   Multicollinearity   เนื่องจาก   VIF   ของ ทุก ตัวแปรอิสระ มีค่า น้อยกว่า   4\"\n [3] \"เหมาะสม เพราะ ไม่ ละเมิด ข้อตกลงเบื้องต้น ใดๆ เลย   และ มี Rscore   อยู่ ที่ 69.7 %\"                                                                                                                                                                                                                                                                                                                                                                                      \n [4] \"มี ความเหมาะสม ที่จะ ใช้ ทำนาย/อธิบายความสัมพันธ์ ที่ พบ ใน ข้อมูล   เพราะ เศษเหลือ มี การ เ เจ กเ เจ ง แบบ ปกติ   เศษเหลือ มี การกระจาย ตัว รอบ เส้น สมการถดถอย อย่าง สม่ำเสมอ ใกล้เคียง กัน   และ ไม่ มีค่า ผิดปกติ ที่ มีอิทธิพล ต่อ การประมาณ สมการถดถอย\"                                                                                                                                                                                                                                     \n [5] \"เหมาะสม   เพราะ จาก การวิเคราะห์เศษเหลือ   เมื่อ พิจารณา   Linearity ,   Normality   และ   Homoscedasticity   ไม่ พบ ปัญหา ละเมิด ข้อตกลงเบื้องต้น   และ เมื่อ พิจารณา สัมประสิทธิ์การตัดสินใจ   บ่งบอก ว่า สมการถดถอย สามารถ อธิบาย ความแปรผัน ที่ เกิดขึ้น ใน ตัวแปรตาม คิด เป็น ถึง ร้อยละ   70\"                                                                                                                                                                                                \n [6] \"เหมาะสม   เพราะ   เมื่อ พิจารณา สัมประสิทธิ์การตัดสินใจ   ( R2 )   พบ ว่า มีค่า   0.697   ซึ่ง มีค่า เข้าใกล้   1   สรุป ได้ ว่า   สมการถดถอย ที่ ประมาณ ได้ สามารถ ทำนาย ตัวแปรตาม ได้ อย่าง แม่นยำ   จึง เหมาะสม ที่จะ ใช้ ทำนาย/อธิบายความสัมพันธ์ ที่ พบ ใน ข้อมูล\"                                                                                                                                                                                                                                \n [7] \"เหมาะสม   เพราะ   เมื่อ พิจารณา สัมประสิทธิ์การตัดสินใจ   ( R ^ 2 )   พบ ว่า มีค่า   0.697   ซึ่ง มีค่า เข้าใกล้   1              สรุป ได้ ว่า   สมการถดถอย ที่ ประมาณ ได้   สามารถ ทำนาย ตัวแปรตาม ได้ แม่นยำ   จึง เหมาะสม ที่จะ ใช้ ทำนาย ความสัมพันธ์ ที่ พบ ใน ข้อมูล\"                                                                                                                                                                                                                          \n [8] \"เหมาะสม   เพราะว่า พิจารณา จาก สัมประสิทธิ์การตัดสินใจ   ( R ^ 2 )   พบ ว่า มีค่า   0.697   ซึ่ง มีค่า เข้าใกล้   1   ดังนั้น จะ ได้ ว่า สมการถดถอย ที่ ประมาณ ได้ สามารถ ทำนาย ตัวแปรตาม ได้ อย่าง แม่นยำ   สรุป จึง เหมาะสม ที่จะ ใช้ ทำนาย/อธิบายความสัมพันธ์ ที่ พบ ใน ข้อมูล\"                                                                                                                                                                                                                       \n [9] \"R ^ 2   =   0.697   ดังนั้น สมการถดถอย ที่ ประมาณ ได้ มีประสิทธิภาพ ใน การทำนาย มาก   เพราะ   R ^ 2   มีค่า ใกล้เคียง   1\"                                                                                                                                                                                                                                                                                                                                                  \n[10] \"R ^ 2 = 0.697   นั่น คือ สมการถดถอย ที่ ประมาณ ได้ มีประสิทธิภาพ ใน การทำนาย ตัวแปรตาม ได้ แม่น มาก   เพราะ มีค่า ใกล้เคียง 1\"                                                                                                                                                                                                                                                                                                                                                 \n```\n:::\n:::\n\n\nสำรวจข้อมูลเบื้องต้น\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidytext)\ntemp<-temp %>%\n  unnest_tokens(input = token, output = word, token = \"ptb\") %>%\n  filter(!word %in% py$stopword()) %>%\n  #filter(!word %in% py$negations()) %>%\n  filter(!word %in% py$syllables())\n\ntemp %>% \n  # remove all numbers\n  mutate(word = str_replace_all(word, \"\\\\d+\", \"\")) %>%\n  # remove all english words\n  mutate(word = str_remove_all(word, \"\\\\b(?!R2\\\\b|\\\\bR\\\\s?\\\\^\\\\s?2\\\\b)[A-Za-z]+\\\\b\")) %>%\n  # correct word\n  mutate(word = str_replace(word, \"อารีสแควร์\",\"อาร์สแควร์\"))%>%\n  #mutate(word = str_replace(word, \"เหมาะสม\",\"ความเหมาะสม\")) %>%\n  #mutate(word = str_replace(word , \"ถดถอย\",\"สมการถดถอย\")) %>%\n  # remove punctuation\n  mutate(word = str_remove(word, \"^[[:punct:]]+$\")) %>%\n  mutate(word = str_remove(word,\" \")) %>%\n  # remove unnescessary words\n  filter(!word %in% c(\"มีค่า\",\"ที่จะ\",\"=\",\"นำมา\",\"^\",\"เ\",\"ใกล้เคียง\",\"ยังมี\",\"ยกกำลัง\",\"ทำการ\",\"ใชต้รวจ\",\"ขอ้\",\"ืองตน้\",\"ดังนั้น\",\"ว่าการ\",\n                      \"บางประการ\",\"ทั้งสอง\")) %>%\n  filter(!word %in% c(\".\",\"\"))%>%\n  group_by(result3,word) %>%\n  count() %>%\n  arrange(desc(n)) %>%\n  head(20) %>%\n  ggplot(aes(x = n, y= reorder(word,n)))+\n  geom_col(aes(fill = factor(result3,\n                             levels=c(0,1,2),\n                             labels=c(\"ผิด\",\"ถูกบางส่วน\",\"ถูก\"))))+\n  theme(text=element_text(family=\"ChulaCharasNew\"))+\n  labs(fill = \"ผลการตอบ\")\n```\n\n::: {.cell-output-display}\n![](TextPreprocessing_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"wordcloud2\")\nlibrary(wordcloud2)\nword_count<-temp %>% \n  # remove all numbers\n  mutate(word = str_replace_all(word, \"\\\\d+\", \"\")) %>%\n  # remove all english words\n  mutate(word = str_remove_all(word, \"\\\\b(?!R2\\\\b|\\\\bR\\\\s?\\\\^\\\\s?2\\\\b)[A-Za-z]+\\\\b\")) %>%\n  # correct word\n  mutate(word = str_replace(word, \"อารีสแควร์\",\"อาร์สแควร์\"))%>%\n  #mutate(word = str_replace(word, \"เหมาะสม\",\"ความเหมาะสม\")) %>%\n  #mutate(word = str_replace(word , \"ถดถอย\",\"สมการถดถอย\")) %>%\n  # remove punctuation\n  mutate(word = str_remove(word, \"^[[:punct:]]+$\")) %>%\n  mutate(word = str_remove(word,\" \")) %>%\n  # remove unnescessary words\n  filter(!word %in% c(\"มีค่า\",\"ที่จะ\",\"=\",\"นำมา\",\"^\",\"เ\",\"ใกล้เคียง\",\"ยังมี\",\"ยกกำลัง\",\"ทำการ\",\"ใชต้รวจ\",\"ขอ้\",\"ืองตน้\",\"ดังนั้น\",\"ว่าการ\",\n                      \"บางประการ\",\"ทั้งสอง\")) %>%\n  filter(!word %in% c(\".\",\"\"))%>%\n  group_by(result3,word) %>%\n  count() \n\ncol<-ifelse(word_count[,1]==0,\"maroon\",\n            ifelse(word_count[,1]==1,\"orange\",\"steelblue\"))\nwordcloud2(data = word_count[,-1], size = 3, fontFamily = \"ChulaCharasNew\",\n           color = col, backgroundColor = \"white\",\n           shape = \"square\")\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"wordcloud2 html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-5f1310d86619d1112ec8\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-5f1310d86619d1112ec8\">{\"x\":{\"word\":[\"การกระจาย\",\"ข้อมูล\",\"ความสัมพันธ์\",\"ความเหมาะสม\",\"คำทำนาย\",\"คำนวณ\",\"ค่าเฉลี่ย\",\"จำนวน\",\"ดัชนี\",\"ตรงกลาง\",\"ตัวแปร\",\"ทำนาย\",\"นักเรียน\",\"นิสิต\",\"บริเวณ\",\"ร้อยละ\",\"ลดลง\",\"วิเคราะห์\",\"สมการถดถอย\",\"สส\",\"อารี\",\"อาร์สแควร์\",\"ีน\",\"เป็นตัวแทน\",\"เพศหญิง\",\"เพิ่มขึ้น\",\"เส้นตรง\",\"เหมาะสม\",\"เเควร์\",\"แควร์\",\"<\",\"=.\",\"การกระจาย\",\"การทำนาย\",\"การประมาณ\",\"การพิจารณา\",\"การวิเคราะห์การถดถอย\",\"การวิเคราะห์เศษเหลือ\",\"การเรียน\",\"การแจกแจงแบบปกติ\",\"กเ\",\"ข้อตกลง\",\"ข้อตกลงเบื้องต้น\",\"ข้อมูล\",\"ค\",\"คณิตศาสตร์\",\"คลาดเคลื่อน\",\"ความสัมพันธ์\",\"ความสัมพันธ์เชิงเส้น\",\"ความเหมาะสม\",\"ความแปรปรวน\",\"คะแนน\",\"คุณสมบัติ\",\"ค่าที่\",\"ค่าเฉลี่ย\",\"ง\",\"จำเป็นต้อง\",\"ตกลง\",\"ตรวจสอบ\",\"ตัวแปร\",\"ตัวแปรตาม\",\"ตัวแปรอิสระ\",\"ถดถอย\",\"ท\",\"ทดสอบ\",\"ทำนาย\",\"ทำนาย/อธิบายความสัมพันธ์\",\"ที่นา\",\"ที่สูง\",\"นักเรียน\",\"นัยสำคัญ\",\"นำมาใช้\",\"นำไปใช้\",\"นิสิต\",\"ปกติ\",\"ปฏิเสธ\",\"ปัญหา\",\"ผลสัมฤทธิ์\",\"ผิดปกติ\",\"ผู้วิจัย\",\"พิจารณา\",\"มากเกินไป\",\"มีประสิทธิภาพ\",\"มีปัญหา\",\"มีอิทธิพล\",\"มีแนวโน้ม\",\"รท\",\"ระดับ\",\"ร้อยละ\",\"ละเมิด\",\"ละเมิน\",\"วิชา\",\"วิเคราะห์\",\"สมการ\",\"สมการถดถอย\",\"สมมติฐาน\",\"สม่ำเสมอ\",\"สหสัมพันธ์\",\"สอดคล้อง\",\"สัดส่วน\",\"สัมประสิทธิ์การตัดสินใจ\",\"สำหรับ\",\"อธิบาย\",\"ออกจาก\",\"ะห์\",\"า\",\"ฺ\",\"เข้าใกล้\",\"เบื้องต้น\",\"เป็นไปตาม\",\"เป็นไปตามข้อตกลงเบื้องต้น\",\"เพียร์สัน\",\"เศษเหลือ\",\"เส้นตรง\",\"เหมาะสม\",\"เเต่\",\"แผนภาพ\",\"แม่นยำ\",\"โรงเรียน\",\"ใช้การ\",\"่\",\"ํา\",\"การทำนาย\",\"การวิเคราะห์การถดถอย\",\"การวิเคราะห์เศษเหลือ\",\"ข้อตกลง\",\"ข้อตกลงเบื้องต้น\",\"ข้อมูล\",\"ความสัมพันธ์\",\"ความเหมาะสม\",\"ความแปรผัน\",\"คะแนน\",\"ตรวจสอบ\",\"ตัวแปร\",\"ตัวแปรตาม\",\"ตัวแปรอิสระ\",\"ทดสอบ\",\"ทำนาย\",\"ที่อยู่\",\"นักเรียน\",\"นัยสำคัญ\",\"บ่งบอก\",\"ปัญหา\",\"พิจารณา\",\"ภาวะร่วมเส้นตรงพหุ\",\"มากเกินไป\",\"มีประสิทธิภาพ\",\"มีปัญหา\",\"ร้อยละ\",\"ละเมิด\",\"วิเคราะห์\",\"สถิติ\",\"สมการถดถอย\",\"สัมประสิทธิ์การตัดสินใจ\",\"อธิบาย\",\"เกิดขึ้น\",\"เกินไป\",\"เป็นไปตามข้อตกลงเบื้องต้น\",\"เหมาะสม\",\"ใช้ไม่ได้\"],\"freq\":[1,1,1,1,1,1,1,2,1,1,1,8,2,1,2,1,1,1,4,1,1,4,1,1,1,2,1,10,1,1,1,1,4,9,1,1,4,2,3,2,1,3,13,54,1,3,4,24,3,28,5,3,2,1,3,2,1,1,32,3,11,1,10,1,4,30,10,2,3,4,2,1,4,2,1,1,2,3,1,1,8,1,11,8,1,1,2,3,3,6,1,3,41,2,38,1,1,1,2,1,19,1,9,1,1,1,1,9,13,3,13,1,19,1,36,2,3,9,4,1,1,2,2,1,4,1,2,1,5,1,1,1,4,1,2,4,4,4,1,1,2,1,4,6,3,1,1,1,2,3,1,4,4,4,1,1,1,2,4,2],\"fontFamily\":\"ChulaCharasNew\",\"fontWeight\":\"bold\",\"color\":[[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"maroon\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"orange\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"],[\"steelblue\"]],\"minSize\":0,\"weightFactor\":10,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"square\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}</script>\n```\n:::\n:::\n\n\n</br>\n\nจะเห็นว่าคำตอบของผู้เรียนมีแนวโน้มที่จะกล่าวถึงข้อตกลงเบื้องต้นของการวิเคราะห์การถดถอยว่ามีความเหมาะสมอย่างไร\n\nผลการวิเคราะห์ข้างต้นยังทำให้ผู้วิเคราะห์ทำความสะอาดข้อมูลไปได้ระดับหนึ่ง ขั้นตอนต่อไปคือจะนำข้อมูลที่จัดระเบียบและทำความสะอาดนี้ไปพัฒนาโมเดลตรวจคำตอบต่อไป\n\n## Training Classification models\n\nการวิเคราะห์ในส่วนนี้เป็นการพัฒนาโมเดลทำนาย เพื่อตรวจคำตอบแบบข้อเขียนของนักเรียน การพัฒนาโมเดลประกอบด้วยขั้นตอนได้แก่ (1) การนำเข้าและจัดการข้อมูลก่อนการวิเคราะห์​ (2) การแบ่งส่วนชุดข้อมูล (3) การกำหนดและระบุโมเดลทำนาย (4) การปรับแต่งค่า hyperparameters ด้วยเทคนิค cross-validation (5) การประเมินประสิทธิภาพของโมเดล และ (6) การนำโมเดลไปใช้ รายละเอียดมีดังนี้\n\n### 1.  การนำเข้าและจัดการข้อมูลก่อนการวิเคราะห์\n\nชุดข้อมูลที่ใช้พัฒนาโมเดลคือ `traindataset_hw.xlsx` สามารถนำเข้าได้ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- read_excel(\"traindataset_hw.xlsx\")\nnames(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Timestamp\"   \"Score\"       \"รหัสนิสิต\"      \"ช่ือ นามสกุล\"   \"ตอนเรียนที่\"   \n [6] \"text1\"       \"result1\"     \"remark1\"     \"residual\"    \"pic1\"       \n[11] \"hetoro\"      \"pic2\"        \"norm\"        \"pic3\"        \"coll\"       \n[16] \"pic4\"        \"text2\"       \"result2\"     \"text3\"       \"result3\"    \n[21] \"text4\"       \"result4\"     \"text5\"       \"result5\"     \"text6\"      \n[26] \"result6\"     \"text7\"       \"result7\"     \"total score\"\n```\n:::\n:::\n\n\nเปลี่ยนชื่อคอลัมน์บางตัวเพื่อให้สะดวกในการเรียกใช้งาน\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(dat)[3]<-\"student_id\"\nnames(dat)[4]<-\"student_name\"\nnames(dat)[5]<-\"section\"\n```\n:::\n\n\nผู้วิเคราะห์จะดำเนินการจัดการข้อมูลทั้งชุดก่อนตามขั้นตอนที่ได้ทำในการสำรวจข้อมูลเบื้องต้น รายละเอียดมีดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### ---- define tokenization function for Thai language\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\",\n                   custom_dict = extended_dict)\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n### ---- python stopword script\nstopword_fn <-\"\ndef stopword():\n  import pythainlp\n  stopword = pythainlp.corpus.common.thai_stopwords()\n  stopword = list(stopword)\n  return stopword\n\"\n### ---- syllables dictionary\nsyllables_fn<-\"\ndef syllables():\n  import pythainlp\n  syllable = pythainlp.corpus.common.thai_syllables()\n  syllable = list(syllable)\n  return(syllable)\n\"\n```\n:::\n\n\nคำสั่งต่อไปนี้ใช้จัดเตรียมข้อมูล training dataset ทั้งหมด ทั้งนี้ผู้วิเคราะห์ได้ตรวจคำตอบตามแนวคำตอบที่กำหนดไว้แล้ว ผลการตรวจอยู่ใน column `result3`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nq3_dat <- dat %>% select(student_id, result3, text3)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(themis)\nlibrary(textrecipes)\n\n## tokenized via mutate function\ntemp<-hw3 %>%\n  mutate(word = tokenize_thai(text3)) %>%\n  # remove all numbers\n  mutate(word = str_replace_all(word, \"\\\\d+\", \"\")) %>%\n  # remove all english words\n  mutate(word = str_remove_all(word, \"\\\\b(?!R2\\\\b|\\\\bR\\\\s?\\\\^\\\\s?2\\\\b)[A-Za-z]+\\\\b\")) %>%\n  # correct word\n  mutate(word = str_replace(word, \"อารีสแควร์\",\"อาร์สแควร์\"))%>%\n  #mutate(word = str_replace(word, \"เหมาะสม\",\"ความเหมาะสม\")) %>%\n  #mutate(word = str_replace(word , \"ถดถอย\",\"สมการถดถอย\")) %>%\n  # remove punctuation\n  mutate(word = str_remove(word, \"^[[:punct:]]+$\")) %>%\n  mutate(word = str_remove(word,\" \")) %>%\n  # remove unnescessary words\n  filter(!word %in% c(\"มีค่า\",\"ที่จะ\",\"=\",\"นำมา\",\"^\",\"เ\",\"ใกล้เคียง\",\"ยังมี\",\"ยกกำลัง\",\"ทำการ\",\"ใชต้รวจ\",\"ขอ้\",\"ืองตน้\",\"ดังนั้น\",\"ว่าการ\",\n                      \"บางประการ\",\"ทั้งสอง\")) %>%\n  filter(!word %in% c(\".\",\"\")) %>%\n    filter(!word %in% py$stopword()) %>%\n  #filter(!word %in% py$negations()) %>%\n  filter(!word %in% py$syllables()) %>%\n  mutate(result3 = factor(result3, levels=c(0,1,2),\n                          labels=c(\"ผิด\",\"ถูก\",\"ถูก\")))\n###----\nrecipe_obj <- recipe(result3~ word, data = temp) %>%\n  step_tokenize(word, token = \"ptb\") %>%\n  step_tfidf(word) %>%  \n  step_normalize(all_numeric_predictors()) %>%\n  step_adasyn(result3)\n\n### --- model specification\nlasso_spec <- multinom_reg(penalty = tune(),\n                           mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"classification\")\nknn_spec <- nearest_neighbor(neighbors = tune(),\n                             weight_func = tune())%>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\n\nwf<- workflow_set(\n  preproc = list(recipe_obj),\n  models = list(lasso_spec, knn_spec)\n)\n  \n\nfold<-vfold_cv(temp, v=10)\neval_metric <- metric_set(accuracy, sens, spec)\nlibrary(doMC)\nregisterDoMC(cores=15)\nresult <- wf %>%\n  workflow_map(\n            resamples = fold,\n            grid = 30,\n            metrics = eval_metric\n            )\nresult %>% autoplot() +\n  theme(legend.position = \"top\")+\n  scale_y_continuous(breaks=seq(0,1,0.1))\n```\n\n::: {.cell-output-display}\n![](TextPreprocessing_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresult %>%\n  extract_workflow_set_result(id = \"recipe_nearest_neighbor\") %>%\n  collect_metrics() %>%\n  ggplot(aes(neighbors, mean, color = .metric)) +\n  geom_line(size = 1, show.legend = FALSE) +\n  geom_point()+\n  facet_wrap(~.metric) +\n  scale_x_log10()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](TextPreprocessing_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\nresult %>%\n  extract_workflow_set_result(id = \"recipe_nearest_neighbor\") %>%\n  collect_metrics() %>%\n  ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +\n  geom_boxplot(size = 0.5, show.legend = FALSE) +\n  facet_wrap(~.metric)\n```\n\n::: {.cell-output-display}\n![](TextPreprocessing_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n:::\n\n\nlast_fit\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "TextPreprocessing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.1/htmlwidgets.js\"></script>\n<link href=\"../site_libs/wordcloud2-0.0.1/wordcloud.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/wordcloud2-0.0.1/wordcloud2-all.js\"></script>\n<script src=\"../site_libs/wordcloud2-0.0.1/hover.js\"></script>\n<script src=\"../site_libs/wordcloud2-binding-0.2.1/wordcloud2.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}