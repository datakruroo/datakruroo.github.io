{
  "hash": "f90ed5a68aedff14902d9d97c16c728f",
  "result": {
    "markdown": "---\ntitle: \"Text Analytics\"\n---\n\n\n‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏Å‡∏±‡∏î‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏ô‡∏∂‡πà‡∏á ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà\n\n-   sentiment analysis\n\n-   topic modeling\n\n-   text classification\n\n-   text summarization\n\n-   relationship extraction\n\n-   trend analysis\n\n-   keyword extraction\n\n‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ML ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢ ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\n\n# Sentiment analysis\n\n‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å (sentiment analysis) ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô ‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏ß‡∏Å ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏ö ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏•‡∏≤‡∏á ‡πÜ ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡∏≠‡∏≤‡∏à‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡πÄ‡∏ä‡πà‡∏ô ‡πÇ‡∏Å‡∏£‡∏ò ‡∏Å‡∏•‡∏±‡∏ß ‡∏Ç‡∏¢‡∏∞‡πÅ‡∏Ç‡∏¢‡∏á ‡∏´‡∏£‡∏∑‡∏≠ ‡∏î‡∏µ‡πÉ‡∏à\n\n‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ ‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏°‡∏µ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà\n\n1.  **lexicon-bases methods:** ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å (sentiment score) ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏ß‡πà‡∏≤ lexicons ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ô‡∏≥‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏°‡∏≤‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß\n2.  **machine learning methods:** ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏ä‡πà‡∏ß‡∏¢ ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏£‡πà‡∏≤‡∏ß ‡πÜ ‡∏Ñ‡∏∑‡∏≠ ‡∏ú‡∏π‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡∏´‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ label ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤ ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n3.  **deep learning methods:** ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö ML methods ‡πÅ‡∏ï‡πà‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° deep learning ‡πÄ‡∏ä‡πà‡∏ô RNNs, LSTM ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏ñ‡∏∂‡∏á‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏∑‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå\n4.  **hybrid methods**: ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 3 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ lexicon-based ‡πÉ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ data preprocess ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n\n## Lexicon-based methods\n\n‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÉ‡∏ô R ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ package `tidytext` ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ package ‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏Ñ‡∏∑‡∏≠‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ grammar ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ö‡∏ô tidyverse ‡∏ã‡∏∂‡πà‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏£‡∏∞‡∏ó‡∏≥ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏•‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ package ‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡∏Å‡πà‡∏≠‡∏ô\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(readxl)\n```\n:::\n\n\n‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- read.csv(\"https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/review_shopping.csv\",\n                header=F, col.names = c(\"text\",\"sentiment\"),\n                sep = \"\\t\")\ndat$id <- 1:dim(dat)[1]\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                            text\n1                                              ‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\n2                                                                   ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏î‡πâ‡∏≠‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û\n3                               ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏´‡∏°‡∏î‡∏ó‡∏≥‡πÑ‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏à‡πâ‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏´‡∏°‡∏î‡∏≠‡∏∞.‡∏Å‡∏î‡πÉ‡∏™‡πà‡∏ï‡∏∞‡∏Å‡∏£‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏ñ‡∏≠‡∏∞.‡πÄ‡∏ã‡πá‡∏á‡πÄ‡∏£‡∏¢\n4                                                    ‡∏≠‡∏¢‡πà‡∏≤‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡πÉ‡∏Ñ‡∏£‡∏≠‡∏µ‡∏Å‡∏ô‡∏∞‡∏Ñ‡πà‡∏∞‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞\n5 ‡∏Ñ‡∏∑‡∏≠‡πÄ‡πÄ‡∏ö‡∏ö‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å ‡πÄ‡πÄ‡∏¢‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á‡∏°‡∏≤‡∏ù‡∏≤‡πÄ‡πÄ‡∏ï‡∏Å ‡πÄ‡πÄ‡∏ï‡∏Å‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡πÄ‡πÄ‡∏ï‡∏Å‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠‡∏à‡∏ö‡πÄ‡πÄ‡∏¢‡∏Åüò¢\n6                                                    ‡∏™‡πà‡∏á‡∏ä‡πâ‡∏≤‡∏à‡∏±‡∏á ‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡πâ‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n  sentiment id\n1       neg  1\n2       neg  2\n3       neg  3\n4       neg  4\n5       neg  5\n6       neg  6\n```\n:::\n\n```{.r .cell-code}\nglimpse(dat, width=60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 128\nColumns: 3\n$ text      <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\", \"‡∏™‡∏¥‡∏ô‚Ä¶\n$ sentiment <chr> \"neg\", \"neg\", \"neg\", \"neg\", \"neg\", \"neg\"‚Ä¶\n$ id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1‚Ä¶\n```\n:::\n:::\n\n\n‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneg<- read.csv(\"https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/neg.txt\", header = F, col.names = \"word\")\npos <- read.csv(\"https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/pos.txt\", header = F, col.names = \"word\")\nneg$sentiment <- \"neg\"\npos$sentiment <- \"pos\"\nlexicon <- neg %>% bind_rows(pos)\nhead(lexicon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                word sentiment\n1                 ‡πÑ‡∏°‡πà       neg\n2             ‡πÑ‡∏°‡πà‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á       neg\n3            ‡∏ó‡∏≥‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏°‡πà       neg\n4         ‡∏ä‡πà‡∏≤‡∏á‡πÄ‡∏•‡∏ß‡∏à‡∏£‡∏¥‡∏á‡πÜ       neg\n5 ‡∏ú‡∏•‡∏á‡∏≤‡∏ô‡∏¢‡πà‡∏≥‡πÅ‡∏¢‡πà‡∏à‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡∏ï‡∏Å‡∏ä‡∏±‡πâ‡∏ô       neg\n6           ‡∏≠‡∏∂‡∏î‡∏≠‡∏±‡∏î‡πÑ‡∏°‡πà‡∏£‡∏±‡∏Å       neg\n```\n:::\n\n```{.r .cell-code}\ndim(lexicon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1063    2\n```\n:::\n\n```{.r .cell-code}\nlexicon <- distinct(lexicon)\ndim(lexicon)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1037    2\n```\n:::\n:::\n\n\n#### Tokenization\n\n‡∏à‡∏≤‡∏Å‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á lexicon-based method ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö‡∏û‡∏à‡∏ì‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô sentiment ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏ú‡∏π‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∂‡∏á‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ tokenization\n\n‡πÉ‡∏ô package tidytext ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `unnest_tokens()` ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏°‡∏µ‡∏≠‡∏≤‡∏£‡πå‡∏Å‡∏¥‡∏ß‡πÄ‡∏°‡∏ô‡∏ó‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà `tbl` ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö data.frame ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î `token` ‡πÉ‡∏ä‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô `words` , `ngrams`, `skip_ngrams` , `sentences`, `lines` ‡∏´‡∏£‡∏∑‡∏≠ `paragraphs`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\n# Import pythainlp library\npythainlp <- import(\"pythainlp\")\n\n# Define a custom tokenization function for textrecipes\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"deepcut\")\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n\n## tokenized via mutate function\ntemp<-dat %>%\n  mutate(token = tokenize_thai(text))\nhead(temp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                            text\n1                                              ‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\n2                                                                   ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏î‡πâ‡∏≠‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û\n3                               ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏´‡∏°‡∏î‡∏ó‡∏≥‡πÑ‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏à‡πâ‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏´‡∏°‡∏î‡∏≠‡∏∞.‡∏Å‡∏î‡πÉ‡∏™‡πà‡∏ï‡∏∞‡∏Å‡∏£‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏ñ‡∏≠‡∏∞.‡πÄ‡∏ã‡πá‡∏á‡πÄ‡∏£‡∏¢\n4                                                    ‡∏≠‡∏¢‡πà‡∏≤‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡πÉ‡∏Ñ‡∏£‡∏≠‡∏µ‡∏Å‡∏ô‡∏∞‡∏Ñ‡πà‡∏∞‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞\n5 ‡∏Ñ‡∏∑‡∏≠‡πÄ‡πÄ‡∏ö‡∏ö‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å ‡πÄ‡πÄ‡∏¢‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á‡∏°‡∏≤‡∏ù‡∏≤‡πÄ‡πÄ‡∏ï‡∏Å ‡πÄ‡πÄ‡∏ï‡∏Å‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡πÄ‡πÄ‡∏ï‡∏Å‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠‡∏à‡∏ö‡πÄ‡πÄ‡∏¢‡∏Åüò¢\n6                                                    ‡∏™‡πà‡∏á‡∏ä‡πâ‡∏≤‡∏à‡∏±‡∏á ‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡πâ‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n  sentiment id\n1       neg  1\n2       neg  2\n3       neg  3\n4       neg  4\n5       neg  5\n6       neg  6\n                                                                                                   token\n1                                                         ‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î   ‡∏ä‡∏≠‡∏ö ‡∏´‡∏•‡∏≠‡∏Å ‡πÉ‡∏´‡πâ ‡∏™‡∏±‡πà‡∏á ‡∏ã‡∏∑‡πâ‡∏≠   ‡∏ó‡∏µ‡πà ‡πÑ‡∏´‡∏ô ‡πÑ‡∏î‡πâ ‡πÑ‡∏°‡πà ‡∏°‡∏µ ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\n2                                                                                         ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏î‡πâ‡∏≠‡∏¢ ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û\n3                                        ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏´‡∏°‡∏î ‡∏ó‡∏≥‡πÑ‡∏° ‡πÑ‡∏°‡πà ‡πÅ‡∏à‡πâ‡∏á ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ß‡πà‡∏≤ ‡∏´‡∏°‡∏î ‡∏≠‡∏∞. ‡∏Å‡∏î ‡πÉ‡∏™‡πà ‡∏ï‡∏∞‡∏Å‡∏£‡πâ‡∏≤ ‡πÑ‡∏õ ‡πÄ‡∏ñ‡∏≠‡∏∞ . ‡πÄ‡∏ã‡πá‡∏á‡πÄ‡∏£‡∏¢\n4                                                                 ‡∏≠‡∏¢‡πà‡∏≤ ‡∏ó‡∏≥ ‡πÅ‡∏ö‡∏ö ‡∏ô‡∏µ‡πâ ‡∏Å‡∏±‡∏ö ‡πÉ‡∏Ñ‡∏£ ‡∏≠‡∏µ‡∏Å ‡∏ô‡∏∞ ‡∏Ñ‡πà‡∏∞ ‡πÅ‡∏¢‡πà ‡∏°‡∏≤‡∏Å ‡∏Ñ‡πà‡∏∞\n5 ‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡πÄ‡∏ö‡∏ö ‡∏ú‡∏¥‡∏î ‡∏´‡∏ß‡∏±‡∏á ‡∏°‡∏≤‡∏Å   ‡πÄ‡πÄ‡∏¢‡πà ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÑ‡∏î‡πâ ‡∏Ç‡∏≠‡∏á ‡∏°‡∏≤‡∏ù‡∏≤‡πÄ‡πÄ‡∏ï‡∏Å   ‡πÄ‡πÄ‡∏ï‡∏Å ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤ ‡πÄ‡πÄ‡∏ï‡∏Å ‡πÄ‡∏¢‡∏≠‡∏∞ ‡∏°‡∏≤‡∏Å   ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏û‡∏≠ ‡∏à‡∏ö ‡πÄ‡πÄ‡∏¢‡∏Å üò¢\n6                                                                 ‡∏™‡πà‡∏á ‡∏ä‡πâ‡∏≤ ‡∏à‡∏±‡∏á   ‡∏£‡∏≠ ‡∏ô‡∏≤‡∏ô ‡∏°‡∏≤‡∏Å   ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å ‡∏Å‡πâ‡∏≠ ‡πÑ‡∏°‡πà ‡πÑ‡∏î‡πâ\n```\n:::\n\n```{.r .cell-code}\ntokenized_dat <- temp %>%\n  unnest_tokens(input = token, token = \"words\", \n                output = word)\nglimpse(tokenized_dat, width=60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,790\nColumns: 4\n$ text      <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\", \"‡πÄ‡∏Å‚Ä¶\n$ sentiment <chr> \"neg\", \"neg\", \"neg\", \"neg\", \"neg\", \"neg\"‚Ä¶\n$ id        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2‚Ä¶\n$ word      <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î\", \"‡∏ä‡∏≠‡∏ö\", \"‡∏´‡∏•‡∏≠‡∏Å\", \"‡πÉ‡∏´‡πâ\", \"‡∏™‡∏±‡πà‡∏á\", \"‡∏ã‡∏∑‡πâ‡∏≠\"‚Ä¶\n```\n:::\n:::\n\n\n#### stopword\n\nStopwords ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡πÄ‡∏ä‡πà‡∏ô a, and, the, ‡πÅ‡∏•‡∏∞ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏≠‡∏∞ ‡∏ã‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏°‡∏≤‡∏Å ‡πÜ ‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô noise ‡∏ó‡∏µ‡πà‡∏£‡∏ö‡∏Å‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡πÉ‡∏ô library pythainlp ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏° stopwords ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£ ‡∏ú‡∏π‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å stopword ‡∏à‡∏≤‡∏Å pythinlp ‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏ó‡∏≥‡∏ô‡∏≠‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef stop word():\n  import pythainlp\n  \n  stopword = pythainlp.corpus.common.thai_stopwords()\n  stopword = list(stopword)\n  return stopword\n```\n:::\n\n\n‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å stopword ‡∏à‡∏≤‡∏Å pythainlp ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• stopword ‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡πÑ‡∏ß‡πâ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 1,030 ‡∏Ñ‡∏≥\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource_python(\"stopword.py\")\nlength(stopword())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1030\n```\n:::\n\n```{.r .cell-code}\nhead(stopword(),10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"‡πÄ‡∏™‡∏µ‡∏¢\"      \"‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà\"     \"‡πÇ‡∏î‡∏¢\"      \"‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏µ\"      \"‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà\"      \"‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ô‡∏±‡πâ‡∏ô\"\n [7] \"‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö\"   \"‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢\"     \"‡∏£‡∏∞‡∏¢‡∏∞\"     \"‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ß‡πà‡∏≤\"  \n```\n:::\n\n```{.r .cell-code}\ntail(stopword(),10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"‡πÅ‡∏°‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á\"  \"‡∏ï‡πà‡∏≠\"       \"‡∏Ñ‡∏£‡∏≤‡∏ó‡∏µ‡πà\"     \"‡∏ñ‡∏∂‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\" \"‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô\"   \"‡∏ó‡∏∏‡∏Å‡∏ó‡∏µ‡πà\"     \n [7] \"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô\"   \"‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô\"    \"‡∏Ñ‡πà‡∏∞\"       \"‡πÉ‡∏´‡∏ç‡πà\"     \n```\n:::\n\n```{.r .cell-code}\ntokenized_dat<-tokenized_dat %>%\n  filter(!word %in% stopword()) %>%\n  filter(!word %in% c(\"‡∏Ñ‡πà‡∏∞\",\"‡∏Ñ‡∏£‡∏±‡∏ö\"))\ntokenized_dat %>% dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 747   4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsentiment_dat <- tokenized_dat %>%\n  inner_join(lexicon, by=\"word\")\nglimpse(sentiment_dat, width=60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 102\nColumns: 5\n$ text        <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\", \"‚Ä¶\n$ sentiment.x <chr> \"neg\", \"neg\", \"neg\", \"neg\", \"neg\", \"ne‚Ä¶\n$ id          <int> 1, 1, 1, 3, 6, 10, 12, 13, 14, 14, 15,‚Ä¶\n$ word        <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î\", \"‡∏ä‡∏≠‡∏ö\", \"‡∏´‡∏•‡∏≠‡∏Å\", \"‡πÄ‡∏ã‡πá‡∏á\", \"‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å\"‚Ä¶\n$ sentiment.y <chr> \"neg\", \"pos\", \"neg\", \"neg\", \"neg\", \"po‚Ä¶\n```\n:::\n:::\n\n\n#### calculate sentiment score\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nsentiment_dat %>%\n  mutate(sentiment_score = ifelse(sentiment.y==\"pos\",1,-1)) %>%\n  group_by(id) %>%\n  summarise(sentiment_score = sum(sentiment_score)) %>%\n  ggplot(aes(x=sentiment_score))+\n  geom_histogram(bins=5, col=\"white\")\n```\n\n::: {.cell-output-display}\n![](tidytext_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsentiment_dat %>%\n  mutate(sentiment_score = ifelse(sentiment.y==\"pos\",1,-1)) %>%\n#  group_by(id, word) %>%\n  count(word, sentiment.y, sort = TRUE) %>%\n  ggplot(aes(x=n, y=word, fill=sentiment.y))+\n  geom_col()+\n  facet_wrap(~sentiment.y)+\n  theme(text=element_text(family=\"ChulaCharasNew\"))\n```\n\n::: {.cell-output-display}\n![](tidytext_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n## Machine Learning based\n\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå sentiment ‡∏î‡πâ‡∏ß‡∏¢‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° ML ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏£‡∏Å ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ label ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå `sentiment` ‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á train model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nglimpse(dat, width=60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 128\nColumns: 3\n$ text      <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\", \"‡∏™‡∏¥‡∏ô‚Ä¶\n$ sentiment <chr> \"neg\", \"neg\", \"neg\", \"neg\", \"neg\", \"neg\"‚Ä¶\n$ id        <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1‚Ä¶\n```\n:::\n:::\n\n\n‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á tidymodels ‡πÇ‡∏î‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡∏ú‡∏π‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ö‡πâ‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Å‡πà‡∏≠‡∏ô\n\n\n::: {.cell}\n\n```{.r .cell-code}\npythainlp <- import(\"pythainlp\")\n# Define a custom tokenization function for textrecipes\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\")\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n## tokenized via mutate function\ntemp<-dat %>%\n  mutate(token = tokenize_thai(text))\nhead(temp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                            text\n1                                              ‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\n2                                                                   ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏î‡πâ‡∏≠‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û\n3                               ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏´‡∏°‡∏î‡∏ó‡∏≥‡πÑ‡∏°‡πÑ‡∏°‡πà‡πÅ‡∏à‡πâ‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏´‡∏°‡∏î‡∏≠‡∏∞.‡∏Å‡∏î‡πÉ‡∏™‡πà‡∏ï‡∏∞‡∏Å‡∏£‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏ñ‡∏≠‡∏∞.‡πÄ‡∏ã‡πá‡∏á‡πÄ‡∏£‡∏¢\n4                                                    ‡∏≠‡∏¢‡πà‡∏≤‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏Å‡∏±‡∏ö‡πÉ‡∏Ñ‡∏£‡∏≠‡∏µ‡∏Å‡∏ô‡∏∞‡∏Ñ‡πà‡∏∞‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞\n5 ‡∏Ñ‡∏∑‡∏≠‡πÄ‡πÄ‡∏ö‡∏ö‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏°‡∏≤‡∏Å ‡πÄ‡πÄ‡∏¢‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÑ‡∏î‡πâ‡∏Ç‡∏≠‡∏á‡∏°‡∏≤‡∏ù‡∏≤‡πÄ‡πÄ‡∏ï‡∏Å ‡πÄ‡πÄ‡∏ï‡∏Å‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡πÄ‡πÄ‡∏ï‡∏Å‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏û‡∏≠‡∏à‡∏ö‡πÄ‡πÄ‡∏¢‡∏Åüò¢\n6                                                    ‡∏™‡πà‡∏á‡∏ä‡πâ‡∏≤‡∏à‡∏±‡∏á ‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏Å‡πâ‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n  sentiment id\n1       neg  1\n2       neg  2\n3       neg  3\n4       neg  4\n5       neg  5\n6       neg  6\n                                                                                                         token\n1                                                                 ‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î   ‡∏ä‡∏≠‡∏ö ‡∏´‡∏•‡∏≠‡∏Å ‡πÉ‡∏´‡πâ ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠   ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ ‡πÑ‡∏°‡πà ‡∏°‡∏µ ‡∏™‡∏¥‡πâ‡∏ô ‡∏Ñ‡πâ‡∏≤\n2                                                                                               ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏î‡πâ‡∏≠‡∏¢ ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û\n3                                             ‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‡∏´‡∏°‡∏î ‡∏ó‡∏≥‡πÑ‡∏° ‡πÑ‡∏°‡πà ‡πÅ‡∏à‡πâ‡∏á ‡∏Ç‡∏∂‡πâ‡∏ô ‡∏ß‡πà‡∏≤ ‡∏´‡∏°‡∏î ‡∏≠‡∏∞ .‡∏Å‡∏î ‡πÉ‡∏™‡πà ‡∏ï‡∏∞‡∏Å‡∏£‡πâ‡∏≤ ‡πÑ‡∏õ ‡πÄ‡∏ñ‡∏≠‡∏∞ . ‡πÄ‡∏ã‡πá‡∏á ‡πÄ‡∏£‡∏¢\n4                                                                         ‡∏≠‡∏¢‡πà‡∏≤ ‡∏ó‡∏≥ ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏Å‡∏±‡∏ö ‡πÉ‡∏Ñ‡∏£ ‡∏≠‡∏µ‡∏Å ‡∏ô‡∏∞ ‡∏Ñ‡πà‡∏∞ ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡∏Ñ‡πà‡∏∞\n5 ‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡πÄ‡∏ö‡∏ö ‡∏ú‡∏¥‡∏î‡∏´‡∏ß‡∏±‡∏á ‡∏°‡∏≤‡∏Å   ‡πÄ‡πÄ‡∏¢‡πà ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÑ‡∏î‡πâ ‡∏Ç‡∏≠‡∏á ‡∏°‡∏≤ ‡∏ù‡∏≤ ‡πÄ ‡πÄ‡∏ï‡∏Å   ‡πÄ ‡πÄ‡∏ï‡∏Å ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡πÑ‡∏°‡πà ‡∏ß‡πà‡∏≤ ‡πÄ ‡πÄ‡∏ï‡∏Å ‡πÄ‡∏¢‡∏≠‡∏∞ ‡∏°‡∏≤‡∏Å   ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡∏û‡∏≠ ‡∏à‡∏ö ‡πÄ ‡πÄ‡∏¢ ‡∏Åüò¢\n6                                                                       ‡∏™‡πà‡∏á ‡∏ä‡πâ‡∏≤ ‡∏à‡∏±‡∏á   ‡∏£‡∏≠ ‡∏ô‡∏≤‡∏ô ‡∏°‡∏≤‡∏Å   ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å ‡∏Å‡πâ‡∏≠ ‡πÑ‡∏°‡πà ‡πÑ‡∏î‡πâ\n```\n:::\n\n```{.r .cell-code}\ntokenized_dat <- temp %>%\n  unnest_tokens(input = token, token = \"words\", \n                output = word)\n# remove stopwords\nsource_python(\"stopword.py\")\nlength(stopword())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1030\n```\n:::\n\n```{.r .cell-code}\nhead(stopword(),10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"‡πÄ‡∏™‡∏µ‡∏¢\"      \"‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà\"     \"‡πÇ‡∏î‡∏¢\"      \"‡∏ó‡∏±‡πâ‡∏á‡∏ó‡∏µ\"      \"‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà\"      \"‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏ô‡∏±‡πâ‡∏ô\"\n [7] \"‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö\"   \"‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢\"     \"‡∏£‡∏∞‡∏¢‡∏∞\"     \"‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ß‡πà‡∏≤\"  \n```\n:::\n\n```{.r .cell-code}\ntail(stopword(),10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"‡πÅ‡∏°‡πâ‡∏Å‡∏£‡∏∞‡∏ó‡∏±‡πà‡∏á\"  \"‡∏ï‡πà‡∏≠\"       \"‡∏Ñ‡∏£‡∏≤‡∏ó‡∏µ‡πà\"     \"‡∏ñ‡∏∂‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\" \"‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô\"   \"‡∏ó‡∏∏‡∏Å‡∏ó‡∏µ‡πà\"     \n [7] \"‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ß‡∏≤‡∏ô\"   \"‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô\"    \"‡∏Ñ‡πà‡∏∞\"       \"‡πÉ‡∏´‡∏ç‡πà\"     \n```\n:::\n\n```{.r .cell-code}\ntokenized_dat<-tokenized_dat %>%\n  filter(!word %in% stopword()) %>%\n  filter(!word %in% c(\"‡∏Ñ‡πà‡∏∞\",\"‡∏Ñ‡∏£‡∏±‡∏ö\",\"‡πÄ\",\"‡πÄ‡∏ï‡∏Å\",\"‡∏≠‡∏∞\",\"10\",\"‡∏õ\",\"‡∏Å‡πä‡∏≠\",\n                      \"‡πÜ‡πÜ‡πÜ‡πÜ\",\"‡πÜ\",\"‡πÜ‡πÜ‡πÜ\",\"‡πÜ‡πÜ\",\"‡πÉ‡∏î\",\"‡∏Ñ\",\"‡∏£\",\n                      \"‡∏™\",\"‡∏á\",\"‡∏ß‡∏à‡∏∞\"))\nglimpse(tokenized_dat, width=60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 743\nColumns: 4\n$ text      <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î ‡∏ä‡∏≠‡∏ö‡∏´‡∏•‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡∏™‡∏±‡πà‡∏á‡∏ã‡∏∑‡πâ‡∏≠ ‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡πâ‡∏ô‡∏Ñ‡πâ‡∏≤\", \"‡πÄ‡∏Å‚Ä¶\n$ sentiment <chr> \"neg\", \"neg\", \"neg\", \"neg\", \"neg\", \"neg\"‚Ä¶\n$ id        <int> 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3‚Ä¶\n$ word      <chr> \"‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î\", \"‡∏ä‡∏≠‡∏ö\", \"‡∏´‡∏•‡∏≠‡∏Å\", \"‡∏™‡∏±‡πà‡∏á\", \"‡∏ã‡∏∑‡πâ‡∏≠\", \"‡∏Ñ‡πâ‡∏≤\"‚Ä¶\n```\n:::\n:::\n\n\n‡∏ô‡∏≥‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡∏°‡∏≤ train ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà logistic regression ‡πÅ‡∏•‡∏∞ random forest\n\n‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å (sentiment) ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥/‡∏ß‡∏•‡∏µ/‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô feature matrix ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ feature matrix ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏à‡∏∞‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ñ‡∏∑‡∏≠ ‡πÅ‡∏ñ‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ö‡∏ß‡∏Å‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏é‡∏≠‡∏¢‡∏π‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏°‡∏µ‡∏™‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà Term Frequency matrix (TF matrix), Inverse Document Frequency matrix (IDF matrix) ‡πÅ‡∏•‡∏∞ TF-IDF matrix ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏µ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\n\n-   TF matrix ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏¥‡∏î‡∏Ñ‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°/‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏à‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏ö‡∏Ñ‡∏≥‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°/‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏±‡∏ô ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏ú‡∏π‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏°‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ\n-   IDF matrix ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å/‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°/‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ corpus) ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö $log(n)/n_j$ ‡πÄ‡∏°‡∏∑‡πà‡∏≠ $n$ ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏•‡∏∞ $n_j$ ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà $j$ ‡∏à‡∏≤‡∏Å‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥ ‡πÜ ‡πÉ‡∏î‡∏û‡∏ö‡∏°‡∏≤‡∏Å‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢ ‡πÜ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏¢‡∏≤‡∏Å ‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≥ ‡πÜ ‡πÉ‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Ñ‡∏≥‡∏î‡∏±‡∏á‡∏Å‡∏•‡πà‡∏≤‡∏ß‡∏à‡∏∂‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤\n-   TF-IDF matrix ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏™‡∏π‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á TF ‡πÅ‡∏•‡∏∞ IDF matrix ‡πÇ‡∏î‡∏¢ TF-IDF ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ‡∏ú‡∏•‡∏Ñ‡∏π‡∏ì‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á TF ‡∏Å‡∏±‡∏ö IDF ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥ TF-IDF matrix ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≠‡∏á‡πÄ‡∏°‡∏ó‡∏£‡∏¥‡∏Å‡∏ã‡πå‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nsplit<-initial_split(tokenized_dat, strata = sentiment)\ntrain<-training(split)\ntest<-testing(split)\n## create preprocessing recipe\ntrain_rec <- recipe(sentiment~text, data=train) %>%\n  step_tokenize(text) %>%\n  step_stopwords(text) %>%\n  step_tokenfilter(text) %>%\n  step_tfidf(text) %>%\n  step_normalize(all_numeric_predictors())\n## model specification 1\nlasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n## model specification 2\nrf_spec <- rand_forest(mtry = tune(),\n                       trees=300,\n                       min_n=tune()) %>%\n  set_engine(\"ranger\",importance = \"permutation\") %>%\n  set_mode(\"classification\")\n## create workflowset\nwf_set <- workflow_set(\n  preproc = list(train_rec),\n  models = list(lasso_spec, rf_spec)\n)\n## training \ndoParallel::registerDoParallel()\nset.seed(321)\nfolds <- vfold_cv(train, v = 10, repeats = 3, strata = sentiment)\nresult <- workflow_map(\n  wf_set,\n  resamples = folds,\n  grid = 50,\n  control = control_grid(save_pred = T),\n  metrics = metric_set(roc_auc, sens,spec)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n\n```{.r .cell-code}\nautoplot(result)\n```\n\n::: {.cell-output-display}\n![](tidytext_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ ‡πÇ‡∏î‡∏¢ random forest ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  arrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 √ó 9\n   wflow_id            .config preproc model .metric .esti‚Ä¶¬π  mean     n std_err\n   <chr>               <chr>   <chr>   <chr> <chr>   <chr>   <dbl> <int>   <dbl>\n 1 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 2 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 3 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 4 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 5 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 6 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 7 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 8 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n 9 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n10 recipe_logistic_reg Prepro‚Ä¶ recipe  logi‚Ä¶ roc_auc binary      1    30       0\n# ‚Ä¶ with 90 more rows, and abbreviated variable name ¬π‚Äã.estimator\n```\n:::\n:::\n\n\n‡∏•‡∏≠‡∏á‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• random forest ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ hyperparameters ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult %>%\n  extract_workflow_set_result(id = \"recipe_rand_forest\") %>%\n  collect_metrics() %>%\n  ggplot(aes(mtry, mean, color = .metric)) +\n  geom_line(size = 1.5, show.legend = FALSE) +\n  facet_wrap(~.metric) +\n  scale_x_log10()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](tidytext_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nresult %>%\n  extract_workflow_set_result(id = \"recipe_rand_forest\") %>%\n  collect_metrics() %>%\n  ggplot(aes(min_n, mean, color = .metric)) +\n  geom_line(size = 1.5, show.legend = FALSE) +\n  facet_wrap(~.metric) +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](tidytext_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n:::\n\n\n‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡∏à‡∏∞‡πÉ‡∏ä‡πâ ML ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ sentiment analysis ‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ logistic regression ‡∏à‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏ß‡πà‡∏≤ random forest (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏∞‡πÑ‡∏£?)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## the best logistic model\nbest<-result %>% \n  extract_workflow_set_result(id = \"recipe_logistic_reg\") %>%\n  show_best(n=1, metric = \"roc_auc\")\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 √ó 7\n   penalty .metric .estimator  mean     n std_err .config              \n     <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1 1.11e-10 roc_auc binary         1    30       0 Preprocessor1_Model01\n```\n:::\n\n```{.r .cell-code}\n## extract logistic regressionworkflow\nlogit_wf <- wf_set%>%\n  extract_workflow(id = \"recipe_logistic_reg\")\nfinal_logit <- logit_wf %>%\n  finalize_workflow(best)\nfinal_logit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: logistic_reg()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nLogistic Regression Model Specification (classification)\n\nMain Arguments:\n  penalty = 1.10842563201833e-10\n  mixture = 1\n\nComputational engine: glmnet \n```\n:::\n\n```{.r .cell-code}\n## sentiment analysis\nlibrary(vip)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vip'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:utils':\n\n    vi\n```\n:::\n\n```{.r .cell-code}\nfinal_logit %>%\n  last_fit(split) %>%\n  extract_fit_engine() %>%\n  vi() %>%\n  group_by(Sign) %>%\n  top_n(10, wt = abs(Importance)) %>%\n  ungroup() %>%\n  mutate(\n    Importance = abs(Importance),\n    Variable = str_remove(Variable, \"tfidf_text_\"),\n    Variable = fct_reorder(Variable, Importance)\n  ) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~Sign, scales = \"free_y\") +\n  labs(y = NULL)+\n  theme(text=element_text(family=\"ChulaCharasNew\"))\n```\n\n::: {.cell-output-display}\n![](tidytext_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n‡∏•‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å random forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## the best random forest\nbest<-result %>% \n  extract_workflow_set_result(id = \"recipe_rand_forest\") %>%\n  show_best(n=1, metric = \"roc_auc\")\nbest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 √ó 8\n   mtry min_n .metric .estimator  mean     n std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n1    83    12 roc_auc binary         1    30       0 Preprocessor1_Model01\n```\n:::\n\n```{.r .cell-code}\n## extract logistic regressionworkflow\nrf_wf <- wf_set%>%\n  extract_workflow(id = \"recipe_rand_forest\")\nfinal_rf <- rf_wf %>%\n  finalize_workflow(best)\nfinal_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n‚ïê‚ïê Workflow ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\nPreprocessor: Recipe\nModel: rand_forest()\n\n‚îÄ‚îÄ Preprocessor ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n5 Recipe Steps\n\n‚Ä¢ step_tokenize()\n‚Ä¢ step_stopwords()\n‚Ä¢ step_tokenfilter()\n‚Ä¢ step_tfidf()\n‚Ä¢ step_normalize()\n\n‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 83\n  trees = 300\n  min_n = 12\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n```\n:::\n\n```{.r .cell-code}\n## create model using whole training dataset\nrf_lastfit <-final_rf %>%\n  last_fit(split, metrics=metric_set(roc_auc, sens,spec))\n```\n:::\n\n\n‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_lastfit %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 √ó 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 sens    binary             1 Preprocessor1_Model1\n2 spec    binary             1 Preprocessor1_Model1\n3 roc_auc binary             1 Preprocessor1_Model1\n```\n:::\n:::\n\n\n‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_dat <- c(\"‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢\",\n             \"‡∏ñ‡πâ‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏î‡∏µ‡∏°‡∏≤‡∏Å\",\n             \"‡πÅ‡∏û‡∏Ñ‡∏Å‡∏•‡πà‡∏≠‡∏á‡∏°‡∏≤‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏Ç‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏µ‡∏¢\")\nnew_dat <- tibble(text = new_dat)\nrf_lastfit  %>%\n  extract_workflow() %>%\n  predict(new_data= new_dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 √ó 1\n  .pred_class\n  <fct>      \n1 neg        \n2 pos        \n3 neg        \n```\n:::\n:::\n\n\n# ‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô\n\n‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ\n\n‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏ñ‡∏î‡∏ñ‡∏≠‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏∑‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary <- read.csv(\"/Users/siwachoat/Downloads/Salary_Data (1).csv\")\nhead(salary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Experience Salary\n1        1.1  39343\n2        1.3  46205\n3        1.5  37731\n4        2.0  43525\n5        2.2  39891\n6        2.9  56642\n```\n:::\n\n```{.r .cell-code}\nfit <- lm(Salary ~ Experience, data= salary)\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Salary ~ Experience, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7958.0 -4088.5  -459.9  3372.6 11448.0 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  25792.2     2273.1   11.35 5.51e-12 ***\nExperience    9450.0      378.8   24.95  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5788 on 28 degrees of freedom\nMultiple R-squared:  0.957,\tAdjusted R-squared:  0.9554 \nF-statistic: 622.5 on 1 and 28 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ---\\> <https://forms.gle/PAXJzC1DjoiibpKE8>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat<-read_excel(\"answer.xlsx\")\nglimpse(dat, width=60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 203\nColumns: 5\n$ Timestamp  <dttm> 2022-10-31 17:15:09, 2022-10-31 20:45:‚Ä¶\n$ student_id <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ‚Ä¶\n$ section    <dbl> 2, 8, 2, 4, 7, 2, 8, 6, 1, 5, 1, 3, 3, ‚Ä¶\n$ answer     <chr> \"‡∏ñ‡πâ‡∏≤‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏£‡∏π‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô 1 ‡∏õ‡∏µ ‡∏à‡∏∞‡∏™‡πà‚Ä¶\n$ result     <chr> \"correct\", \"correct\", \"wrong\", \"correct‚Ä¶\n```\n:::\n:::\n\n\n‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏¥‡∏™‡∏¥‡∏ï‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô\n",
    "supporting": [
      "tidytext_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}