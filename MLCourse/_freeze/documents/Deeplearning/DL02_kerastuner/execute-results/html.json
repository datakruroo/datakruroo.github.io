{
  "hash": "809c24e8651409a925430c60ae284bca",
  "result": {
    "markdown": "---\ntitle: \"Hyperparameter Tuning using Keras Tuner\"\n---\n\n\nการเรียนรู้เชิงลึก (Deep Learning) เป็นแขนงย่อยของศาสตร์ด้านการเรียนรู้ของเครื่อง (machine learning) ที่พัฒนาขึ้นโดยใช้การทำงานของเซลล์ประสาทในสมองที่เรียกว่า neuron เป็นต้นแบบ เรียกโมเดลการเรียนรู้ดังกล่าวว่า โมเดลโครงข่ายประสาทเทียม (artificial neuron network models: ANNs)\n\n![ความแตกต่างระหว่าง ML กับ DL](https://editor.analyticsvidhya.com/uploads/21745d3.png)\n\n# ชุดข้อมูลที่ใช้เป็นตัวอย่าง\n\nชุดข้อมูลที่ใช้เป็นตัวอย่างจะใช้ชุดข้อมูล [**Sign Language Digits Dataset**](https://github.com/ardamavi/Sign-Language-Digits-Dataset)\n\n### Details of datasets:\n\n-   Image size: 100 x 100 pixels\n\n-   Color space: RGB\n\n-   Number of classes: 10 (Digits: 0-9)\n\n-   Number of participant students: 218\n\n-   Number of samples per student: 10\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem(\"git clone https://github.com/ardamavi/Sign-Language-Digits-Dataset.git\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndir(\"~/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/Deeplearning/sign\")\n```\n:::\n\n\nการนำเข้าไฟล์ข้อมูลทั้งสองสามารถทำได้โดยใช้ library `numpy` ซึ่งสามารถทำบน R ผ่าน reticulate library ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nnp <- import(\"numpy\")\nx_array <- np$load(\"sign/X.npy\")\ny_array <- np$load(\"sign/Y.npy\")\n```\n:::\n\n\nชุดข้อมูลดังกล่าวประกอบด้วยภาพภาษามือของตัวเลข 10 ตัว ตั้งแต่ 0, 1, 2, ..., 9 จำนวน 2,062 ภาพ โดยที่แต่ละภาพมีขนาด 64 x 64 pixels\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(x_array)\ndim(y_array)\n```\n:::\n\n\nในเบื้องต้นจะลอง train โมเดลเพื่อจำแนกภาษามือสำหรับตัวเลข 0 กับ 1 ก่อน เมื่อพิจารณาใน `y_array`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_array %>% head()\n```\n:::\n\n\nบทเรียนนี้จะกล่าวถึงการปรับแต่งค่า hyperparameters ในโมเดลโครงข่ายประสาทเทียม (neural network models) ซึ่งเป็นขั้นตอนสำคัญของการพัฒนาโมเดลการเรียนรู้เชิงลึก hyperparameters ของ neural network models มีหลายตัวได้แก่ จำนวน hidden layer จำนวน node ภายในแต่ละ layer ระดับของ learning rate และ activation function ที่เลือกใช้\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2)\n# Helper function to convert image data to a data frame for ggplot2\nprepare_image_data <- function(image_data) {\n  df <- melt(image_data)\n  colnames(df) <- c(\"x\", \"y\", \"value\")\n  return(df)\n}\n\n# Prepare image data for plotting\nimage_1 <- prepare_image_data(x_array[206,,])\nimage_2 <- prepare_image_data(x_array[4,,])\n\n# Plot images using ggplot2\np1 <- ggplot(image_1, aes(x = y, y = -x, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  coord_fixed() +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank())\n\np1\np2 <- ggplot(image_2, aes(x = y, y = -x, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  coord_fixed() +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank())\n\n# Display the plots\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n```\n:::\n\n\n# 1. Setup\n\nก่อนการดำเนินการใด ๆ ผู้วิเคราะห์จำเป็นต้องติดตั้งและเตรียมสภาพแวดล้อมการทำงานให้พร้อมก่อน ได้แก่ การติดตั้ง **`tensorflow`**, **`keras`** และ **`keras-tuner`** ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\npip install tensorflow\npip install keras\npip install keras-tuner\n```\n:::\n\n\nจากนั้นติดตั้งและเรียก reticulate\n\n\n::: {.cell}\n\n:::\n\n\n# 2. Import Python library\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnp <- import(\"numpy\")\ntf <- import(\"tensorflow\")\nkeras_tuner <- import(\"keras_tuner\")\nkeras <- import(\"keras\")\nlayers <- keras$layers\nRandomSearch <- import(\"kerastuner.tuners\")$RandomSearch\nHyperParameters <- import(\"kerastuner.engine.hyperparameters\")$HyperParameters\n```\n:::\n\n\n# 3. Load the data\n\nนำข้อมูลเข้าและจัดกระทำข้อมูลก่อนการวิเคราะห์\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ndat <- read.csv(\"criminal.csv\")\ndat <- dat %>% drop_na()\ndat <- recipe(TheifperPop ~., data = dat) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  prep(NULL) %>%\n  juice()\nsplit <- initial_split(dat)\ntrain <- training(split)\ntest <- testing(split)\n## convert training dataset to train_x and train_y matrix\ntrain_x <- train %>% select(-TheifperPop) %>% as.matrix()\ntrain_y <- train %>% select(TheifperPop) %>% as.matrix()\ntest_x <- test %>% select(-TheifperPop) %>% as.matrix()\ntest_y <- test %>% select(TheifperPop) %>% as.matrix()\n```\n:::\n\n\n# 4. Model Specification\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n# Build the model function\nbuild_model <- function(hp) {\n  model <- keras_model_sequential() %>%\n    layer_dense(units = hp$Int(\"units_1\", min_value = 32L, max_value = 512L, step = 32L), \n                activation = \"relu\", input_shape = 17L) %>%\n    layer_dense(units = hp$Int(\"units_2\", min_value = 32L, max_value = 512L, step = 32L), \n                activation = \"relu\") %>%\n    layer_dense(units = 1L)\n\n  model %>% compile(\n    optimizer = optimizer_adam(\n      learning_rate = hp$Float(\"learning_rate\", min_value = 1e-4, max_value = 1e-2, sampling = \"LOG\")\n    ),\n    loss = \"mse\",\n    metrics = \"mse\"\n  )\n\n  return(model)\n}\n\n# Configure the tuner\nRandomSearch <- keras_tuner$tuners$RandomSearch\n\ntuner <- RandomSearch(\n  build_model,\n  objective = \"val_mse\",\n  max_trials = 20L,\n  executions_per_trial = 2L,\n  directory = \"tuning_results\",\n  project_name = \"test_keras_tuner\"\n)\n\n# Prepare the data\n# Replace the following lines with your own dataset\n#train_x <- matrix(runif(1107 * 17), nrow = 1107) %>% class()\n#train_y <- matrix(runif(1107), nrow = 1107)\n#test_x <- matrix(runif(369 * 17), nrow = 369)\n#test_y <- matrix(runif(369), nrow = 369)\n\n# Set early stopping\nearly_stopping <- keras$callbacks$EarlyStopping(monitor = \"val_loss\", patience = 3L)\n\n# Search for the best hyperparameters\nresult<-tuner$search(\n  train_x,\n  train_y,\n  validation_data = list(test_x, test_y),\n  epochs = 10L,\n  batch_size = 256L,\n  callbacks = list(early_stopping)\n)\n```\n:::\n",
    "supporting": [
      "DL02_kerastuner_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}