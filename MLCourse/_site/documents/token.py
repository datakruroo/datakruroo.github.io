def tokenize_thai_sentence(text):
  import pythainlp
  import deepcut

  tokens = pythainlp.word_tokenize(text,
  keep_whitespace=False,
  engine="deepcut")
  return tokens
