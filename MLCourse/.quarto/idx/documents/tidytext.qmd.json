{"title":"Text Analytics","markdown":{"yaml":{"title":"Text Analytics"},"headingText":"Sentiment analysis","containsRefs":false,"markdown":"\n\nการวิเคราะห์ข้อความเป็นวิธีการวิเคราะห์ข้อมูลที่มีวัตถุประสงค์หลักคือเพื่อสกัดสารสนเทศที่มีความหมายออกมาจากข้อมูลแบบข้อความ ซึ่งเป็นข้อมูลแบบไม่มีโครงสร้างประเภทหนึ่ง การวิเคราะห์ข้อมูลสามารถจำแนกได้เป็นหลายประเภท ตามวัตถุประสงค์ของการวิเคราะห์ได้แก่\n\n-   sentiment analysis\n\n-   topic modeling\n\n-   text classification\n\n-   text summarization\n\n-   relationship extraction\n\n-   trend analysis\n\n-   keyword extraction\n\nเนื้อหาในส่วนนี้จะกล่าวถึงการวิเคราะห์ในบางตัวที่มีการใช้ ML เข้ามาช่วย รายละเอียดมีดังนี้\n\n\nการวิเคราะห์ความรู้สึก (sentiment analysis) เป็นการวิเคราะห์เพื่อวินิจฉัยอารมณ์หรือความรู้สึกของกลุ่มเป้าหมายโดยใช้ข้อความที่ได้จากการพูด การเขียน โดยปกติการวิเคราะหความรู้สึกจะให้สารสนเทศของความรู้สึกเป็น อารมณ์เชิงบวก เชิงลบ หรือกลาง ๆ นอกจากนี้ยังอาจจำแนกความรู้สึกให้ละเอียดได้ตามต้องการ เช่น โกรธ กลัว ขยะแขยง หรือ ดีใจ\n\nปัจจุบันมีการพัฒนาการวิเคราะห์ความรู้สึกหลายวิธีการ ซึ่งแต่ละวิธีการจะมีความเหมาะสมที่แตกต่างกัน ขึ้นอยู่กับลักษณะของปัญหา และลักษณะของข้อมูลที่ผู้วิเคราะห์มี วิธีการที่มีการใช้กันในปัจจุบัน ได้แก่\n\n1.  **lexicon-bases methods:** การประเมินความรู้สึกด้วยวิธีการนี้จะประเมินโดยอิงจากฐานข้อมูลของคำที่ได้มีการให้คะแนนความรู้สึก (sentiment score) เอาไว้ก่อนหน้าแล้ว เรียกฐานข้อมูลดังกล่าวว่า lexicons วิธีการนี้จะนำคะแนนความรู้สึกของคำแต่ละคำภายในประโยคที่สนใจมาหาค่าเฉลี่ย และสรุปแนวโน้มของอารมณ์ความรู้สึกจากค่าเฉลี่ยดังกล่าว\n2.  **machine learning methods:** วิธีการนี้มีการใช้อัลกอริทึมการเรียนรู้ของเครื่องเข้ามาช่วย หลักการคร่าว ๆ คือ ผู้วิเคราะห์จะต้องมีชุดข้อมูลฝึกหัดและทดสอบที่มีการให้ label กับข้อความเอาไว้ก่อนหน้า แล้วใช้ชุดข้อมูลดังกล่าวในการพัฒนาโมเดลทำนายอารมณ์ความรู้สึกจากข้อความ\n3.  **deep learning methods:** วิธีการนี้มีความคล้ายกับ ML methods แต่อัลกอริทึมที่ใช้ในการทำนายจะเป็นอัลกอริทึมในกลุ่ม deep learning เช่น RNNs, LSTM วิธีการนี้ถึงแม้จะเป็นวิธีการที่มีประสิทธิภาพสูง แต่มีข้อจำกัดคือต้องการข้อมูลขนาดใหญ่ในการวิเคราะห์\n4.  **hybrid methods**: เป็นวิธีการที่บูรณาการร่วมกันระหว่าง 3 วิธีการข้างต้น เช่น ผู้วิเคราะห์อาจใช้ lexicon-based ในกระบวนการ data preprocess และใช้อัลกอริทึมการเรียนรู้ของเครื่องเพื่อพัฒนาโมเดลทำนาย\n\n## Lexicon-based methods\n\nการวิเคราะห์ความรู้สึกด้วยวิธีการนี้สามารถทำใน R ได้หลายวิธี วิธีการหนึ่งที่แนะนำคือการใช้ package `tidytext` ข้อดีของการใช้ package ดังกล่าวคือสามารถใช้ grammar เดียวกับการทำงานบน tidyverse ซึ่งง่ายต่อการจัดกระทำ วิเคราะห์ นำเสนอและเชื่อมโยงกับเครื่องมือต่าง ๆ ที่จำเป็น ก่อนการดำเนินการวิเคราะห์จำเป็นต้องติดตั้งและเรียกใช้ package ต่อไปนี้ก่อน\n\n```{r message=F}\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidytext)\nlibrary(readxl)\n```\n\nชุดข้อมูลตัวอย่างที่ใช้\n\n```{r}\ndat <- read.csv(\"https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/review_shopping.csv\",\n                header=F, col.names = c(\"text\",\"sentiment\"),\n                sep = \"\\t\")\ndat$id <- 1:dim(dat)[1]\nhead(dat)\nglimpse(dat, width=60)\n```\n\nสร้าง dictionary สำหรับประเมินอารมณ์ความรู้สึกของข้อความ\n\n```{r}\nneg<- read.csv(\"https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/neg.txt\", header = F, col.names = \"word\")\npos <- read.csv(\"https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/pos.txt\", header = F, col.names = \"word\")\nneg$sentiment <- \"neg\"\npos$sentiment <- \"pos\"\nlexicon <- neg %>% bind_rows(pos)\nhead(lexicon)\ndim(lexicon)\nlexicon <- distinct(lexicon)\ndim(lexicon)\n```\n\n#### Tokenization\n\nจากลักษณะของ lexicon-based method ที่ต้องมีการเปรียบเทียบคำในชุดข้อมูลกับพจณานุกรมเพื่อให้คะแนน sentiment ของคำแต่ละคำภายในแต่ละข้อความ ผู้วิเคราะห์จึงจำเป็นต้องตัดข้อความในชุดข้อมูลออกเป็นคำย่อย ๆ ก่อนที่จะดำเนินการวิเคราะห์ กระบวนการตัดคำดังกล่าวเรียกว่า tokenization\n\nใน package tidytext การตัดคำสามารถทำได้ด้วยฟังก์ชัน `unnest_tokens()` ฟังก์ชันดังกล่าวมีอาร์กิวเมนท์สำคัญได้แก่ `tbl` คือชุดข้อมูลแบบ data.frame ที่เก็บข้อความทั้งหมด `token` ใช้กำหนดหน่วยของการตัดคำ เช่น `words` , `ngrams`, `skip_ngrams` , `sentences`, `lines` หรือ `paragraphs`\n\n```{r message=F}\nlibrary(reticulate)\n# Import pythainlp library\npythainlp <- import(\"pythainlp\")\n\n# Define a custom tokenization function for textrecipes\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"deepcut\")\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n\n## tokenized via mutate function\ntemp<-dat %>%\n  mutate(token = tokenize_thai(text))\nhead(temp)\n\ntokenized_dat <- temp %>%\n  unnest_tokens(input = token, token = \"words\", \n                output = word)\nglimpse(tokenized_dat, width=60)\n```\n\n#### stopword\n\nStopwords คือคำที่ไม่ให้ความหมายหรือไม่มีประโยชน์ในการตีความหมายของประโยคหรือข้อความเป้าหมาย เช่น a, and, the, และ หรือ อะ ซึ่งในชุดข้อมูลหากมีคำประเภทนี้อยู่มาก ๆ จะกลายเป็น noise ที่รบกวนการวิเคราะห์ ใน library pythainlp มีการรวบรวม stopwords สำหรับภาษาไทยเอาไว้พอสมควร ผู้วิเคราะห์สามารถนำมาใช้ได้ การเรียก stopword จาก pythinlp มาใช้สามารถทำได้ในทำนองเดียวกับการตัดคำดังนี้\n\n```{python eval=F}\ndef stop word():\n  import pythainlp\n  \n  stopword = pythainlp.corpus.common.thai_stopwords()\n  stopword = list(stopword)\n  return stopword\n```\n\nการเรียก stopword จาก pythainlp ออกมาใช้สามารถทำได้ดังนี้ จะเห็นว่าชุดข้อมูล stopword ดังกล่าวรวบรวมคำไว้จำนวน 1,030 คำ\n\n```{r}\nsource_python(\"stopword.py\")\nlength(stopword())\nhead(stopword(),10)\ntail(stopword(),10)\ntokenized_dat<-tokenized_dat %>%\n  filter(!word %in% stopword()) %>%\n  filter(!word %in% c(\"ค่ะ\",\"ครับ\"))\ntokenized_dat %>% dim()\n```\n\n```{r}\nsentiment_dat <- tokenized_dat %>%\n  inner_join(lexicon, by=\"word\")\nglimpse(sentiment_dat, width=60)\n```\n\n#### calculate sentiment score\n\n```{r}\nlibrary(ggplot2)\nsentiment_dat %>%\n  mutate(sentiment_score = ifelse(sentiment.y==\"pos\",1,-1)) %>%\n  group_by(id) %>%\n  summarise(sentiment_score = sum(sentiment_score)) %>%\n  ggplot(aes(x=sentiment_score))+\n  geom_histogram(bins=5, col=\"white\")\n```\n\n```{r}\nsentiment_dat %>%\n  mutate(sentiment_score = ifelse(sentiment.y==\"pos\",1,-1)) %>%\n#  group_by(id, word) %>%\n  count(word, sentiment.y, sort = TRUE) %>%\n  ggplot(aes(x=n, y=word, fill=sentiment.y))+\n  geom_col()+\n  facet_wrap(~sentiment.y)+\n  theme(text=element_text(family=\"ChulaCharasNew\"))\n```\n\n## Machine Learning based\n\nตัวอย่างส่วนนี้จะวิเคราะห์ sentiment ด้วยอัลกอริทึม ML ชุดข้อมูลที่ใช้จะใช้ชุดข้อมูลตัวอย่างเดียวกับตัวอย่างแรก โดยใช้ label ที่อยู่ในคอลัมน์ `sentiment` ที่มาพร้อมกับชุดข้อมูลดังกล่าวเพื่อสร้าง train model\n\n```{r message=F}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(textrecipes)\nglimpse(dat, width=60)\n```\n\nการสร้างโมเดลทำนายในตัวอย่างนี้จะใช้กรอบของ tidymodels โดยก่อนที่จะดำเนินการวิเคราะห์ ผู้วิเคราะห์จะดำเนินการตัดคำที่ไม่เกี่ยวข้องบ้างส่วนก่อน\n\n```{r}\npythainlp <- import(\"pythainlp\")\n# Define a custom tokenization function for textrecipes\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\")\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n## tokenized via mutate function\ntemp<-dat %>%\n  mutate(token = tokenize_thai(text))\nhead(temp)\n\ntokenized_dat <- temp %>%\n  unnest_tokens(input = token, token = \"words\", \n                output = word)\n# remove stopwords\nsource_python(\"stopword.py\")\nlength(stopword())\nhead(stopword(),10)\ntail(stopword(),10)\ntokenized_dat<-tokenized_dat %>%\n  filter(!word %in% stopword()) %>%\n  filter(!word %in% c(\"ค่ะ\",\"ครับ\",\"เ\",\"เตก\",\"อะ\",\"10\",\"ป\",\"ก๊อ\",\n                      \"ๆๆๆๆ\",\"ๆ\",\"ๆๆๆ\",\"ๆๆ\",\"ใด\",\"ค\",\"ร\",\n                      \"ส\",\"ง\",\"วจะ\"))\nglimpse(tokenized_dat, width=60)\n```\n\nนำชุดข้อมูลข้างต้นมา train ในตัวอย่างนี้จะใช้ 2 โมเดลได้แก่ logistic regression และ random forest\n\nการสร้างโมเดลทำนายอารมณ์ความรู้สึก (sentiment) ด้วยข้อความจะใช้คำ/วลี/ประโยคภายในข้อความที่ผ่านการจัดระเบียบและทำความสะอาดแล้วมาสร้างเป็น feature matrix สำหรับทำนาย feature matrix ของข้อความดังกล่าวจะมีลักษณะคือ แถวเป็นข้อความเต็มที่มีความหมายบวกหรือลบหรืออื่น ๆ ส่วนคอลัมน์เป็นความถี่ของคำแต่ละคำที่ปรากฎอยู่ภายในแต่ละข้อความ เมทริกซ์ดังกล่าวมีสามประเภท ได้แก่ Term Frequency matrix (TF matrix), Inverse Document Frequency matrix (IDF matrix) และ TF-IDF matrix รายละเอียดมีดังนี้\n\n-   TF matrix เป็นเมทริกซ์ที่ใช้แสดงความถี่ของการเกิดคำแต่ละคำภายในข้อความ/เอกสารเป้าหมาย โดยส่วนใหญ่ค่าความถี่ดังกล่าวจะรายงานเป็นร้อยละการพบคำดังกล่าวเทียบกับข้อความทั้งหมด หากมีแนวโน้มที่จะพบคำที่แตกต่างกันภายในข้อความ/เอกสารที่แตกต่างประเภทกัน แสดงว่ามีความสัมพันธ์กันระหว่างคำที่ใช้กับข้อความ ผู้วิเคราะห์สามารถใช้ความสัมพันธ์ดังกล่าวมาสร้างเป็นโมเดลทำนายได้\n-   IDF matrix เป็นเมทริกซ์ที่ให้ค่าน้ำหนัก/ความสำคัญของคำแต่ละคำภายในข้อความ/เอกสารเป้าหมายทั้งหมด (เรียกว่า corpus) ค่าน้ำหนักดังกล่าวมีค่าเท่ากับ $log(n)/n_j$ เมื่อ $n$ คือจำนวนข้อความทั้งหมด และ $n_j$ คือจำนวนข้อความที่พบคำที่ $j$ จากค่าน้ำหนักดังกล่าวจะเห็นว่า หากคำ ๆ ใดพบมากพบบ่อยในหลาย ๆ ข้อความ แสดงว่าคำนั้นเป็นคำทั่วไปและมีแนวโน้มที่จะมีความสามารถในการจำแนกข้อความที่มีความแตกต่างกันได้ยาก ในทางกลับกันคำ ๆ ใดที่พบน้อยแสดงว่าเป็นคำเฉพาะสำหรับข้อความบางประเภท ดังนั้นคำดังกล่าวจึงมีแนวโน้มที่จะใช้จำแนกข้อความที่มีความแตกต่างกันได้ดีกว่า\n-   TF-IDF matrix เป็นเมทริกซ์ที่รวมสูตรการคำนวณระหว่าง TF และ IDF matrix โดย TF-IDF จะคำนวณน้ำหนักของคำโดยใช้ ผลคูณระหว่าง TF กับ IDF วิธีการนี้จึงทำให้สามารถระบุน้ำหนักของคำต่าง ๆ ภายใต้ข้อความเป้าหมายได้ครอบคลุมมิติทั้งในด้านความบ่อยและความสำคัญไปได้พร้อมกัน โดยปกติ TF-IDF matrix เป็นเทคนิคที่ใช้มากกว่าสองเมทริกซ์ข้างต้น\n\n```{r}\nset.seed(123)\nsplit<-initial_split(tokenized_dat, strata = sentiment)\ntrain<-training(split)\ntest<-testing(split)\n## create preprocessing recipe\ntrain_rec <- recipe(sentiment~text, data=train) %>%\n  step_tokenize(text) %>%\n  step_stopwords(text) %>%\n  step_tokenfilter(text) %>%\n  step_tfidf(text) %>%\n  step_normalize(all_numeric_predictors())\n## model specification 1\nlasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%\n  set_engine(\"glmnet\")\n## model specification 2\nrf_spec <- rand_forest(mtry = tune(),\n                       trees=300,\n                       min_n=tune()) %>%\n  set_engine(\"ranger\",importance = \"permutation\") %>%\n  set_mode(\"classification\")\n## create workflowset\nwf_set <- workflow_set(\n  preproc = list(train_rec),\n  models = list(lasso_spec, rf_spec)\n)\n## training \ndoParallel::registerDoParallel()\nset.seed(321)\nfolds <- vfold_cv(train, v = 10, repeats = 3, strata = sentiment)\nresult <- workflow_map(\n  wf_set,\n  resamples = folds,\n  grid = 50,\n  control = control_grid(save_pred = T),\n  metrics = metric_set(roc_auc, sens,spec)\n)\nautoplot(result)\n```\n\nผลการวิเคราะห์ข้างต้นแสดงให้เห็นว่าทั้งสองโมเดลสามารถทำนายได้อยู่ในระดับที่ดีขึ้นไป โดย random forest เป็นโมเดลที่ทำนายได้ดีมากที่สุด\n\n```{r}\nresult %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  arrange(desc(mean))\n```\n\nลองพิจารณาประสิทธิภาพของโมเดล random forest ภายใต้ hyperparameters ที่กำหนด\n\n```{r}\nresult %>%\n  extract_workflow_set_result(id = \"recipe_rand_forest\") %>%\n  collect_metrics() %>%\n  ggplot(aes(mtry, mean, color = .metric)) +\n  geom_line(size = 1.5, show.legend = FALSE) +\n  facet_wrap(~.metric) +\n  scale_x_log10()\n\nresult %>%\n  extract_workflow_set_result(id = \"recipe_rand_forest\") %>%\n  collect_metrics() %>%\n  ggplot(aes(min_n, mean, color = .metric)) +\n  geom_line(size = 1.5, show.legend = FALSE) +\n  facet_wrap(~.metric) +\n  scale_x_log10()\n```\n\nผลการวิเคราะห์ส่วนแรกจะใช้ ML เพื่อทำ sentiment analysis ดังนั้นการใช้ logistic regression จะเหมาะสมกว่า random forest (เพราะอะไร?)\n\n```{r}\n## the best logistic model\nbest<-result %>% \n  extract_workflow_set_result(id = \"recipe_logistic_reg\") %>%\n  show_best(n=1, metric = \"roc_auc\")\nbest\n## extract logistic regressionworkflow\nlogit_wf <- wf_set%>%\n  extract_workflow(id = \"recipe_logistic_reg\")\nfinal_logit <- logit_wf %>%\n  finalize_workflow(best)\nfinal_logit\n## sentiment analysis\nlibrary(vip)\nfinal_logit %>%\n  last_fit(split) %>%\n  extract_fit_engine() %>%\n  vi() %>%\n  group_by(Sign) %>%\n  top_n(10, wt = abs(Importance)) %>%\n  ungroup() %>%\n  mutate(\n    Importance = abs(Importance),\n    Variable = str_remove(Variable, \"tfidf_text_\"),\n    Variable = fct_reorder(Variable, Importance)\n  ) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~Sign, scales = \"free_y\") +\n  labs(y = NULL)+\n  theme(text=element_text(family=\"ChulaCharasNew\"))\n```\n\nลองสร้างโมเดลทำนายจาก random forest\n\n```{r}\n## the best random forest\nbest<-result %>% \n  extract_workflow_set_result(id = \"recipe_rand_forest\") %>%\n  show_best(n=1, metric = \"roc_auc\")\nbest\n## extract logistic regressionworkflow\nrf_wf <- wf_set%>%\n  extract_workflow(id = \"recipe_rand_forest\")\nfinal_rf <- rf_wf %>%\n  finalize_workflow(best)\nfinal_rf\n## create model using whole training dataset\nrf_lastfit <-final_rf %>%\n  last_fit(split, metrics=metric_set(roc_auc, sens,spec))\n```\n\nประสิทธิภาพของโมเดลทำนายเป็นดังนี้\n\n```{r}\nrf_lastfit %>%\n  collect_metrics()\n```\n\nทดลองทำนายชุดข้อมูลใหม่\n\n```{r}\nnew_dat <- c(\"ซื้อมาแล้วใช้งานไม่ได้เลย\",\n             \"ถ้าราคาถูกกว่านี้จะดีมาก\",\n             \"แพคกล่องมาแย่มาก แต่ของไม่เสีย\")\nnew_dat <- tibble(text = new_dat)\nrf_lastfit  %>%\n  extract_workflow() %>%\n  predict(new_data= new_dat)\n```\n\n# กิจกรรมพัฒนาโมเดลตรวจการบ้าน\n\nขอให้นิสิตตอบคำถามต่อไปนี้\n\nพิจารณาผลการวิเคราะห์การถดถอยด้านล่าง โดยตัวแปรตามคือเงินเดือนอาจารย์ และตัวแปรอิสระคือประสบการณ์ทำงานของอาจารย์มหาวิทยาลัย\n\n```{r}\nsalary <- read.csv(\"/Users/siwachoat/Downloads/Salary_Data (1).csv\")\nhead(salary)\nfit <- lm(Salary ~ Experience, data= salary)\nsummary(fit)\n```\n\nตอบคำถามที่นี่ ---\\> <https://forms.gle/PAXJzC1DjoiibpKE8>\n\n```{r}\ndat<-read_excel(\"answer.xlsx\")\nglimpse(dat, width=60)\n```\n\nขอให้นิสิตใช้ข้อมูลข้างต้นเพื่อพัฒนาโมเดลทำนายสำหรับตรวจข้อสอบข้อเขียน\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"tidytext.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"visual","urlcolor":"steelblue","linkcolor":"steelblue","theme":{"light":["pandoc","../theme.scss"]},"mainfont":"Krub","code-copy":true,"title":"Text Analytics"},"extensions":{"book":{"multiFile":true}}}}}