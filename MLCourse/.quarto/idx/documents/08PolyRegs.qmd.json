{"title":"Polynomial Regression and MARs","markdown":{"yaml":{"title":"Polynomial Regression and MARs","author":"ผศ.ดร.สิวะโชติ ศรีสุทธิยากร","toc":true,"toc-depth":3,"toc-title":"สารบัญ","theme":"default"},"headingText":"Polynomial Regression","containsRefs":false,"markdown":"\n\nMultiple regression และ regression โมเดลที่เกี่ยวข้องในบทเรียนก่อนหน้านี้เป็นโมเดลที่อยู่ภายใต้กลุ่มของโมเดลเชิงเส้น (linear model) ที่มีวัตถุประสงค์หลักคือเพื่อเรียนรู้ความสัมพันธ์ภายในข้อมูลที่มีรูปแบบเชิงเส้นตรงเป็นหลัก อย่างไรก็ตามก็อาจมีบางโมเดลที่สามารถใช้เรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้นได้ คือ regression model with interaction อย่างไรก็ตามโมเดล regression ดังกล่าวก็ยังมีความยืดหยุ่นที่ไม่มากเมื่อเปรียบเทียบกับอัลกอริทึมอื่น ๆ ที่มีในปัจจุบัน โดยบทเรียนนี้จะกล่าวถึงอัลกอริทึม multivariate adaptive regression splines (MARs) ที่กล่าวได้ว่าเป็น linear model ประเภทหนึ่งแต่มีความสามารถสูงในการเรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้น นอกจากนี้ยังสามารถใช้ได้ทั้งในปัญหาแบบ regression และ classification\n\n\nMARs เป็นโมเดลที่พัฒนาขึ้นโดยมีพื้นฐานมาจาก polynomial regression หัวข้อนี้จึงจะกล่าวถึงมโนทัศน์ของ polynomial regression ก่อนเพื่อเป็นพื้นฐานในการทำความเข้าใจ MARs ในหัวข้อถัดไป\n\n## Basic concept\n\nโมเดลการถดถอยพหุนาม (polynomial regression models) เป็นโมเดลการถดถอยแบบเชิงเส้นที่ใช้วิเคราะห์หรือเรียนรู้ความสัมพันธ์ในข้อมูลแบบที่ไม่ใช่เชิงเส้นตรง โดยใช้ฟังก์ชันพหุนาม (polynomial function) เป็นส่วน systematic part ของโมเดลแทนการใช้ฟังก์ชันเชิงเส้นตรงธรรมดา ลองพิจารณาความสัมพันธ์\n\n```{r message = F}\nlibrary(ggplot2)\nlibrary(dplyr)\nset.seed(123)\nx<-runif(1000,0,8)\ny<-rnorm(1000, sin(x), 0.5) \ndata <- data.frame(x,y)\ndata %>% ggplot()+\n  geom_point(aes(x=x, y=y),col=\"steelblue\", alpha=0.6)+\n  theme_light()\n```\n\nจะเห็นว่าความสัมพันธ์ข้างต้นมีลักษณะเป็นเส้นโค้ง การ fit ความสัมพันธ์ดังกล่าวด้วย regression model สามารถทำได้หลายวิธีการ วิธีการแรกคือการใช้ linear model เหมือนเดิมแต่มีการเพิ่ม term พหุนามที่มีดีกรีต่าง ๆ สมการถดถอยพหุนามมีสมการทั่วไปดังนี้\n\n$$\ny_i = \\beta_0+\\beta_1 x_i + \\beta_2 x^2_i + \\beta_3 x^3_i + ...+ \\beta_p x^p_i + \\epsilon_i\n$$\n\nคำสั่งต่อไปนี้แสดงการ fit สมการถดถอยพหุนาม degree 2,3 และ 4 กับข้อมูลข้างต้น (<https://en.wikipedia.org/wiki/Degree_of_a_polynomial>)\n\n```{r}\nlibrary(tidyr)\nlinear.fit <- lm(y~x, data = data)\npoly2.fit <- lm(y~x + I(x^2), data = data)\npoly3.fit <- lm(y~x+I(x^2)+I(x^3), data=data)\npoly4.fit <- lm(y~x+I(x^2)+I(x^3)+I(x^4), data=data)\nsummary(poly2.fit)\nsummary(poly3.fit)\nsummary(poly4.fit)\n\ndata %>% bind_cols(linear = predict(linear.fit),\n                   quadratic = predict(poly2.fit),\n                   cubic = predict(poly3.fit),\n                   quartic = predict(poly4.fit)) %>%\n  gather(linear:quartic, key = \"model\", value = \"pred\") %>%\n  mutate(model = factor(model, levels=c(\"linear\",\n                                        \"quadratic\",\n                                        \"cubic\",\n                                        \"quartic\"))) %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point(col = \"steelblue\", alpha=0.7)+\n  geom_line(aes(y=pred))+\n  facet_wrap(vars(model))\n```\n\nปัญหาของ polynomial regression คือ multicollinearity\n\n```{r message=F}\nlibrary(car)\nvif(poly2.fit)\nvif(poly3.fit)\n```\n\nการแก้ปัญหา multicollinearity สามารถทำได้หลายวิธีการ วิธีการแรกคือการ centering ตัวแปรอิสระ ดังนี้\n\n```{r message=F}\ndata %>%bind_cols(\n  pred2 = data %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2),data=.) %>%\n   predict(.)\n  )%>%\n    ggplot(aes(x=x,y=y))+\n    geom_point(col=\"steelblue\",alpha=0.6)+\n    geom_line(aes(y=pred2))\n\ndata %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2),data=.) %>% \n  vif()\n\ndata %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2)+I(x_c^3),data=.) %>% \n  vif()\n```\n\nอีกวิธีการหนึ่งคือการแปลงตัวแปรอิสระที่มี degree ของโมเดลให้เป็นด้วยพหุนามเชิงตั้งฉาก (orthogonal polynomial) <http://home.iitk.ac.in/~shalab/regression/Chapter12-Regression-PolynomialRegression.pdf>\n\nการสร้างเทอมพหุนามเชิงตั้งฉากใน R สามารถทำได้โดยใช้ฟังก์ชัน `poly()` ดังนี้\n\n```{r}\npoly2.fit <- lm(y~poly(x,2), data=data)\nsummary(poly2.fit)\ndata %>% bind_cols(pred = predict(poly2.fit)) %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point()+\n  geom_line(aes(y=pred), col=\"orange\", linewidth = 1.5)\n```\n\n## Polynomial Regression in tidymodels\n\n```{r}\nlibrary(tidymodels)\ndat <- read.csv(\"https://raw.githubusercontent.com/ssiwacho/2758688_ML/main/week%201/TeacherSalaryData.csv\")\nhead(dat)\nset.seed(1234)\nsplit <- initial_split(dat, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\ntrain<-train %>%\n  mutate(salary = log(salary))\n```\n\n### pre-processing\n\n```{r}\npreproc <- recipe(salary ~ ., data= train) %>%\n  step_select(-X)%>%\n  step_poly(yrs.service, yrs.since.phd, degree = tune()) %>%\n  step_dummy(rank, discipline, sex) %>%\n  step_interact(terms = ~starts_with(\"yrs.service\"):contains(\"discipline\"))\n```\n\n### model specification\n\n```{r}\npoly_mod <- linear_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"regression\")\n```\n\n### set workflow and tuning\n\n```{r message=F}\npoly_workflow <- workflow() %>%\n  add_recipe(preproc) %>%\n  add_model(poly_mod)\n\nfolds <- vfold_cv(train, v=5, repeats=2)\nparams = parameters(degree(range=c(1,4)),\n                    penalty(),\n                    mixture())\nmy_grid <- grid_max_entropy(params, size=30)\nlibrary(doMC)\nregisterDoMC(cores = parallel::detectCores())\ntune_results <- poly_workflow %>% \n  tune_grid(resamples = folds,\n            grid = my_grid,\n            control = control_grid(verbose = T,\n                                   save_pred = T)\n            )\ntune_results %>% autoplot()\nmybest<-show_best(tune_results, n=1, metric = \"rsq\")\n```\n\n```{r}\nlastfit <- poly_workflow %>%\n  finalize_workflow(mybest) %>%\n  last_fit(split)\n\n\nlastfit %>% collect_metrics()\nlastfit %>% collect_predictions() %>%\n  ggplot(aes(x=salary, y=.pred))+\n  geom_point()\n```\n\n## Piecewise Polynomial Regression\n\nการวิเคราะห์สมการถดถอยพหุนามในข้างต้นเป็นการกำหนดฟังก์ชันให้กับข้อมูลทั้งชุดเหมือนกันทั้งหมด อย่างไรก็ตามมีวิธีการที่สามารถกำหนดฟังก์ชันพหุนามหลาย ๆ ตัวให้กับส่วนย่อยข้อมูลเป็นส่วน ๆ ไป ซึ่งช่วยให้ประสิทธิภาพการทำนายสูงขึ้นได้ ยกตัวอย่างในรูปด้านล่าง\n\n```{r}\ndata %>% ggplot()+\n  geom_point(aes(x=x, y=y),col=\"steelblue\", alpha=0.6)+\n  geom_vline(xintercept = 4, linetype=2, col=\"maroon\", linewidth = 1.2)+\n  theme_light()\n```\n\n```{r}\ndata %>% filter(x < 4) %>%\n  lm(y~poly(x,2), data=.) %>%\n  predict() -> pred1\n\ndata %>% filter(x >= 4) %>%\n  lm(y~x, data=.) %>%\n  predict() -> pred2\n\ntemp <- data\ntemp$pred<-NA\ntemp[temp$x>=4,\"pred\"]<-pred2\ntemp[temp$x<4,\"pred\"]<-pred1\nhead(temp)\ntemp %>% ggplot(aes(x=x))+\n  geom_point(aes(y=y),col=\"steelblue\", alpha=0.3)+\n  geom_vline(xintercept = 4, linetype=2, col=\"maroon\", linewidth = 1.2)+\n  geom_line(aes(y=pred,col= x>4) , linewidth=1.2)+\n  theme_light()\n```\n\nเรียกจุดที่ใช้แบ่งเพื่อเปลี่ยนโมเดลทำนายว่า knot อย่างไรก็ตามการ fit piecewise regression ด้วยวิธีการข้างต้นจะเปิดปัญหาความไม่ต่อเนื่องระหว่างรอยต่อ (discontinuity problem) การแก้ปัญหาดังกล่าวสามารถทำได้โดยใช้เงื่อนไขเพิ่มเติมในขั้นตอนการประมาณค่าพารามิเตอร์ โดยแทนที่จะประมาณค่าพารามิเตอร์แยกโมเดลก็ให้ประมาณค่ารวมทั้งโมเดลดังนี้\n\n$$\ny = \\beta_0 + \\sum_{d=1}^D \\beta_d x^d + \\sum_{k=1}^K \\beta_k (x-\\tau)^K\n$$\n\nโดยที่ $(x-\\tau)^K = (x-\\tau)^K \\ \\;x \\geq \\tau$ และ $(x-\\tau)^K =0 \\ \\ ;x<\\tau$\n\n```{r}\ndata %>%\n  mutate(d = ifelse(x>=4,1,0),\n         x2 = x-4) %>%\n  lm(y ~ poly(x,2) + poly(x2*d,2), data=.)%>%\n  predict()->pred\n\ntemp$pred2<-pred\ntemp%>%ggplot(aes(x=x))+\n  geom_point(aes(y=y),col=\"steelblue\", alpha=0.3)+\n  geom_vline(xintercept = 4, linetype=2, col=\"maroon\", linewidth = 1.2)+\n  geom_line(aes(y=pred2,col= x>4) , linewidth=1.2)+\n  theme_light()\n```\n\n### Piecewise polynomial using tidymodels\n\n```{r eval=F}\nsplit <- initial_split(data, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\nhead(train)\n\nbase_rec <- recipe(y~., data=train)\n\n# basis spline\nbs_rec <- base_rec %>%\n  step_bs(x, degree = 1, knots=1)\n\n# natural spline\nns_rec <- base_rec %>%\n  step_ns(x, deg_free = tune())\n\nfit_spline1<-linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nfit<-fit_spline1 %>%\n  fit(y~.,data=bs_rec%>%prep()%>%juice())\n```\n\n```{r eval=F}\nbase_rec <- recipe(y~., data=train)\n\n# basis spline\nbs_rec <- base_rec %>%\n  step_bs(x, degree = tune())\n\n# natural spline\nns_rec <- base_rec %>%\n  step_ns(x, deg_free = tune())\n\nfit_spline1<-linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n\nfolds<-vfold_cv(train, v=5, repeats=3)\ntune_results<-workflow()%>%\n  add_recipe(ns_rec) %>%\n  add_model(fit_spline1) %>%\n  tune_grid(resamples=folds,\n            grid=20,\n            control = control_grid(verbose = T,\n                                   save_pred = T))\ntune_results %>% autoplot()\n\nworkflow()%>%\n  add_recipe(ns_rec) %>%\n  add_model(fit_spline1) %>%\n  finalize_workflow(show_best(tune_results,1, metric = \"rsq\"))%>%\n  last_fit(split) %>%\n  collect_predictions() \n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"output-file":"08PolyRegs.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","urlcolor":"steelblue","linkcolor":"steelblue","theme":{"light":["pandoc","../theme.scss"]},"mainfont":"Krub","code-copy":true,"title":"Polynomial Regression and MARs","author":"ผศ.ดร.สิวะโชติ ศรีสุทธิยากร","toc-title":"สารบัญ"},"extensions":{"book":{"multiFile":true}}}}}