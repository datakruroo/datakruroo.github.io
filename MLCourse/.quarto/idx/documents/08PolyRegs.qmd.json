{"title":"Polynomial Regression and MARs","markdown":{"yaml":{"title":"Polynomial Regression and MARs","author":"ผศ.ดร.สิวะโชติ ศรีสุทธิยากร","toc":true,"toc-depth":3,"toc-title":"สารบัญ","theme":"default"},"headingText":"Polynomial Regression","containsRefs":false,"markdown":"\n\nMultiple regression และ regression โมเดลที่เกี่ยวข้องในบทเรียนก่อนหน้านี้เป็นโมเดลที่อยู่ภายใต้กลุ่มของโมเดลเชิงเส้น (linear model) ที่มีวัตถุประสงค์หลักคือเพื่อเรียนรู้ความสัมพันธ์ภายในข้อมูลที่มีรูปแบบเชิงเส้นตรงเป็นหลัก อย่างไรก็ตามก็อาจมีบางโมเดลที่สามารถใช้เรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้นได้ คือ regression model with interaction อย่างไรก็ตามโมเดล regression ดังกล่าวก็ยังมีความยืดหยุ่นที่ไม่มากเมื่อเปรียบเทียบกับอัลกอริทึมอื่น ๆ ที่มีในปัจจุบัน โดยบทเรียนนี้จะกล่าวถึงอัลกอริทึม multivariate adaptive regression splines (MARs) ที่กล่าวได้ว่าเป็น linear model ประเภทหนึ่งแต่มีความสามารถสูงในการเรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้น นอกจากนี้ยังสามารถใช้ได้ทั้งในปัญหาแบบ regression และ classification\n\n\nMARs เป็นโมเดลที่พัฒนาขึ้นโดยมีพื้นฐานมาจาก polynomial regression หัวข้อนี้จึงจะกล่าวถึงมโนทัศน์ของ polynomial regression ก่อนเพื่อเป็นพื้นฐานในการทำความเข้าใจ MARs ในหัวข้อถัดไป\n\n## Basic concept\n\nโมเดลการถดถอยพหุนาม (polynomial regression models) เป็นโมเดลการถดถอยแบบเชิงเส้นที่ใช้วิเคราะห์หรือเรียนรู้ความสัมพันธ์ในข้อมูลแบบที่ไม่ใช่เชิงเส้นตรง โดยใช้ฟังก์ชันพหุนาม (polynomial function) เป็นส่วน systematic part ของโมเดลแทนการใช้ฟังก์ชันเชิงเส้นตรงธรรมดา ลองพิจารณาความสัมพันธ์\n\n```{r message = F}\nlibrary(ggplot2)\nlibrary(dplyr)\nset.seed(123)\nx<-runif(1000,0,8)\ny<-rnorm(1000, sin(x), 0.5) \ndata <- data.frame(x,y)\ndata %>% ggplot()+\n  geom_point(aes(x=x, y=y),col=\"steelblue\", alpha=0.6)+\n  theme_light()\n```\n\nจะเห็นว่าความสัมพันธ์ข้างต้นมีลักษณะเป็นเส้นโค้ง การ fit ความสัมพันธ์ดังกล่าวด้วย regression model สามารถทำได้หลายวิธีการ วิธีการแรกคือการใช้ linear model เหมือนเดิมแต่มีการเพิ่ม term พหุนามที่มีดีกรีต่าง ๆ สมการถดถอยพหุนามมีสมการทั่วไปดังนี้\n\n$$\ny_i = \\beta_0+\\beta_1 x_i + \\beta_2 x^2_i + \\beta_3 x^3_i + ...+ \\beta_p x^p_i + \\epsilon_i\n$$\n\nคำสั่งต่อไปนี้แสดงการ fit สมการถดถอยพหุนาม degree 2,3 และ 4 กับข้อมูลข้างต้น (<https://en.wikipedia.org/wiki/Degree_of_a_polynomial>)\n\n```{r}\nlibrary(tidyr)\nlinear.fit <- lm(y~x, data = data)\npoly2.fit <- lm(y~x + I(x^2), data = data)\npoly3.fit <- lm(y~x+I(x^2)+I(x^3), data=data)\npoly4.fit <- lm(y~x+I(x^2)+I(x^3)+I(x^4), data=data)\nsummary(poly2.fit)\nsummary(poly3.fit)\nsummary(poly4.fit)\n\ndata %>% bind_cols(linear = predict(linear.fit),\n                   quadratic = predict(poly2.fit),\n                   cubic = predict(poly3.fit),\n                   quartic = predict(poly4.fit)) %>%\n  gather(linear:quartic, key = \"model\", value = \"pred\") %>%\n  mutate(model = factor(model, levels=c(\"linear\",\n                                        \"quadratic\",\n                                        \"cubic\",\n                                        \"quartic\"))) %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point(col = \"steelblue\", alpha=0.7)+\n  geom_line(aes(y=pred))+\n  facet_wrap(vars(model))\n```\n\nปัญหาของ polynomial regression คือ multicollinearity\n\n```{r message=F}\nlibrary(car)\nvif(poly2.fit)\nvif(poly3.fit)\n```\n\nการแก้ปัญหา multicollinearity สามารถทำได้หลายวิธีการ วิธีการแรกคือการ centering ตัวแปรอิสระ ดังนี้\n\n```{r message=F}\ndata %>%bind_cols(\n  pred2 = data %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2),data=.) %>%\n   predict(.)\n  )%>%\n    ggplot(aes(x=x,y=y))+\n    geom_point(col=\"steelblue\",alpha=0.6)+\n    geom_line(aes(y=pred2))\n\ndata %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2),data=.) %>% \n  vif()\n\ndata %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2)+I(x_c^3),data=.) %>% \n  vif()\n```\n\nอีกวิธีการหนึ่งคือการแปลงตัวแปรอิสระที่มี degree ของโมเดลให้เป็นด้วยพหุนามเชิงตั้งฉาก (orthogonal polynomial) <http://home.iitk.ac.in/~shalab/regression/Chapter12-Regression-PolynomialRegression.pdf>\n\nการสร้างเทอมพหุนามเชิงตั้งฉากใน R สามารถทำได้โดยใช้ฟังก์ชัน `poly()` ดังนี้\n\n```{r}\npoly2.fit <- lm(y~poly(x,2), data=data)\nsummary(poly2.fit)\ndata %>% bind_cols(pred = predict(poly2.fit)) %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point()+\n  geom_line(aes(y=pred), col=\"orange\", linewidth = 1.5)\n```\n\n## Polynomial Regression in tidymodels\n\n```{r}\nlibrary(tidymodels)\ndat <- read.csv(\"https://raw.githubusercontent.com/ssiwacho/2758688_ML/main/week%201/TeacherSalaryData.csv\")\nhead(dat)\nset.seed(1234)\nsplit <- initial_split(dat, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\ntrain<-train %>%\n  mutate(salary = log(salary))\n```\n\n### pre-processing\n\n```{r eval=F}\npreproc <- recipe(salary ~ ., data= train) %>%\n  step_select(-X)%>%\n  step_poly(yrs.service, yrs.since.phd, degree = tune()) %>%\n  step_dummy(rank, discipline, sex)\n```\n\n### model specification\n\n```{r}\npoly_mod <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n```\n\n### set workflow and tuning\n\n```{r message=F, echo=F, eval=F}\npoly_workflow <- workflow() %>%\n  add_recipe(preproc) %>%\n  add_model(poly_mod)\n\nfolds <- vfold_cv(train, v=5, repeats=2)\nparams = parameters(degree(range=c(1,4)),\n                    penalty(),\n                    mixture())\nmy_grid <- grid_max_entropy(params, size=30)\nlibrary(doMC)\nregisterDoMC(cores = parallel::detectCores())\ntune_results <- poly_workflow %>% \n  tune_grid(resamples = folds,\n            grid = my_grid,\n            control = control_grid(verbose = T,\n                                   save_pred = T)\n            )\ntune_results %>% autoplot()\nmybest<-show_best(tune_results, n=1, metric = \"rsq\")\n```\n\n```{r echo=F, eval=F}\nlastfit <- poly_workflow %>%\n  finalize_workflow(mybest) %>%\n  last_fit(split)\n\n\nlastfit %>% collect_metrics()\nlastfit %>% collect_predictions() %>%\n  ggplot(aes(x=salary, y=.pred))+\n  geom_point()\n```\n\n### Syntax ในคาบเรียน\n\n```{r}\nlibrary(ggplot2)\nlibrary(dplyr)\nset.seed(123)\nx<-runif(1000,0,8)\ny<-rnorm(1000, sin(x), 0.5) \ndata <- data.frame(x,y)\ndata %>% ggplot()+\n  geom_point(aes(x=x, y=y),col=\"steelblue\", alpha=0.6)+\n  theme_light()\n\n## method 1\ndata<-data %>%\n  mutate(x2 = x^2,\n         x3 = x^3)\nhead(data)\n\nfit<-lm(y~x+x2+x3, data=data)\nsummary(fit)\npred.poly<-predict(fit)\nhead(pred.poly)\ndata$pred3<-pred.poly\nhead(data)\ndata %>% ggplot(aes(x=x))+\n  geom_point(aes(y=y),col=\"steelblue\", alpha=0.6)+\n  geom_line(aes(y=pred3),col=\"orange\",linewidth=1.5)+\n  theme_light()\n\n## method 2\nfit<-lm(y~x+I(x^2)+I(x^3), data=data)\nsummary(fit)\npred.poly<-predict(fit)\nhead(pred.poly)\ndata$pred3<-pred.poly\nhead(data)\ndata %>% ggplot(aes(x=x))+\n  geom_point(aes(y=y),col=\"steelblue\", alpha=0.6)+\n  geom_line(aes(y=pred3),col=\"orange\",linewidth=1.5)+\n  theme_light()\n\ndata %>% ggplot(aes(x = x, y= x2))+\n  geom_point()+\n  geom_smooth(method=\"lm\")\nlibrary(car)\nvif(fit)\n\n\n## method 3\ndata<-data %>% \n  mutate(x_c = x-mean(x),#centering\n         x_c2 = x_c^2,\n         x_c3 = x_c^3\n         ) \n\nfit<-lm(y~x_c+x_c2+x_c3, data=data)\nsummary(fit)\nvif(fit)\n\n## method 4: orthoginal polynomial\n\nfit<-lm(y~poly(x,3), data=data)\npred.poly<-predict(fit)\nhead(pred.poly)\ndata$pred_orthogonal<-pred.poly\nhead(data)\ndata %>% ggplot(aes(x=x))+\n  geom_point(aes(y=y),col=\"steelblue\", alpha=0.6)+\n  geom_line(aes(y=pred3),col=\"orange\",linewidth=1.5)+\n  geom_line(aes(y=pred_orthogonal),col=\"maroon\",linewidth=1.5)+\n  theme_light()\n```\n\n## Multivariate Adaptive Regression Spline (MARs)\n\nMultivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinear relationships in the data by assessing cutpoints (*knots*) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). (XXX, 2020)\n\nฟังก์ชันทำนายของ MARs มีความแตกต่างจาก regression model ทั่วไป กล่าวคือจะมีการแบ่งส่วน domain ของ feature เป็นช่วง ๆ จุดที่ใช้แบ่งส่วนเรียกว่า knot จากนั้นทำการ fit regression model ให้กับแต่ละส่วนด้วยโมเดลที่แตกต่างกัน\n\nลองพิจารณารูป A อัลกอริทึมจะเริ่มจากการหา knot ที่เหมาะสมที่สุดจาก domain ของ feature จากนั้นประมาณค่าพารามิเตอร์ในโมเดลดังนี้\n\n![](images/image-1750132179.png){width=\"60%\"}\n\nเมื่อได้ knot แรกแล้วอัลกอริทึมจะหา knot ต่อไปดังรูป B\n\n![](images/image-31836073.png){width=\"70%\"}\n\n![](images/image-1029173891.png)\n\n### MARs using tidymodels\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"output-file":"08PolyRegs.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"visual","urlcolor":"steelblue","linkcolor":"steelblue","theme":{"light":["pandoc","../theme.scss"]},"mainfont":"Krub","code-copy":true,"title":"Polynomial Regression and MARs","author":"ผศ.ดร.สิวะโชติ ศรีสุทธิยากร","toc-title":"สารบัญ"},"extensions":{"book":{"multiFile":true}}}}}