{"title":"Text PreProcessing","markdown":{"yaml":{"title":"Text PreProcessing"},"headingText":"Prerequired","containsRefs":false,"markdown":"\n\n\nเนื้อหาในบทความนี้จะกล่าวถึงการจัดระเบียบและจัดกระทำข้อมูลแบบข้อความด้วย R และ library PyThaiNLP ของ Python ก่อนการดำเนินการผู้วิเคราะห์จำเป็นต้องดำเนินการติดตั้งโปรแกรมและ library ที่จำเป็นก่อนดังนี้\n\n1.  ติดตั้ง R และ RStudio\n2.  ติดตั้ง Python และดำเนินการตั้งค่าตามขั้นตอนในลิงค์ <https://support.posit.co/hc/en-us/articles/360023654474-Installing-and-Configuring-Python-with-RStudio>\n3.  ติดตั้ง package reticulate บน R โดยพิมคำสั่ง `install.packages(\"reticulate\")` บน R console\n4.  ติดตั้ง PyThaiNLP โดยเข้าไปที่หน้า Terminal ใน RStudio จากนั้นพิมพ์คำสั่ง `pip install pythainlp`\n\nเมื่อดำเนินการในขั้นตอนทั้ง 4 เรียบร้อยแล้ว ขั้นตอนต่อมาคือการเรียกใช้ library pythainlp บน R โดยพิมพ์คำสั่งดังนี้\n\n```{r message = F}\nlibrary(tidyverse)\nlibrary(reticulate)\npythainlp <- import(\"pythainlp\")\n```\n\n**เอกสารอ้างอิง**\n\n-   [คู่มือการใช้งาน PyThaiNLP](https://pythainlp.readthedocs.io/en/latest/#pythainlp)\n-   <https://towardsdatascience.com/analysis-of-presidential-speeches-throughout-american-history-bb088d36d7dd>\n\n# Introduction\n\nการทำ text preprocessing เป็นขั้นตอนสำคัญสำหรับการดำเนินการวิเคราะห์ข้อความ วัตถุประสงค์ของการดำเนินงานในขั้นตอนนี้คือเพื่อ (1) จัดระเบียบและทำความสะอาดข้อความแบบข้อความ เพื่อสร้างชุดข้อมูลที่มีโครงสร้างจากข้อความดิบ และ (2) จัดกระทำข้อมูลเพื่อให้อยู่ในรูปแบบที่มีความสอดคล้องกับการวิเคราะห์ตามวัตถุประสงค์\n\nการทำ text preprocessing มีเทคนิคเฉพาะหลายตัวซึ่งผู้เรียนจำเป็นต้องรู้จักไว้ การเลือกใช้เทคนิคต่าง ๆ ขึ้นอยู่กับความต้องการของเทคนิคการวิเคราะห์ที่เลือกใช้ หรือสภาพ/ปัญหาที่พบในข้อมูลแต่ละชุด เทคนิคดังกล่าวมีดังนี้\n\n1.  **Lowercasing:** คือการแปลงตัวอักษรตัวใหญ่ให้เป็นตัวเล็ก (ใช้ในภาษาอังกฤษ)\n2.  **Tokenization:** คือการแปลงข้อความออกเป็นส่วนย่อย ๆ เช่น คำ/วลี หรือประโยค ที่สามารถนำไปวิเคราะห์ได้อย่างมีความหมาย ส่วนย่อยของข้อความดังกล่าวเรียกว่า tokens\n3.  **Stopword removal:** stopword คือคำที่ไม่ให้ความหมายหรือไม่ได้มีประโยชน์ในการตีความหมายของข้อความ โดยมากมักเป็นคำเชื่อมหรือคำลงท้ายที่เขียนเพื่อให้ประโยคมีความสมบูรณ์มากขึ้น เช่น a, is, are, and, the หรือในภาษาไทย เช่น ที่ ซึ่ง อัน และ\n4.  **Stemming:** เป็นเทคนิคการแปลงประเภทหนึ่ง มีวัตถุประสงค์เพื่อลดรูปคำที่พบในข้อความให้กลับไปเป็นรากศัพท์ (root words) ของคำ ๆ นั้น เช่น running, runs หรือ ran ทั้งหมดนี้มาจาก stem เดียวกันคือ run ดังนั้นการทำ stemming จะช่วยลด noise ที่เกิดขึ้นโดยจะจัดกลุ่มคำที่มีความหมายเหมือนกันดังกล่าวให้กลายเป็นคำเดียวกัน\n5.  **Lemmatization:** เป็นเทคนิคที่คล้ายกับ stemming แต่มีความซับซ้อนกว่า กล่าวคือเป็นการแปลง/ลดรูปคำที่พบโดยใช้บริบทของข้อความและการวิเคราะห์ morphological (morphological analysis) ซึ่งช่วยให้คำที่ผ่านกระบวนการดังกล่าวจะมีความหมาย และสอดคล้องกับบริบทของข้อความ\n6.  **Removing special character and punctuation:** เป็นการคัดกรองสัญลักษณ์หรืออักขระที่ไม่จำเป็นออกไปจากการวิเคราะห์ เป็นเทคนิคที่ดำเนินการเพื่อลด noise ออกจากข้อมูล\n7.  **Removing HTML tags and URLs:** ในกรณีที่ข้อมูลถูก scrapping จาก website การวิเคราะห์ข้อความดังกล่าวอาจจะต้องมีการลบ HTML tags หรือ URL รวมทั้ง syntax อื่น ๆ ที่ติดมากับข้อความด้วยก่อนดำเนินการวิเคราะห์\n8.  **Spell checking and correction:** เนื่องจากข้อความอาจมีการพิมพ์ผิดพลาดหรือคลาดเคลื่อนมาจากต้นทางของข้อมูล การแก้ไขคำผิดดังกล่าวจะช่วยให้การวิเคราะห์/นำเสนอความหมายสามารถทำได้อย่างถูกต้องและคงเส้นคงวามากขึ้น\n9.  **n-grams:** คือคำภายในข้อความที่ต่อเนื่องกันเป็นลำดับจำนวน n คำ การดึงคำแบบ n-gram ขึ้นมาจะช่วยให้ได้ข้อมูลเกี่ยวกับความหมายและบริบทของข้อความนั้นได้ดีขึ้น\n\nในเชิงปฏิบัติผู้วิเคราะห์จะต้องเป็นผู้พิจารณาว่าจะเลือกใช้เทคนิคใดบ้างมาทำ text preprocessing ในงานของตนเอง ต้องอย่าลืมว่ากระบวนการ text preprocessing เป็นกระบวนการทวนซ้ำที่อาจจะต้องใช้เวลาและลองผิดลองถูกหลายรอบจนกว่าจะได้ผลการวิเคราะห์ที่มีความสมบูรณ์ นอกจากนี้เทคนิคดังกล่าวยังอาจมีการใช้งานที่แตกต่างกันในภาษาต่าง ๆ ที่ผู้วิเคราะห์ทำงานด้วย ส่วนที่เหลือของบทความนี้จะกล่าวถึงการใช้เทคนิคข้างต้นนการทำ text preprocessing โดยเน้นการใช้งานสำหรับภาษาไทย รายละเอียดมีดังนี้\n\n# Tokenization\n\nการตัดคำ (tokenization) เป็นเทคนิคพื้นฐานที่สำคัญสำหรับการทำ text preprocessing ในการประมวลผลภาษาธรรมชาติ (NLP) กระบวนการนี้เป็นส่วนสำคัญที่สุดส่วนหนึ่งเพราะเป็นการแปลงข้อมูลข้อความที่ไม่มีโครงสร้าง (unstructure) ให้เป็นข้อมูลที่มีโครงสร้าง (structure) การแบ่งส่วนของข้อความออกเป็นส่วนย่อย เช่น คำ (words) วลี (phrases) หรือประโยค (sentences) ที่มีความหมาย ทั้งนี้การเลือกว่าควรใช้การแบ่งคำลักษณะไหนขึ้นอยู่กับบริบทของข้อความ และความละเอียดของผลการวิเคราะห์ที่ต้องการ tokenization อาจจำแนกได้เป็น 3 ประเภท ได้แก่\n\n1.  Word tokenization\n2.  Sentence tokenization\n3.  Subword tokenization\n\n## Word tokenization\n\nwork tokenization หรือการตัดคำเป็นเทคนิคการแบ่งข้อความออกเป็นคำ ซึ่งสามารถทำได้หลายลักษณะ ทั้งการตัดคำด้วยเครื่องหมายวรรคตอนหรืออักขระที่กำหนด หรือการตัดคำด้วยอัลกอริทึม การตัดคำเป็นเทคนิคที่ใช้อย่างมากในกระบวนการวิเคราะห์ข้อความ เช่น การวิเคราะห์ sentiment หรือการจำแนกข้อความ\n\nการตัดคำภาษาไทยสามารถทำได้อย่างมีประสิทธิภาพมากขึ้นด้วย libraryPyThaiNLP ที่ถูกพัฒนาขึ้นบนภาษา Python และสามารถเรียกใช้ได้บนภาษา R ผ่าน package reticulate โดยฟังก์ชันที่ใช้สำหรับตัดคำคือ `word_tokenize()`\n\nฟังก์ชัน `word_tokenize()` มีพารามิเตอร์ที่สามารถกำหนดเพื่อปรับเปลี่ยนการทำงานได้หลายตัว พารามิเตอร์จำเป็นได้แก่ `text` เป็นข้อความนำเข้าสำหรับฟังก์ชัน ส่วนพารามิเตอร์ `engine` ใช้สำหรับกำหนดอัลกอริทึมเพื่อตัดคำภายในประโยคที่กำหนด อัลกอริทึมที่แตกต่างกันมีผลให้การตัดคำที่ได้อาจมีความแตกต่างกัน ค่าเริ่มต้นของพารามิเตอร์นี้คือ `engine = \"newmm\"` เป็นวิธีการตัดคำโดยพยายามให้คำที่ตัดมีความสอดคล้องกับคำภายในพจนานุกรมภาษาไทยมากที่สุด ส่วนวิธีการอื่น ๆ ที่สามารถกำหนดได้ เช่น `longest`, `icu` หรือ `deepcut` เป็นต้น อีกพารามิเตอร์หนึ่งที่มีประโยชน์มากคือ `keep_whitespace` โดยมีค่าเริ่มต้นเท่ากับ `True` ซึ่งหมายถึงการเก็บเว้นวรรคไว้ในผลการตัดคำ ในกรณีที่ผู้วิเคราะห์ต้องการให้นำเครื่องหมายเว้นวรรคทั้งหมดออกจากผลการตัดคำให้กำหนดพารามิเตอร์นี้เป็น `False`\n\nตัวอย่างต่อไปนี้แสดงการใช้ฟังก์ชันดังกล่าวเพื่อตัดคำในประโยคที่กำหนด\n\n```{r}\ntext <- \"ส่วนเบี่ยงเบนมาตรฐานให้วัดการกระจายสัมบูรณ์ แต่สัมประสิทธิ์การแปรผันใช้วัดการกระจายสัมพัทธ์\"\n\ntokennized_text <- pythainlp$word_tokenize(text)\ntokennized_text\n```\n\nตัวอย่างต่อไปนี้แสดงผลการตัดคำด้วยฟังก์ชัน `word_tokenize()` ที่มีการกำหนดพารามิเตอร์ต่าง ๆ\n\n```{r}\n#default\npythainlp$word_tokenize(text)\n# longest engine\npythainlp$word_tokenize(text, engine = \"longest\")\n# keep whitespace = False\npythainlp$word_tokenize(text, engine = \"longest\", \n                        keep_whitespace = F)\n# use deepcut and keep whitespace = False\npythainlp$word_tokenize(text, engine = \"deepcut\", \n                        keep_whitespace = F)\n```\n\nผลการตัดคำข้างต้นจะเห็นว่าการใช้อัลกอริทึม `newmm` และ `deepcut` มีแนวโน้มที่จะได้คำที่มีความหมายถูกต้องมากที่สุด อย่างไรก็ตามจะเห็นว่ามีคำศัพท์เฉพาะที่มีการตัดคำที่ยังไม่ถูกต้อง ได้แก่ ส่วนเบี่ยงเบนมาตรฐาน การกระจายสัมบูรณ์ สัมประสิทธิ์การแปรผัน และการกระจายสัมพัทธ์ ในกรณีนี้ผู้วิเคราะห์สามารถเพิ่มคำศัพท์เฉพาะที่ใช้สำหรับประกอบการตัดคำได้ดังนี้\n\n```{r}\ntrie <- pythainlp$tokenize$Trie\n## custom word\ncustom_word <- c(\"ส่วนเบี่ยงเบนมาตรฐาน\",\n                 \"การกระจายสัมบูรณ์\",\n                 \"สัมประสิทธิ์การแปรผัน\",\n                 \"การกระจายสัมพัทธ์\")\n## load thai dictionary from pythainlp\ndict_default <- pythainlp$corpus$thai_words()\n\nextended_dict <- trie(pythainlp$corpus$thai_words())\nfor (word in custom_word) {\n  extended_dict$add(word)\n}\n\n```\n\n```{r}\npythainlp$word_tokenize(text, custom_dict = extended_dict)\n```\n\nจากตัวอย่างข้างต้นจะเห็นว่า การกำหนด dictionary เพิ่มเติมทำให้ผู้วิเคราะห์สามารถตัดคำ/ดึงคำจากข้อความที่มีความหมายสอดคล้องกับบริบทของการวิเคราะห์ได้มากขึ้น\n\n## Sentences tokenization\n\nเป็นเทคนิคการแบ่งข้อความออกเป็นประโยคหรือวลีย่อย โดยอาจใช้เว้นวรรคหรือเครื่องหมายจบประโยคต่าง ๆ เป็นตัวแบ่ง การทำ tokenization ลักษณะนี้เหมาะสำหรับงานวิเคราะห์ที่ต้องการสรุปความ (text summarization) ที่โครงสร้างของประโยคมีความสำคัญ\n\nการทำ sentences tokenization ด้วย PyThaiNLP สามารถทำได้ด้วยฟังก์ชัน `sent_tokenize()` ทั้งนี้ก่อนการใช้ฟังก์ชันดังกล่าวผู้วิเคราะห์จำเป็นต้องติดตั้ง library `python-crfsuite` ก่อนโดยพิมพ์คำสั่งต่อไปนี้ใน terminal ของ R Studio\n\n```{terminal eval=F}\npip install python-crfsuite\n```\n\nฟังก์ชัน `sent_tokenize()` มีพารามิเตอร์ที่สำคัญ 2 ตัว ได้แก่ `text` เป็นข้อความนำเข้าสำหรับฟังก์ชัน ส่วนพารามิเตอร์ `engine` ใช้สำหรับกำหนดอัลกอริทึมเพื่อแบ่งประโยคจากข้อความที่กำหนด โดยมี 2 อัลกอริทึมให้เลือกได้แก่ 'whitespace' กับ 'whitespace+newline' ซึ่งเป็นการแบ่งประโยคด้วยช่องว่าง และการเว้นบรรทัด\n\n```{r}\npythainlp$sent_tokenize(text, engine = \"whitespace\")\n\nmytext <- readLines(\"mytext.txt\")\ntemp<-pythainlp$sent_tokenize(text = mytext[1],engine = \"whitespace+newline\")\ntemp\n```\n\n# Stopword removal\n\nStopwords คือคำที่ไม่ให้ความหมายหรือไม่มีประโยชน์ในการตีความหมายของประโยคหรือข้อความเป้าหมาย เช่น a, and, the, และ หรือ อะ ซึ่งในชุดข้อมูลหากมีคำประเภทนี้อยู่มาก ๆ จะกลายเป็น noise ที่รบกวนการวิเคราะห์ ใน library pythainlp มีการรวบรวม stopwords สำหรับภาษาไทยเอาไว้พอสมควร ผู้วิเคราะห์สามารถนำมาใช้ได้ การเรียก stopword จาก pythinlp มาใช้สามารถทำได้ในทำนองเดียวกับการตัดคำดังนี้\n\n```{r}\nstopword <- pythainlp$corpus$thai_stopwords\nstopword\n```\n\nอย่างไรก็ตาม stopword ข้างต้นอยู่ใน format แบบ frozenset ซึ่งไม่สามารถนำมาใช้บน R ได้โดยตรง การนำ stopword ดังกล่าวมาใช้บน R สามารถเขียนคำสั่งเพิ่มเติมได้ดังนี้\n\n```{r}\n# python stopword script\nstopword_fn <-\"\ndef stopword():\n  import pythainlp\n  stopword = pythainlp.corpus.common.thai_stopwords()\n  stopword = list(stopword)\n  return stopword\n\"\npy_run_string(stopword_fn)\n```\n\nผลลัพธ์ที่ได้จากการเรียก stopword จากฟังก์ชันใน python script ด้วย `py_run_string()` จะเก็บอยู่ใน object ชื่อ `py` ซึ่งจะเก็บในชื่อเดียวกับฟังก์ชันดังนี้\n\n```{r}\npy$stopword() %>% head(20)\npythainlp$corpus$thai_negations\npythainlp$corpus$thai_syllables\n```\n\nนอกจาก stopword แล้ว pythainlp ยังมี dictionary ตัวอื่น ๆ ที่สามารถนำมาใช้ประโยชน์ได้ ได้แก่ พจนานุกรมคำปฏิเสธ `thai_negations` และพจนานุกรมพยางค์ `thai_syllables` ดังตัวอย่างต่อไปนี้\n\n```{r}\n# negations dictionary\nnegations_fn <-\"\ndef negations():\n  import pythainlp\n  stopword = pythainlp.corpus.common.thai_negations()\n  stopword = list(stopword)\n  return stopword\n\"\n# syllables dictionary\nsyllables_fn<-\"\ndef syllables():\n  import pythainlp\n  syllable = pythainlp.corpus.common.thai_syllables()\n  syllable = list(syllable)\n  return(syllable)\n\"\npy_run_string(negations_fn)\npy_run_string(syllables_fn)\npy$negations() %>% head(20)\npy$syllables() %>% head(20)\n```\n\n# วิเคราะห์คำตอบอัตนัยของผู้เรียน\n\nตัวอย่างส่วนนี้ผู้วิเคราะห์จะพัฒนาโมเดลตรวจคำตอบการบ้านของนิสิต ข้อมูลฝึกหัดได้จากผลการตอบการบ้านของนิสิตจำนวน 74 คน ในรายวิชา 2758501 ภาคปลาย ปีการศึกษา 2565 เรื่องการวิเคราะห์การถดถอย\n\n```{r message=F}\nlibrary(tidyverse)\nlibrary(readxl)\n# importing data\ndat <- read_excel(\"traindataset_hw.xlsx\")\nglimpse(dat,60)\n```\n\nคำถามที่จะนำมาทดลองพัฒนาโมเดลคือ \"สมการถดถอยที่ประมาณได้มีความเหมาะสมที่จะใช้ทำนาย/อธิบายความสัมพันธ์ที่พบในข้อมูลหรือไม่ เพราะเหตุใด\" แนวคำตอบของคำถามนี้คือ \"เหมาะสม ด้วยเหตุผล 2 ประการ ประการแรก คือไม่พบหลักฐานว่ามีการละเมิดข้อตกลงเบื้องต้นของการวิเคราะห์การถดถอย และประการที่สอง ค่าสัมประสิทธิ์การตัดสินใจของสมการถดถอยที่ประมาณได้มีค่าเท่ากับ 0.697 แสดงว่าสมการสามารถอธิบายความผันแปรใน mathach ได้คิดเป็นร้อยละ 69.7 ซึ่งอยู่ในระดับที่สูง\"\n\nการให้คะแนนมีการให้คะแนน 3 ระดับ คือ 0, 1, 2 หมายถึงตอบผิด ตอบถูกบางส่วน และตอบถูกทั้งหมด การวิเคราะห์ด้านล่างแสดงการแจกแจงความถี่ของคะแนนที่ได้ซึ่งพบว่า ส่วนใหญ่สามารถตอบถูกได้บางส่วน และมีส่วนน้อยที่สามารถตอบถูกได้ทั้งหมด\n\n```{r}\ndat$result3 %>% table()\n```\n\n## data preprocessing\n\nผู้วิเคราะห์สร้างชุดข้อมูลใหม่สำหรับพัฒนาโมเดลก่อนดังนี้\n\n```{r}\nhw3 <- dat %>% select(`รหัสนิสิต`,result3, text3)\nhead(hw3)\n```\n\nขั้นตอนต่อมาจะทำ tokenization เพื่อแบ่งคำจากข้อความที่นิสิตตอบมา\n\n```{r}\n# define tokenization function\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\")\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n## tokenized via mutate function\ntemp<-hw3 %>%\n  mutate(token = tokenize_thai(text3))\nhead(temp$token,10)\n```\n\nจะสังเกตเห็นว่าการตัดคำด้วย dictionary ของ pythainlp สามารถทำได้ดีพอสมควร แต่คำศัพท์ทางสถิติที่แปลเป็นภาษาไทยยังไม่สามารถตัดคำได้อย่างเหมาะสม ดังนั้นเพื่อให้การตัดคำทำได้อย่างมีประสิทธิภาพมากขึ้น ผู้วิเคราะห์จึงจะเพิ่มคำศัพท์ทางสถิติที่เกี่ยวข้องไว้ใน dictionary โดยเก็บคำศัพท์ดังกล่าวไว้ในไฟล์ `mytext.txt`\n\n```{r}\ntrie <- pythainlp$tokenize$Trie\n## custom word\ncustom_word <- readLines(\"mytext.txt\", skipNul = TRUE)\n## load thai dictionary from pythainlp\ndict_default <- pythainlp$corpus$thai_words()\n\nextended_dict <- trie(pythainlp$corpus$thai_words())\nfor (word in custom_word) {\n  extended_dict$add(word)\n}\n```\n\nจากนั้นลองดำเนินการตัดคำใหม่ด้วย dictionary ข้างต้น\n\n```{r}\n# define tokenization function\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\",\n                   custom_dict = extended_dict)\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n## tokenized via mutate function\ntemp<-hw3 %>%\n  mutate(token = tokenize_thai(text3))\nhead(temp$token,10)\n```\n\nสำรวจข้อมูลเบื้องต้น\n\n```{r}\nlibrary(tidytext)\ntemp<-temp %>%\n  unnest_tokens(input = token, output = word, token = \"ptb\") %>%\n  filter(!word %in% py$stopword()) %>%\n  #filter(!word %in% py$negations()) %>%\n  filter(!word %in% py$syllables())\n\ntemp %>% \n  # remove all numbers\n  mutate(word = str_replace_all(word, \"\\\\d+\", \"\")) %>%\n  # remove all english words\n  mutate(word = str_remove_all(word, \"\\\\b(?!R2\\\\b|\\\\bR\\\\s?\\\\^\\\\s?2\\\\b)[A-Za-z]+\\\\b\")) %>%\n  # correct word\n  mutate(word = str_replace(word, \"อารีสแควร์\",\"อาร์สแควร์\"))%>%\n  #mutate(word = str_replace(word, \"เหมาะสม\",\"ความเหมาะสม\")) %>%\n  #mutate(word = str_replace(word , \"ถดถอย\",\"สมการถดถอย\")) %>%\n  # remove punctuation\n  mutate(word = str_remove(word, \"^[[:punct:]]+$\")) %>%\n  mutate(word = str_remove(word,\" \")) %>%\n  # remove unnescessary words\n  filter(!word %in% c(\"มีค่า\",\"ที่จะ\",\"=\",\"นำมา\",\"^\",\"เ\",\"ใกล้เคียง\",\"ยังมี\",\"ยกกำลัง\",\"ทำการ\",\"ใชต้รวจ\",\"ขอ้\",\"ืองตน้\",\"ดังนั้น\",\"ว่าการ\",\n                      \"บางประการ\",\"ทั้งสอง\")) %>%\n  filter(!word %in% c(\".\",\"\"))%>%\n  group_by(result3,word) %>%\n  count() %>%\n  arrange(desc(n)) %>%\n  head(20) %>%\n  ggplot(aes(x = n, y= reorder(word,n)))+\n  geom_col(aes(fill = factor(result3,\n                             levels=c(0,1,2),\n                             labels=c(\"ผิด\",\"ถูกบางส่วน\",\"ถูก\"))))+\n  theme(text=element_text(family=\"ChulaCharasNew\"))+\n  labs(fill = \"ผลการตอบ\")\n```\n\n```{r}\n#install.packages(\"wordcloud2\")\nlibrary(wordcloud2)\nword_count<-temp %>% \n  # remove all numbers\n  mutate(word = str_replace_all(word, \"\\\\d+\", \"\")) %>%\n  # remove all english words\n  mutate(word = str_remove_all(word, \"\\\\b(?!R2\\\\b|\\\\bR\\\\s?\\\\^\\\\s?2\\\\b)[A-Za-z]+\\\\b\")) %>%\n  # correct word\n  mutate(word = str_replace(word, \"อารีสแควร์\",\"อาร์สแควร์\"))%>%\n  #mutate(word = str_replace(word, \"เหมาะสม\",\"ความเหมาะสม\")) %>%\n  #mutate(word = str_replace(word , \"ถดถอย\",\"สมการถดถอย\")) %>%\n  # remove punctuation\n  mutate(word = str_remove(word, \"^[[:punct:]]+$\")) %>%\n  mutate(word = str_remove(word,\" \")) %>%\n  # remove unnescessary words\n  filter(!word %in% c(\"มีค่า\",\"ที่จะ\",\"=\",\"นำมา\",\"^\",\"เ\",\"ใกล้เคียง\",\"ยังมี\",\"ยกกำลัง\",\"ทำการ\",\"ใชต้รวจ\",\"ขอ้\",\"ืองตน้\",\"ดังนั้น\",\"ว่าการ\",\n                      \"บางประการ\",\"ทั้งสอง\")) %>%\n  filter(!word %in% c(\".\",\"\"))%>%\n  group_by(result3,word) %>%\n  count() \n\ncol<-ifelse(word_count[,1]==0,\"maroon\",\n            ifelse(word_count[,1]==1,\"orange\",\"steelblue\"))\nwordcloud2(data = word_count[,-1], size = 3, fontFamily = \"ChulaCharasNew\",\n           color = col, backgroundColor = \"white\",\n           shape = \"square\")\n```\n\n</br>\n\nจะเห็นว่าคำตอบของผู้เรียนมีแนวโน้มที่จะกล่าวถึงข้อตกลงเบื้องต้นของการวิเคราะห์การถดถอยว่ามีความเหมาะสมอย่างไร\n\nผลการวิเคราะห์ข้างต้นยังทำให้ผู้วิเคราะห์ทำความสะอาดข้อมูลไปได้ระดับหนึ่ง ขั้นตอนต่อไปคือจะนำข้อมูลที่จัดระเบียบและทำความสะอาดนี้ไปพัฒนาโมเดลตรวจคำตอบต่อไป\n\n## Training Classification models\n\nการวิเคราะห์ในส่วนนี้เป็นการพัฒนาโมเดลทำนาย เพื่อตรวจคำตอบแบบข้อเขียนของนักเรียน การพัฒนาโมเดลประกอบด้วยขั้นตอนได้แก่ (1) การนำเข้าและจัดการข้อมูลก่อนการวิเคราะห์​ (2) การแบ่งส่วนชุดข้อมูล (3) การกำหนดและระบุโมเดลทำนาย (4) การปรับแต่งค่า hyperparameters ด้วยเทคนิค cross-validation (5) การประเมินประสิทธิภาพของโมเดล และ (6) การนำโมเดลไปใช้ รายละเอียดมีดังนี้\n\n### 1. การนำเข้าและจัดการข้อมูลก่อนการวิเคราะห์\n\nชุดข้อมูลที่ใช้พัฒนาโมเดลคือ `traindataset_hw.xlsx` สามารถนำเข้าได้ดังนี้\n\n```{r}\ndat <- read_excel(\"traindataset_hw.xlsx\")\nnames(dat)\n```\n\nเปลี่ยนชื่อคอลัมน์บางตัวเพื่อให้สะดวกในการเรียกใช้งาน\n\n```{r}\nnames(dat)[3]<-\"student_id\"\nnames(dat)[4]<-\"student_name\"\nnames(dat)[5]<-\"section\"\n```\n\nผู้วิเคราะห์จะดำเนินการจัดการข้อมูลทั้งชุดก่อนตามขั้นตอนที่ได้ทำในการสำรวจข้อมูลเบื้องต้น รายละเอียดมีดังนี้\n\n```{r}\n### ---- define tokenization function for Thai language\ntokenize_thai <- function(text) {\n  tokens <- lapply(text, pythainlp$word_tokenize, engine=\"newmm\",\n                   custom_dict = extended_dict)\n  tokens_list <- lapply(tokens, function(x) {\n                              paste(x, collapse = \" \")})\n  tokens_list <- unlist(tokens_list)\nreturn(tokens_list)\n}\n### ---- python stopword script\nstopword_fn <-\"\ndef stopword():\n  import pythainlp\n  stopword = pythainlp.corpus.common.thai_stopwords()\n  stopword = list(stopword)\n  return stopword\n\"\n### ---- syllables dictionary\nsyllables_fn<-\"\ndef syllables():\n  import pythainlp\n  syllable = pythainlp.corpus.common.thai_syllables()\n  syllable = list(syllable)\n  return(syllable)\n\"\n```\n\nคำสั่งต่อไปนี้ใช้จัดเตรียมข้อมูล training dataset ทั้งหมด ทั้งนี้ผู้วิเคราะห์ได้ตรวจคำตอบตามแนวคำตอบที่กำหนดไว้แล้ว ผลการตรวจอยู่ใน column `result3`\n\n```{r}\nq3_dat <- dat %>% select(student_id, result3, text3)\n\n```\n\n```{r message=F}\nlibrary(tidymodels)\nlibrary(themis)\nlibrary(textrecipes)\n\n## tokenized via mutate function\ntemp<-hw3 %>%\n  mutate(word = tokenize_thai(text3)) %>%\n  # remove all numbers\n  mutate(word = str_replace_all(word, \"\\\\d+\", \"\")) %>%\n  # remove all english words\n  mutate(word = str_remove_all(word, \"\\\\b(?!R2\\\\b|\\\\bR\\\\s?\\\\^\\\\s?2\\\\b)[A-Za-z]+\\\\b\")) %>%\n  # correct word\n  mutate(word = str_replace(word, \"อารีสแควร์\",\"อาร์สแควร์\"))%>%\n  #mutate(word = str_replace(word, \"เหมาะสม\",\"ความเหมาะสม\")) %>%\n  #mutate(word = str_replace(word , \"ถดถอย\",\"สมการถดถอย\")) %>%\n  # remove punctuation\n  mutate(word = str_remove(word, \"^[[:punct:]]+$\")) %>%\n  mutate(word = str_remove(word,\" \")) %>%\n  # remove unnescessary words\n  filter(!word %in% c(\"มีค่า\",\"ที่จะ\",\"=\",\"นำมา\",\"^\",\"เ\",\"ใกล้เคียง\",\"ยังมี\",\"ยกกำลัง\",\"ทำการ\",\"ใชต้รวจ\",\"ขอ้\",\"ืองตน้\",\"ดังนั้น\",\"ว่าการ\",\n                      \"บางประการ\",\"ทั้งสอง\")) %>%\n  filter(!word %in% c(\".\",\"\")) %>%\n    filter(!word %in% py$stopword()) %>%\n  #filter(!word %in% py$negations()) %>%\n  filter(!word %in% py$syllables()) %>%\n  mutate(result3 = factor(result3, levels=c(0,1,2),\n                          labels=c(\"ผิด\",\"ถูก\",\"ถูก\")))\n###----\nrecipe_obj <- recipe(result3~ word, data = temp) %>%\n  step_tokenize(word, token = \"ptb\") %>%\n  step_tfidf(word) %>%  \n  step_normalize(all_numeric_predictors()) %>%\n  step_adasyn(result3)\n\n### --- model specification\nlasso_spec <- multinom_reg(penalty = tune(),\n                           mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"classification\")\nknn_spec <- nearest_neighbor(neighbors = tune(),\n                             weight_func = tune())%>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\n\nwf<- workflow_set(\n  preproc = list(recipe_obj),\n  models = list(lasso_spec, knn_spec)\n)\n  \n\nfold<-vfold_cv(temp, v=10)\neval_metric <- metric_set(accuracy, sens, spec)\nlibrary(doMC)\nregisterDoMC(cores=15)\nresult <- wf %>%\n  workflow_map(\n            resamples = fold,\n            grid = 30,\n            metrics = eval_metric\n            )\nresult %>% autoplot() +\n  theme(legend.position = \"top\")+\n  scale_y_continuous(breaks=seq(0,1,0.1))\n```\n\n```{r}\nresult %>%\n  extract_workflow_set_result(id = \"recipe_nearest_neighbor\") %>%\n  collect_metrics() %>%\n  ggplot(aes(neighbors, mean, color = .metric)) +\n  geom_line(size = 1, show.legend = FALSE) +\n  geom_point()+\n  facet_wrap(~.metric) +\n  scale_x_log10()\n\nresult %>%\n  extract_workflow_set_result(id = \"recipe_nearest_neighbor\") %>%\n  collect_metrics() %>%\n  ggplot(aes(y=factor(weight_func),x= mean, fill = .metric)) +\n  geom_boxplot(size = 0.5, show.legend = FALSE) +\n  facet_wrap(~.metric)\n\n\n```\n\nlast_fit\n\n```{r}\n\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"TextPreprocessing.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"visual","urlcolor":"steelblue","linkcolor":"steelblue","theme":{"light":["pandoc","../theme.scss"]},"mainfont":"Krub","code-copy":true,"title":"Text PreProcessing"},"extensions":{"book":{"multiFile":true}}}}}