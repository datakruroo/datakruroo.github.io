{"title":"Hyperparameter Tuning using Keras Tuner","markdown":{"yaml":{"title":"Hyperparameter Tuning using Keras Tuner"},"headingText":"ชุดข้อมูลที่ใช้เป็นตัวอย่าง","containsRefs":false,"markdown":"\n\nการเรียนรู้เชิงลึก (Deep Learning) เป็นแขนงย่อยของศาสตร์ด้านการเรียนรู้ของเครื่อง (machine learning) ที่พัฒนาขึ้นโดยใช้การทำงานของเซลล์ประสาทในสมองที่เรียกว่า neuron เป็นต้นแบบ เรียกโมเดลการเรียนรู้ดังกล่าวว่า โมเดลโครงข่ายประสาทเทียม (artificial neuron network models: ANNs)\n\n![ความแตกต่างระหว่าง ML กับ DL](https://editor.analyticsvidhya.com/uploads/21745d3.png)\n\n\nชุดข้อมูลที่ใช้เป็นตัวอย่างจะใช้ชุดข้อมูล [**Sign Language Digits Dataset**](https://github.com/ardamavi/Sign-Language-Digits-Dataset)\n\n### Details of datasets:\n\n-   Image size: 100 x 100 pixels\n\n-   Color space: RGB\n\n-   Number of classes: 10 (Digits: 0-9)\n\n-   Number of participant students: 218\n\n-   Number of samples per student: 10\n\n```{r eval=F}\nsystem(\"git clone https://github.com/ardamavi/Sign-Language-Digits-Dataset.git\")\n```\n\n```{r eval=F}\ndir(\"~/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/Deeplearning/sign\")\n```\n\nการนำเข้าไฟล์ข้อมูลทั้งสองสามารถทำได้โดยใช้ library `numpy` ซึ่งสามารถทำบน R ผ่าน reticulate library ดังนี้\n\n```{r eval=F}\nlibrary(reticulate)\nnp <- import(\"numpy\")\nx_array <- np$load(\"sign/X.npy\")\ny_array <- np$load(\"sign/Y.npy\")\n```\n\nชุดข้อมูลดังกล่าวประกอบด้วยภาพภาษามือของตัวเลข 10 ตัว ตั้งแต่ 0, 1, 2, ..., 9 จำนวน 2,062 ภาพ โดยที่แต่ละภาพมีขนาด 64 x 64 pixels\n\n```{r eval=F}\ndim(x_array)\ndim(y_array)\n\n```\n\nในเบื้องต้นจะลอง train โมเดลเพื่อจำแนกภาษามือสำหรับตัวเลข 0 กับ 1 ก่อน เมื่อพิจารณาใน `y_array`\n\n```{r eval=F}\ny_array %>% head()\n```\n\nบทเรียนนี้จะกล่าวถึงการปรับแต่งค่า hyperparameters ในโมเดลโครงข่ายประสาทเทียม (neural network models) ซึ่งเป็นขั้นตอนสำคัญของการพัฒนาโมเดลการเรียนรู้เชิงลึก hyperparameters ของ neural network models มีหลายตัวได้แก่ จำนวน hidden layer จำนวน node ภายในแต่ละ layer ระดับของ learning rate และ activation function ที่เลือกใช้\n\n```{r eval=F}\nlibrary(reshape2)\n# Helper function to convert image data to a data frame for ggplot2\nprepare_image_data <- function(image_data) {\n  df <- melt(image_data)\n  colnames(df) <- c(\"x\", \"y\", \"value\")\n  return(df)\n}\n\n# Prepare image data for plotting\nimage_1 <- prepare_image_data(x_array[206,,])\nimage_2 <- prepare_image_data(x_array[4,,])\n\n# Plot images using ggplot2\np1 <- ggplot(image_1, aes(x = y, y = -x, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  coord_fixed() +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank())\n\np1\np2 <- ggplot(image_2, aes(x = y, y = -x, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"black\") +\n  coord_fixed() +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank())\n\n# Display the plots\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n```\n\n# 1. Setup\n\nก่อนการดำเนินการใด ๆ ผู้วิเคราะห์จำเป็นต้องติดตั้งและเตรียมสภาพแวดล้อมการทำงานให้พร้อมก่อน ได้แก่ การติดตั้ง **`tensorflow`**, **`keras`** และ **`keras-tuner`** ดังนี้\n\n```{r eval=F}\npip install tensorflow\npip install keras\npip install keras-tuner\n```\n\nจากนั้นติดตั้งและเรียก reticulate\n\n```{r echo=F, eval=F}\n#install.packages(\"reticulate\")\nlibrary(reticulate)\n```\n\n# 2. Import Python library\n\n```{r eval=F}\nnp <- import(\"numpy\")\ntf <- import(\"tensorflow\")\nkeras_tuner <- import(\"keras_tuner\")\nkeras <- import(\"keras\")\nlayers <- keras$layers\nRandomSearch <- import(\"kerastuner.tuners\")$RandomSearch\nHyperParameters <- import(\"kerastuner.engine.hyperparameters\")$HyperParameters\n```\n\n# 3. Load the data\n\nนำข้อมูลเข้าและจัดกระทำข้อมูลก่อนการวิเคราะห์\n\n```{r message=F, eval=F}\nlibrary(tidymodels)\ndat <- read.csv(\"criminal.csv\")\ndat <- dat %>% drop_na()\ndat <- recipe(TheifperPop ~., data = dat) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  prep(NULL) %>%\n  juice()\nsplit <- initial_split(dat)\ntrain <- training(split)\ntest <- testing(split)\n## convert training dataset to train_x and train_y matrix\ntrain_x <- train %>% select(-TheifperPop) %>% as.matrix()\ntrain_y <- train %>% select(TheifperPop) %>% as.matrix()\ntest_x <- test %>% select(-TheifperPop) %>% as.matrix()\ntest_y <- test %>% select(TheifperPop) %>% as.matrix()\n```\n\n# 4. Model Specification\n\n```{r, eval=F}\nlibrary(keras)\n# Build the model function\nbuild_model <- function(hp) {\n  model <- keras_model_sequential() %>%\n    layer_dense(units = hp$Int(\"units_1\", min_value = 32L, max_value = 512L, step = 32L), \n                activation = \"relu\", input_shape = 17L) %>%\n    layer_dense(units = hp$Int(\"units_2\", min_value = 32L, max_value = 512L, step = 32L), \n                activation = \"relu\") %>%\n    layer_dense(units = 1L)\n\n  model %>% compile(\n    optimizer = optimizer_adam(\n      learning_rate = hp$Float(\"learning_rate\", min_value = 1e-4, max_value = 1e-2, sampling = \"LOG\")\n    ),\n    loss = \"mse\",\n    metrics = \"mse\"\n  )\n\n  return(model)\n}\n\n# Configure the tuner\nRandomSearch <- keras_tuner$tuners$RandomSearch\n\ntuner <- RandomSearch(\n  build_model,\n  objective = \"val_mse\",\n  max_trials = 20L,\n  executions_per_trial = 2L,\n  directory = \"tuning_results\",\n  project_name = \"test_keras_tuner\"\n)\n\n# Prepare the data\n# Replace the following lines with your own dataset\n#train_x <- matrix(runif(1107 * 17), nrow = 1107) %>% class()\n#train_y <- matrix(runif(1107), nrow = 1107)\n#test_x <- matrix(runif(369 * 17), nrow = 369)\n#test_y <- matrix(runif(369), nrow = 369)\n\n# Set early stopping\nearly_stopping <- keras$callbacks$EarlyStopping(monitor = \"val_loss\", patience = 3L)\n\n# Search for the best hyperparameters\nresult<-tuner$search(\n  train_x,\n  train_y,\n  validation_data = list(test_x, test_y),\n  epochs = 10L,\n  batch_size = 256L,\n  callbacks = list(early_stopping)\n)\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"DL02_kerastuner.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","editor":"visual","urlcolor":"steelblue","linkcolor":"steelblue","theme":{"light":["pandoc","../../theme.scss"]},"mainfont":"Krub","code-copy":true,"title":"Hyperparameter Tuning using Keras Tuner"},"extensions":{"book":{"multiFile":true}}}}}