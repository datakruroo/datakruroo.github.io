{"title":"Deep Learning 101","markdown":{"yaml":{"title":"Deep Learning 101","format":{"revealjs":{"theme":"slide.scss","scrollable":true}}},"headingText":"What's Deep Learning","headingAttr":{"id":"","classes":["small"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\nพิจารณา neural network ต่อไปนี้\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.29.19.png)\n\n## Deep Learning\n\n![](images/Screenshot%202566-04-28%20at%2021.22.05.png)\n\n## Outline {.small}\n\n-   Neuron\n\n-   Activation function\n\n-   How do neural networks work?\n\n-   How do neural networks learn?\n\n-   Gradient descent\n\n-   Stochastic gradient descent\n\n-   backpropagation\n\n# The Neuron\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.32.24.png)\n\n# Activation Functions {.small}\n\nactivation function เป็นฟังก์ชันทางคณิตศาสตร์ ที่ผู้วิเคราะห์ใช้สำหรับแปลงค่าผลรวมเชิงเส้นของข้อมูลนำเข้า ให้มีพิสัยอยู่ในช่วงที่สมเหตุสมผลหรือสอดคล้องกับตัวแปรตามที่ต้องการทำนาย\n\n![](images/Screenshot%202566-04-28%20at%2022.04.23.png)\n\n## Linear Activation Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the identity activation function\nidentity_activation <- function(x) {\n  x\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the identity_activation function\ny_values <- sapply(x_values, identity_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the identity activation function\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Identity Activation Function\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"y =\", \"x\"))\n\np <- p + annotate(\"text\", x = 0, y = -5,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n```\n\n## Step Function\n\n```{r echo=F}\nlibrary(ggplot2)\n\n# Define the threshold activation function\nthreshold_activation <- function(x) {\n  ifelse(x >= 0, 1, 0)\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the threshold_activation function\ny_values <- sapply(x_values, threshold_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the threshold activation function\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Threshold Activation Function\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\np <- p + annotate(\"text\", x = -8, y = 0.5,\n                  label = \"y = ifelse(Wx+b >= 0, 1, 0)\",\n                  size = 5, hjust = 0)\n\n# Display the plot\nprint(p)\n\n\n```\n\n## Sigmoid Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the sigmoid activation function\nsigmoid_activation <- function(x) {\n  1 / (1 + exp(-x))\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the sigmoid_activation function\ny_values <- sapply(x_values, sigmoid_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the sigmoid activation function\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Sigmoid Activation Function\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"Wx+b =\", frac(1, (1 + e^(-x)))))\n\np <- p + annotate(\"text\", x = 1, y = 0.6,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n\n```\n\n## Rectified Linear Activation Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the rectified activation function (ReLU)\nrelu_activation <- function(x) {\n  ifelse(x > 0, x, 0)\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the relu_activation function\ny_values <- sapply(x_values, relu_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the rectified activation function (ReLU)\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Rectified Activation Function (ReLU)\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"Wx+b =\", \"max(0, x)\"))\n\np <- p + annotate(\"text\", x = 1, y = 6,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n\n```\n\n## Hyperbolic Tangent Activation Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the hyperbolic tangent activation function (tanh)\ntanh_activation <- function(x) {\n  (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the tanh_activation function\ny_values <- sapply(x_values, tanh_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the hyperbolic tangent activation function (tanh)\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Hyperbolic Tangent Activation Function (tanh)\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"Wx+b =\", \"(e^{x} - e^{-x}) / (e^{x} + e^{-x})\"))\n\np <- p + annotate(\"text\", x = -5, y = 0.25,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n\n```\n\n## Activation Function\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.29.19.png)\n\n# How do NNs works? {.small}\n\nตัวอย่างที่ใช้ประกอบจะใช้ชุดข้อมูลเงินเดือนอาจารย์ (TeacherSalarydata.csv)\n\n```{r}\ndat <- read.csv(\"~/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/TeacherSalaryData.csv\")\nhead(dat[,-1])\n```\n\n![](images/Screenshot%202566-04-28%20at%2023.27.25.png)\n\n# How do NNs learn? {.small}\n\n-   Forward propagation\n\n-   Backward propagation\n\n![](images/Screenshot%202566-04-29%20at%2000.27.16.png)\n\n## Forward Propagation\n\n![](images/Screenshot%202566-04-29%20at%2001.00.43.png)\n\n## Backward Propagation\n\n![](images/Screenshot%202566-04-29%20at%2008.51.19.png)\n\n## Backward Propagation : (Whole Batch) Gradient Descent\n\n![](images/Screenshot%202566-04-29%20at%2012.35.21.png)\n\n## Gradient Descent\n\n-   https://en.wikipedia.org/wiki/Gradient_descent\n\n![](images/Screenshot%202566-04-29%20at%2010.44.47.png)\n\n## Gradient Descent (Regression)\n\n![](images/Screenshot%202566-04-29%20at%2010.47.49.png)\n\n## Gradient Descent\n\n![](images/Screenshot%202566-04-29%20at%2010.55.41.png)\n\n## Gradient Descent (Classification)\n\n![](images/Screenshot%202566-04-29%20at%2010.58.00.png)\n\n## Gradient Descent\n\n![https://machinelearningnotepad.wordpress.com/2018/04/15/gradient-descent/](https://cdn-images-1.medium.com/max/1600/1*vXpodxSx-nslMSpOELhovg.png)\n\n## Stochastic Gradient Descent {.small}\n\n![](images/Screenshot%202566-04-29%20at%2012.28.31.png)\n\n## Stochastic Gradient Descent {.small}\n\n![](images/Screenshot%202566-04-29%20at%2012.49.32.png)\n\n-   Stochastic gradient descent (SGD) is a more computationally efficient implementation of gradient descent, which considers **1 training example** per iteration rather than summing over the cost of all training examples.\n\n-   The stochastic (random) aspect comes from the random shuffling of the training examples before hand. By harnessing this random selection of the training example, the path to the minimum of the cost function may be noisier (less direct) but will take less time to converge.\n\nhttps://machinelearningnotepad.wordpress.com/2018/04/15/gradient-descent/\n\n## Minibatch Stochastic Gradient Descent {.small}\n\n![](images/Screenshot%202566-04-29%20at%2012.49.32.png)\n\n## Training ANN with SGD {.small}\n\n1.  กำหนดน้ำหนัก (weight) ใน NN model อย่างสุ่ม โดยให้มีค่าใกล้ 0\n2.  ป้อนข้อมูล X แถวแรกของชุดข้อมูลเข้าไปใน input layer\n3.  ทำ forward propagation เพื่อคำนวณค่าทำนาย $\\hat{y}$\n4.  คำนวณค่า error ของการทำนาย ตาม cost function ที่กำหนด\n5.  ทำ backward propagation\n6.  เรียกกระบวนการข้อ 2. - 5. ว่า **iteration** ให้ดำเนินการทำซ้ำข้อ 2. - 5. จนครบทั้งชุดข้อมูล\n7.  เมื่อดำเนินการข้อ 6. จนครบทั้งชุดข้อมูลแล้ว เรียกว่า train ครบ 1 วงรอบ (**1 epoch**)\n\n# How to Train ANNs {.small}\n\nANNs หรือ Multi-layer perceptron เป็น neural network ประเภทหนึ่งที่เรียกว่า Feedforward neural networks (FFNNs) ซึ่งเป็นโมเดลพื้นฐานที่ใช้ในการทำงานทั่วไป ภายในโมเดลประกอบด้วย input, hidden และ output layers ดังที่กล่าวมาแล้ว โมเดลประเภทนี้สามารถประยุกต์ใช้ได้กับทั้งปัญหา classification และ regression\n\n![](images/Screenshot%202566-04-28%20at%2021.22.05.png)\n\n## Keras {.small}\n\n-   ***Tensorflow was previously the most widely used Deep Learning library, however, it was tricky to figure with for newbies.*** A simple one-layer network involves a substantial amount of code. With Keras, however, the entire process of creating a Neural Network's structure, as well as training and tracking it, becomes exceedingly straightforward.\n\n-   Keras is a high-level API that works with the backends Tensorflow, Theano, and CNTK. It includes a good and user-friendly API for implementing neural network tests. It's also capable of running on both CPUs as well as GPUs.Keras comes with 10 different neural network modelling and training API modules. Let's take a look at a few of them one by one.\n\nhttps://www.analyticsvidhya.com/blog/2021/11/training-neural-network-with-keras-and-basics-of-deep-learning/\n\n## Keras {.small}\n\nkeras สามารถใช้งานได้ทั้งบน Python และ R ทั้งนี้ก่อนการใช้งานผู้วิเคราะห์จำเป็นต้องดำเนินการติดตั้ง library ที่เกี่ยวข้องดังนี้\n\nสำหรับผู้ใช้งาน Python เป็นหลักใน terminal ให้ดาวน์โหลดและติดตั้ง tensorflow และ keras ดังนี้\n\n```{terminal eval=F, echo=T}\n# in terminal\npip install --upgrade tensorflow\npip install --upgrade keras\n```\n\nปัจจุบันสามารถใช้งาน keras จาก Python ผ่าน reticulate ซึ่งทำให้มีความสะดวกมากขึ้น\n\n### Criminal CSV\n\n```{r echo=T}\nlibrary(tidyverse)\ndat <- read.csv(\"criminal.csv\")\nglimpse(dat[,-1], 60)\n```\n\n## Data Preprocessing\n\n```{r echo=T}\nlibrary(tidymodels)\ndat <- dat %>% drop_na()\ndat_preproc <- recipe(TheifperPop~., data= dat) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_select(-X) %>%\n  prep(NULL) %>%\n  juice()\nglimpse(dat_preproc, 60)\n```\n\n## Splitting Dataset\n\n```{r echo=T}\n## data spliting\nsplit <- initial_split(dat_preproc, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\ntrain_x <- train %>% select(-TheifperPop) %>% as.matrix()\ntrain_y <- train %>% select(TheifperPop) %>% as.matrix()\ntest_x <- test %>% select(-TheifperPop) %>% as.matrix()\ntest_y <- test %>% select(TheifperPop) %>% as.matrix()\n#write.csv(train, file=\"train.csv\")\n#write.csv(test, file=\"test.csv\")\n```\n\n## Model specification\n\n```{r echo=T}\nlibrary(reticulate)\nkeras <- import(\"keras\")\n# create sequential model\nmodel <- keras$Sequential()\n# add layers\nmodel$add(keras$layers$Dense(units = 8, activation = 'relu', input_dim = ncol(train_x)))\nmodel$add(keras$layers$Dense(units = 1, activation = 'relu'))\n# Compile the model\nmodel$compile(loss = 'mse', optimizer = 'adam', metrics = 'mse')\nmodel$summary()\n```\n\n## Training\n\n```{r warning = F, echo=T}\nresult <- model$fit(x = train_x, y =train_y, \n                        epochs=as.integer(500),\n                    validation_split = 0.3, verbose=F)\n```\n\n## Model Evaluation\n\n```{r echo=T}\ndata.frame(epoch = 1:500, rmse_train = sqrt(result$history$mse),\n           rmse_validate = sqrt(result$history$val_mse)) %>%\n  ggplot(aes(x=epoch))+\n  geom_line(aes(y=rmse_train), col=\"steelblue\")+\n  geom_line(aes(y=rmse_validate), col=\"orange\")+\n  ylab(\"RMSE\")\n```\n\n```{r echo=T}\nmse <- result$model$evaluate(x = test_x, y = test_y)\nmse[1]\n```\n\n## Predicting the new data\n\n```{r echo=T}\npred_test <- model$predict(test_x)\nplot(test_y, pred_test)\ncor(test_y, pred_test)\n```\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"DL01.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"editor":"visual","urlcolor":"steelblue","linkcolor":"steelblue","title":"Deep Learning 101","theme":"slide.scss","scrollable":true}}}}