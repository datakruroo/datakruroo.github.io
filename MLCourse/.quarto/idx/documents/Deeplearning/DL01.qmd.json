{"title":"Deep Learning 101","markdown":{"yaml":{"title":"Deep Learning 101","format":{"revealjs":{"theme":"slide.scss","scrollable":true}}},"headingText":"What's Deep Learning","headingAttr":{"id":"","classes":["small"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\nพิจารณา neural network ต่อไปนี้\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.29.19.png)\n\n# \n\n![](images/Screenshot%202566-04-28%20at%2021.22.05.png)\n\n# Artificial Neural Networks {.small}\n\n-   Neuron\n\n-   Activation function\n\n-   How do neural networks work?\n\n-   How do neural networks learn?\n\n-   Gradient descent\n\n-   Stochastic gradient descent\n\n-   backpropagation\n\n## The Neuron\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.32.24.png)\n\n## Activation Functions {.small}\n\nactivation function เป็นฟังก์ชันทางคณิตศาสตร์ ที่ผู้วิเคราะห์ใช้สำหรับแปลงค่าผลรวมเชิงเส้นของข้อมูลนำเข้า ให้มีพิสัยอยู่ในช่วงที่สมเหตุสมผลหรือสอดคล้องกับตัวแปรตามที่ต้องการทำนาย\n\n![](images/Screenshot%202566-04-28%20at%2022.04.23.png)\n\n### Linear Activation Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the identity activation function\nidentity_activation <- function(x) {\n  x\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the identity_activation function\ny_values <- sapply(x_values, identity_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the identity activation function\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Identity Activation Function\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"y =\", \"x\"))\n\np <- p + annotate(\"text\", x = 0, y = -5,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n```\n\n### Step Function\n\n```{r echo=F}\nlibrary(ggplot2)\n\n# Define the threshold activation function\nthreshold_activation <- function(x) {\n  ifelse(x >= 0, 1, 0)\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the threshold_activation function\ny_values <- sapply(x_values, threshold_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the threshold activation function\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Threshold Activation Function\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\np <- p + annotate(\"text\", x = -8, y = 0.5,\n                  label = \"y = ifelse(Wx+b >= 0, 1, 0)\",\n                  size = 5, hjust = 0)\n\n# Display the plot\nprint(p)\n\n\n```\n\n### Sigmoid Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the sigmoid activation function\nsigmoid_activation <- function(x) {\n  1 / (1 + exp(-x))\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the sigmoid_activation function\ny_values <- sapply(x_values, sigmoid_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the sigmoid activation function\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Sigmoid Activation Function\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"Wx+b =\", frac(1, (1 + e^(-x)))))\n\np <- p + annotate(\"text\", x = 1, y = 0.6,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n\n```\n\n### Rectified Linear Activation Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the rectified activation function (ReLU)\nrelu_activation <- function(x) {\n  ifelse(x > 0, x, 0)\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the relu_activation function\ny_values <- sapply(x_values, relu_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the rectified activation function (ReLU)\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Rectified Activation Function (ReLU)\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"Wx+b =\", \"max(0, x)\"))\n\np <- p + annotate(\"text\", x = 1, y = 6,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n\n```\n\n### Hyperbolic Tangent Activation Function\n\n```{r}\nlibrary(ggplot2)\n\n# Define the hyperbolic tangent activation function (tanh)\ntanh_activation <- function(x) {\n  (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n}\n\n# Generate a sequence of x values\nx_values <- seq(-10, 10, by = 0.1)\n\n# Calculate the corresponding y values using the tanh_activation function\ny_values <- sapply(x_values, tanh_activation)\n\n# Create a data frame with the x and y values\ndata <- data.frame(x = x_values, y = y_values)\n\n# Plot the hyperbolic tangent activation function (tanh)\np <- ggplot(data, aes(x = x, y = y)) +\n  geom_line() +\n  labs(title = \"Hyperbolic Tangent Activation Function (tanh)\",\n       x = \"Wx+b\",\n       y = \"Output\") +\n  theme_minimal()\n\n# Add equation text to the plot\nequation <- expression(paste(\"Wx+b =\", \"(e^{x} - e^{-x}) / (e^{x} + e^{-x})\"))\n\np <- p + annotate(\"text\", x = -5, y = 0.25,\n                  label = equation,\n                  parse = TRUE,\n                  size = 5)\n\n# Display the plot\nprint(p)\n\n```\n\n## Activation Function\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.29.19.png)\n\n## How do NNs works? {.small}\n\nตัวอย่างที่ใช้ประกอบจะใช้ชุดข้อมูลเงินเดือนอาจารย์ (TeacherSalarydata.csv)\n\n```{r}\ndat <- read.csv(\"~/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/TeacherSalaryData.csv\")\nhead(dat[,-1])\n```\n\n## Input Layers\n\n![](images/Screenshot%202566-04-28%20at%2023.27.25.png)\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"DL01.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"editor":"visual","urlcolor":"steelblue","linkcolor":"steelblue","title":"Deep Learning 101","theme":"slide.scss","scrollable":true}}}}