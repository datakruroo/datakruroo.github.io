{
  "hash": "a9db854128736cf3344b0136891d5b4a",
  "result": {
    "markdown": "---\ntitle: \"Deep Learning 101\"\nformat: \n  revealjs:\n     theme: slide.scss\n     scrollable: true\n---\n\n\n# What's Deep Learning {.small}\n\nพิจารณา neural network ต่อไปนี้\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.29.19.png)\n\n## Deep Learning\n\n![](images/Screenshot%202566-04-28%20at%2021.22.05.png)\n\n## Outline {.small}\n\n-   Neuron\n\n-   Activation function\n\n-   How do neural networks work?\n\n-   How do neural networks learn?\n\n-   Gradient descent\n\n-   Stochastic gradient descent\n\n-   backpropagation\n\n# The Neuron\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.32.24.png)\n\n# Activation Functions {.small}\n\nactivation function เป็นฟังก์ชันทางคณิตศาสตร์ ที่ผู้วิเคราะห์ใช้สำหรับแปลงค่าผลรวมเชิงเส้นของข้อมูลนำเข้า ให้มีพิสัยอยู่ในช่วงที่สมเหตุสมผลหรือสอดคล้องกับตัวแปรตามที่ต้องการทำนาย\n\n![](images/Screenshot%202566-04-28%20at%2022.04.23.png)\n\n## Linear Activation Function\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](DL01_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n## Step Function\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](DL01_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n## Sigmoid Function\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](DL01_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Rectified Linear Activation Function\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](DL01_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Hyperbolic Tangent Activation Function\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](DL01_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Activation Function\n\n![](images/Screen%20Shot%202564-05-15%20at%2002.29.19.png)\n\n# How do NNs works? {.small}\n\nตัวอย่างที่ใช้ประกอบจะใช้ชุดข้อมูลเงินเดือนอาจารย์ (TeacherSalarydata.csv)\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n       rank discipline yrs.since.phd yrs.service  sex salary\n1      Prof          B            19          18 Male 139750\n2      Prof          B            20          16 Male 173200\n3  AsstProf          B             4           3 Male  79750\n4      Prof          B            45          39 Male 115000\n5      Prof          B            40          41 Male 141500\n6 AssocProf          B             6           6 Male  97000\n```\n:::\n:::\n\n\n![](images/Screenshot%202566-04-28%20at%2023.27.25.png)\n\n# How do NNs learn? {.small}\n\n-   Forward propagation\n\n-   Backward propagation\n\n![](images/Screenshot%202566-04-29%20at%2000.27.16.png)\n\n## Forward Propagation\n\n![](images/Screenshot%202566-04-29%20at%2001.00.43.png)\n\n## Backward Propagation\n\n![](images/Screenshot%202566-04-29%20at%2008.51.19.png)\n\n## Backward Propagation : (Whole Batch) Gradient Descent\n\n![](images/Screenshot%202566-04-29%20at%2012.35.21.png)\n\n## Gradient Descent\n\n-   https://en.wikipedia.org/wiki/Gradient_descent\n\n![](images/Screenshot%202566-04-29%20at%2010.44.47.png)\n\n## Gradient Descent (Regression)\n\n![](images/Screenshot%202566-04-29%20at%2010.47.49.png)\n\n## Gradient Descent\n\n![](images/Screenshot%202566-04-29%20at%2010.55.41.png)\n\n## Gradient Descent (Classification)\n\n![](images/Screenshot%202566-04-29%20at%2010.58.00.png)\n\n## Gradient Descent\n\n![https://machinelearningnotepad.wordpress.com/2018/04/15/gradient-descent/](https://cdn-images-1.medium.com/max/1600/1*vXpodxSx-nslMSpOELhovg.png)\n\n## Stochastic Gradient Descent {.small}\n\n![](images/Screenshot%202566-04-29%20at%2012.28.31.png)\n\n## Stochastic Gradient Descent {.small}\n\n![](images/Screenshot%202566-04-29%20at%2012.49.32.png)\n\n-   Stochastic gradient descent (SGD) is a more computationally efficient implementation of gradient descent, which considers **1 training example** per iteration rather than summing over the cost of all training examples.\n\n-   The stochastic (random) aspect comes from the random shuffling of the training examples before hand. By harnessing this random selection of the training example, the path to the minimum of the cost function may be noisier (less direct) but will take less time to converge.\n\nhttps://machinelearningnotepad.wordpress.com/2018/04/15/gradient-descent/\n\n## Minibatch Stochastic Gradient Descent {.small}\n\n![](images/Screenshot%202566-04-29%20at%2012.49.32.png)\n\n## Training ANN with SGD {.small}\n\n1.  กำหนดน้ำหนัก (weight) ใน NN model อย่างสุ่ม โดยให้มีค่าใกล้ 0\n2.  ป้อนข้อมูล X แถวแรกของชุดข้อมูลเข้าไปใน input layer\n3.  ทำ forward propagation เพื่อคำนวณค่าทำนาย $\\hat{y}$\n4.  คำนวณค่า error ของการทำนาย ตาม cost function ที่กำหนด\n5.  ทำ backward propagation\n6.  เรียกกระบวนการข้อ 2. - 5. ว่า **iteration** ให้ดำเนินการทำซ้ำข้อ 2. - 5. จนครบทั้งชุดข้อมูล\n7.  เมื่อดำเนินการข้อ 6. จนครบทั้งชุดข้อมูลแล้ว เรียกว่า train ครบ 1 วงรอบ (**1 epoch**)\n\n# How to Train ANNs {.small}\n\nANNs หรือ Multi-layer perceptron เป็น neural network ประเภทหนึ่งที่เรียกว่า Feedforward neural networks (FFNNs) ซึ่งเป็นโมเดลพื้นฐานที่ใช้ในการทำงานทั่วไป ภายในโมเดลประกอบด้วย input, hidden และ output layers ดังที่กล่าวมาแล้ว โมเดลประเภทนี้สามารถประยุกต์ใช้ได้กับทั้งปัญหา classification และ regression\n\n![](images/Screenshot%202566-04-28%20at%2021.22.05.png)\n\n## Keras {.small}\n\n-   ***Tensorflow was previously the most widely used Deep Learning library, however, it was tricky to figure with for newbies.*** A simple one-layer network involves a substantial amount of code. With Keras, however, the entire process of creating a Neural Network's structure, as well as training and tracking it, becomes exceedingly straightforward.\n\n-   Keras is a high-level API that works with the backends Tensorflow, Theano, and CNTK. It includes a good and user-friendly API for implementing neural network tests. It's also capable of running on both CPUs as well as GPUs.Keras comes with 10 different neural network modelling and training API modules. Let's take a look at a few of them one by one.\n\nhttps://www.analyticsvidhya.com/blog/2021/11/training-neural-network-with-keras-and-basics-of-deep-learning/\n\n## Keras {.small}\n\nkeras สามารถใช้งานได้ทั้งบน Python และ R ทั้งนี้ก่อนการใช้งานผู้วิเคราะห์จำเป็นต้องดำเนินการติดตั้ง library ที่เกี่ยวข้องดังนี้\n\nสำหรับผู้ใช้งาน Python เป็นหลักใน terminal ให้ดาวน์โหลดและติดตั้ง tensorflow และ keras ดังนี้\n\n\n::: {.cell}\n\n```{.terminal .cell-code}\n# in terminal\npip install --upgrade tensorflow\npip install --upgrade keras\n```\n:::\n\n\nปัจจุบันสามารถใช้งาน keras จาก Python ผ่าน reticulate ซึ่งทำให้มีความสะดวกมากขึ้น\n\n### Criminal CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndat <- read.csv(\"criminal.csv\")\nglimpse(dat[,-1], 60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 2,212\nColumns: 17\n$ householdSize          <dbl> NA, 2.82, 2.43, 2.40, 2.76,…\n$ pct.lao                <dbl> 1.37, 0.80, 0.74, 1.70, 0.5…\n$ pct.thai               <dbl> 91.78, 95.57, 94.33, 97.35,…\n$ pct.myanmar            <dbl> 6.50, 3.44, 3.43, 0.50, 1.1…\n$ pct.cambodia           <dbl> 1.88, 0.85, 2.35, 0.70, 0.5…\n$ numPop.urban           <int> 11980, 23123, 29344, 0, 0, …\n$ medIncome.household    <int> 75122, 47917, 35669, 20580,…\n$ pctWPubAsst            <dbl> 1.03, 2.75, 2.94, 11.71, 11…\n$ pctPop.poor            <dbl> 1.96, 3.98, 4.75, 17.23, 29…\n$ pchLessM3              <dbl> 5.81, 5.61, 2.80, 11.05, 12…\n$ PctUnemployed          <dbl> 2.70, 2.43, 4.01, 9.86, 9.0…\n$ NumIlllegal.labor      <int> 1277, 1920, 1468, 339, 196,…\n$ HousVacant             <int> 64, 240, 544, 669, 333, 511…\n$ Num.HomelessInShelters <int> 11, 0, 16, 0, 2, 327, 0, 21…\n$ Num.HomelessInStreet   <int> 0, 0, 0, 0, 0, 4, 0, 0, 15,…\n$ PopDens                <dbl> 1845.9, 2186.7, 2780.9, 321…\n$ TheifperPop            <dbl> 114.85, 242.37, 758.14, 130…\n```\n:::\n:::\n\n\n## Data Preprocessing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ndat <- dat %>% drop_na()\ndat_preproc <- recipe(TheifperPop~., data= dat) %>%\n  step_normalize(all_numeric_predictors()) %>%\n  step_select(-X) %>%\n  prep(NULL) %>%\n  juice()\nglimpse(dat_preproc, 60)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,476\nColumns: 17\n$ householdSize          <dbl> 0.43882185, -0.83914743, -0…\n$ pct.lao                <dbl> -0.55228025, -0.55727423, -…\n$ pct.thai               <dbl> 0.606938897, 0.515639240, 0…\n$ pct.myanmar            <dbl> 0.32422511, 0.32134682, -0.…\n$ pct.cambodia           <dbl> -0.4595089, -0.3148988, -0.…\n$ numPop.urban           <dbl> -0.090254530, 0.004414026, …\n$ medIncome.household    <dbl> 0.9568309, 0.0393216, -1.09…\n$ pctWPubAsst            <dbl> -0.8094356, -0.7606976, 1.4…\n$ pctPop.poor            <dbl> -0.7965221, -0.6979778, 0.8…\n$ pchLessM3              <dbl> -0.45100194, -0.96290986, 0…\n$ PctUnemployed          <dbl> -1.2901782, -0.6322239, 1.8…\n$ NumIlllegal.labor      <dbl> -0.076327167, -0.119778750,…\n$ HousVacant             <dbl> -0.24422840, -0.15522650, -…\n$ Num.HomelessInShelters <dbl> -0.28719746, -0.09787561, -…\n$ Num.HomelessInStreet   <dbl> -0.14990463, -0.14990463, -…\n$ PopDens                <dbl> -0.11341205, 0.12376845, 0.…\n$ TheifperPop            <dbl> 242.37, 758.14, 1301.78, 72…\n```\n:::\n:::\n\n\n## Splitting Dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## data spliting\nsplit <- initial_split(dat_preproc, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\ntrain_x <- train %>% select(-TheifperPop) %>% as.matrix()\ntrain_y <- train %>% select(TheifperPop) %>% as.matrix()\ntest_x <- test %>% select(-TheifperPop) %>% as.matrix()\ntest_y <- test %>% select(TheifperPop) %>% as.matrix()\n#write.csv(train, file=\"train.csv\")\n#write.csv(test, file=\"test.csv\")\n```\n:::\n\n\n## Model specification\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nkeras <- import(\"keras\")\n# create sequential model\nmodel <- keras$Sequential()\n# add layers\nmodel$add(keras$layers$Dense(units = 8, activation = 'relu', input_dim = ncol(train_x)))\nmodel$add(keras$layers$Dense(units = 1, activation = 'relu'))\n# Compile the model\nmodel$compile(loss = 'mse', optimizer = 'adam', metrics = 'mse')\nmodel$summary()\n```\n:::\n\n\n## Training\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- model$fit(x = train_x, y =train_y, \n                        epochs=as.integer(500),\n                    validation_split = 0.3, verbose=F)\n```\n:::\n\n\n## Model Evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(epoch = 1:500, rmse_train = sqrt(result$history$mse),\n           rmse_validate = sqrt(result$history$val_mse)) %>%\n  ggplot(aes(x=epoch))+\n  geom_line(aes(y=rmse_train), col=\"steelblue\")+\n  geom_line(aes(y=rmse_validate), col=\"orange\")+\n  ylab(\"RMSE\")\n```\n\n::: {.cell-output-display}\n![](DL01_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmse <- result$model$evaluate(x = test_x, y = test_y)\nmse[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 255819.6\n```\n:::\n:::\n\n\n## Predicting the new data\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](DL01_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                 [,1]\nTheifperPop 0.5913123\n```\n:::\n:::\n",
    "supporting": [
      "DL01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}