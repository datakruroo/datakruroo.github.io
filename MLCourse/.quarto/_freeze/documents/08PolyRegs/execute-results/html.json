{
  "hash": "830b2fe27155e9b5dc599f177517ad58",
  "result": {
    "markdown": "---\ntitle: \"Polynomial Regression and MARs\"\nauthor: \"ผศ.ดร.สิวะโชติ ศรีสุทธิยากร\"\ntoc: true\ntoc-depth: 3\ntoc-title: สารบัญ\ntheme: default\n---\n\n\nMultiple regression และ regression โมเดลที่เกี่ยวข้องในบทเรียนก่อนหน้านี้เป็นโมเดลที่อยู่ภายใต้กลุ่มของโมเดลเชิงเส้น (linear model) ที่มีวัตถุประสงค์หลักคือเพื่อเรียนรู้ความสัมพันธ์ภายในข้อมูลที่มีรูปแบบเชิงเส้นตรงเป็นหลัก อย่างไรก็ตามก็อาจมีบางโมเดลที่สามารถใช้เรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้นได้ คือ regression model with interaction อย่างไรก็ตามโมเดล regression ดังกล่าวก็ยังมีความยืดหยุ่นที่ไม่มากเมื่อเปรียบเทียบกับอัลกอริทึมอื่น ๆ ที่มีในปัจจุบัน โดยบทเรียนนี้จะกล่าวถึงอัลกอริทึม multivariate adaptive regression splines (MARs) ที่กล่าวได้ว่าเป็น linear model ประเภทหนึ่งแต่มีความสามารถสูงในการเรียนรู้ความสัมพันธ์ที่ไม่ใช่เชิงเส้น นอกจากนี้ยังสามารถใช้ได้ทั้งในปัญหาแบบ regression และ classification\n\n# Polynomial Regression\n\nMARs เป็นโมเดลที่พัฒนาขึ้นโดยมีพื้นฐานมาจาก polynomial regression หัวข้อนี้จึงจะกล่าวถึงมโนทัศน์ของ polynomial regression ก่อนเพื่อเป็นพื้นฐานในการทำความเข้าใจ MARs ในหัวข้อถัดไป\n\n## Basic concept\n\nโมเดลการถดถอยพหุนาม (polynomial regression models) เป็นโมเดลการถดถอยแบบเชิงเส้นที่ใช้วิเคราะห์หรือเรียนรู้ความสัมพันธ์ในข้อมูลแบบที่ไม่ใช่เชิงเส้นตรง โดยใช้ฟังก์ชันพหุนาม (polynomial function) เป็นส่วน systematic part ของโมเดลแทนการใช้ฟังก์ชันเชิงเส้นตรงธรรมดา ลองพิจารณาความสัมพันธ์\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\nset.seed(123)\nx<-runif(1000,0,8)\ny<-rnorm(1000, sin(x), 0.5) \ndata <- data.frame(x,y)\ndata %>% ggplot()+\n  geom_point(aes(x=x, y=y),col=\"steelblue\", alpha=0.6)+\n  theme_light()\n```\n\n::: {.cell-output-display}\n![](08PolyRegs_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nจะเห็นว่าความสัมพันธ์ข้างต้นมีลักษณะเป็นเส้นโค้ง การ fit ความสัมพันธ์ดังกล่าวด้วย regression model สามารถทำได้หลายวิธีการ วิธีการแรกคือการใช้ linear model เหมือนเดิมแต่มีการเพิ่ม term พหุนามที่มีดีกรีต่าง ๆ สมการถดถอยพหุนามมีสมการทั่วไปดังนี้\n\n$$\ny_i = \\beta_0+\\beta_1 x_i + \\beta_2 x^2_i + \\beta_3 x^3_i + ...+ \\beta_p x^p_i + \\epsilon_i\n$$\n\nคำสั่งต่อไปนี้แสดงการ fit สมการถดถอยพหุนาม degree 2,3 และ 4 กับข้อมูลข้างต้น (<https://en.wikipedia.org/wiki/Degree_of_a_polynomial>)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\nlinear.fit <- lm(y~x, data = data)\npoly2.fit <- lm(y~x + I(x^2), data = data)\npoly3.fit <- lm(y~x+I(x^2)+I(x^3), data=data)\npoly4.fit <- lm(y~x+I(x^2)+I(x^3)+I(x^4), data=data)\nsummary(poly2.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + I(x^2), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.39122 -0.44623  0.00919  0.48724  1.99155 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.403152   0.065188   21.52   <2e-16 ***\nx           -0.816456   0.037673  -21.67   <2e-16 ***\nI(x^2)       0.094354   0.004569   20.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6866 on 997 degrees of freedom\nMultiple R-squared:  0.321,\tAdjusted R-squared:  0.3197 \nF-statistic: 235.7 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(poly3.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + I(x^2) + I(x^3), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.60503 -0.37429  0.01347  0.38804  1.65828 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.414729   0.072795   5.697  1.6e-08 ***\nx            0.665338   0.078705   8.454  < 2e-16 ***\nI(x^2)      -0.369551   0.022892 -16.143  < 2e-16 ***\nI(x^3)       0.038733   0.001884  20.555  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5757 on 996 degrees of freedom\nMultiple R-squared:  0.5233,\tAdjusted R-squared:  0.5218 \nF-statistic: 364.4 on 3 and 996 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(poly4.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + I(x^2) + I(x^3) + I(x^4), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.58032 -0.35378  0.01344  0.33812  1.73464 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.3817872  0.0816978  -4.673 3.37e-06 ***\nx            2.6435685  0.1418019  18.643  < 2e-16 ***\nI(x^2)      -1.4777624  0.0719822 -20.530  < 2e-16 ***\nI(x^3)       0.2536516  0.0134917  18.801  < 2e-16 ***\nI(x^4)      -0.0134086  0.0008352 -16.055  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5133 on 995 degrees of freedom\nMultiple R-squared:  0.6213,\tAdjusted R-squared:  0.6198 \nF-statistic: 408.2 on 4 and 995 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\ndata %>% bind_cols(linear = predict(linear.fit),\n                   quadratic = predict(poly2.fit),\n                   cubic = predict(poly3.fit),\n                   quartic = predict(poly4.fit)) %>%\n  gather(linear:quartic, key = \"model\", value = \"pred\") %>%\n  mutate(model = factor(model, levels=c(\"linear\",\n                                        \"quadratic\",\n                                        \"cubic\",\n                                        \"quartic\"))) %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point(col = \"steelblue\", alpha=0.7)+\n  geom_line(aes(y=pred))+\n  facet_wrap(vars(model))\n```\n\n::: {.cell-output-display}\n![](08PolyRegs_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nปัญหาของ polynomial regression คือ multicollinearity\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nvif(poly2.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       x   I(x^2) \n15.90657 15.90657 \n```\n:::\n\n```{.r .cell-code}\nvif(poly3.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       x   I(x^2)   I(x^3) \n 98.7773 568.2299 221.9771 \n```\n:::\n:::\n\n\nการแก้ปัญหา multicollinearity สามารถทำได้หลายวิธีการ วิธีการแรกคือการ centering ตัวแปรอิสระ ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%bind_cols(\n  pred2 = data %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2),data=.) %>%\n   predict(.)\n  )%>%\n    ggplot(aes(x=x,y=y))+\n    geom_point(col=\"steelblue\",alpha=0.6)+\n    geom_line(aes(y=pred2))\n```\n\n::: {.cell-output-display}\n![](08PolyRegs_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndata %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2),data=.) %>% \n  vif()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     x_c I(x_c^2) \n1.000162 1.000162 \n```\n:::\n\n```{.r .cell-code}\ndata %>%\n  mutate(x_c = x-mean(x)) %>%\n  lm(y~x_c+I(x_c^2)+I(x_c^3),data=.) %>% \n  vif()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     x_c I(x_c^2) I(x_c^3) \n6.173692 1.000595 6.175168 \n```\n:::\n:::\n\n\nอีกวิธีการหนึ่งคือการแปลงตัวแปรอิสระที่มี degree ของโมเดลให้เป็นด้วยพหุนามเชิงตั้งฉาก (orthogonal polynomial) <http://home.iitk.ac.in/~shalab/regression/Chapter12-Regression-PolynomialRegression.pdf>\n\nการสร้างเทอมพหุนามเชิงตั้งฉากใน R สามารถทำได้โดยใช้ฟังก์ชัน `poly()` ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly2.fit <- lm(y~poly(x,2), data=data)\nsummary(poly2.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ poly(x, 2), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.39122 -0.44623  0.00919  0.48724  1.99155 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.14695    0.02171   6.768 2.23e-11 ***\npoly(x, 2)1 -4.59769    0.68664  -6.696 3.57e-11 ***\npoly(x, 2)2 14.18116    0.68664  20.653  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6866 on 997 degrees of freedom\nMultiple R-squared:  0.321,\tAdjusted R-squared:  0.3197 \nF-statistic: 235.7 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\ndata %>% bind_cols(pred = predict(poly2.fit)) %>%\n  ggplot(aes(x=x, y=y))+\n  geom_point()+\n  geom_line(aes(y=pred), col=\"orange\", linewidth = 1.5)\n```\n\n::: {.cell-output-display}\n![](08PolyRegs_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Polynomial Regression in tidymodels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ broom        1.0.3     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tibble       3.1.8\n✔ infer        1.0.4     ✔ tune         1.0.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.0.4     ✔ workflowsets 1.0.0\n✔ purrr        1.0.1     ✔ yardstick    1.1.0\n✔ recipes      1.0.5     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ car::recode()    masks dplyr::recode()\n✖ purrr::some()    masks car::some()\n✖ recipes::step()  masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n```\n:::\n\n```{.r .cell-code}\ndat <- read.csv(\"https://raw.githubusercontent.com/ssiwacho/2758688_ML/main/week%201/TeacherSalaryData.csv\")\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  X      rank discipline yrs.since.phd yrs.service  sex salary\n1 1      Prof          B            19          18 Male 139750\n2 2      Prof          B            20          16 Male 173200\n3 3  AsstProf          B             4           3 Male  79750\n4 4      Prof          B            45          39 Male 115000\n5 5      Prof          B            40          41 Male 141500\n6 6 AssocProf          B             6           6 Male  97000\n```\n:::\n\n```{.r .cell-code}\nset.seed(1234)\nsplit <- initial_split(dat, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\ntrain<-train %>%\n  mutate(salary = log(salary))\n```\n:::\n\n\n### pre-processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreproc <- recipe(salary ~ ., data= train) %>%\n  step_select(-X)%>%\n  step_poly(yrs.service, yrs.since.phd, degree = tune()) %>%\n  step_dummy(rank, discipline, sex)\n```\n:::\n\n\n### model specification\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly_mod <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  set_mode(\"regression\")\n```\n:::\n\n\n### set workflow and tuning\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n## Multivariate Adaptive Regression Spline (MARs)\n\nMultivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinear relationships in the data by assessing cutpoints (*knots*) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). (XXX, 2020)\n\nฟังก์ชันทำนายของ MARs มีความแตกต่างจาก regression model ทั่วไป กล่าวคือจะมีการแบ่งส่วน domain ของ feature เป็นช่วง ๆ จุดที่ใช้แบ่งส่วนเรียกว่า knot จากนั้นทำการ fit regression model ให้กับแต่ละส่วนด้วยโมเดลที่แตกต่างกัน\n\nลองพิจารณารูป A อัลกอริทึมจะเริ่มจากการหา knot ที่เหมาะสมที่สุดจาก domain ของ feature จากนั้นประมาณค่าพารามิเตอร์ในโมเดลดังนี้\n\n![](images/image-1750132179.png){width=\"60%\"}\n\nเมื่อได้ knot แรกแล้วอัลกอริทึมจะหา knot ต่อไปดังรูป B\n\n![](images/image-31836073.png){width=\"70%\"}\n\n![](images/image-1029173891.png)\n\n### MARs using tidymodels\n",
    "supporting": [
      "08PolyRegs_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}