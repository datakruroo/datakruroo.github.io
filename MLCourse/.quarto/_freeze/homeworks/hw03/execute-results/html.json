{
  "hash": "1b782457c37ff2548180becadbf9e4d3",
  "result": {
    "markdown": "# การบ้าน 3: Digit Recognition Problem\n\n## Introduction\n\nขอให้นิสิตใช้ชุดข้อมูล [`train.csv`](datasets/train.csv.zip) และ [`test.csv`](datasets/test.csv.zip) เพื่อพัฒนาโมเดลรู้จำเพื่อจำแนกตัวเลขจากลายมือ (digit recognition) เมื่อดาวน์โหลดข้อมูลมาแล้วนิสิตจะต้องแตก zip ไฟล์ข้อมูลทั้งสองก่อนที่จะวิเคราะห์จริง\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n#unzip(zipfile = \"datasets/train.csv.zip\", files = \"train.csv\")\n#unzip(zipfile = \"datasets/test.csv.zip\", file = \"test.csv\")\n\ntrain <- read.csv(\"train.csv\")\ntest <- read.csv(\"test.csv\")\nhead(train[,1:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  label pixel0 pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8\n1     1      0      0      0      0      0      0      0      0      0\n2     0      0      0      0      0      0      0      0      0      0\n3     1      0      0      0      0      0      0      0      0      0\n4     4      0      0      0      0      0      0      0      0      0\n5     0      0      0      0      0      0      0      0      0      0\n6     0      0      0      0      0      0      0      0      0      0\n```\n:::\n:::\n\n\n## สำรวจข้อมูล\n\nลองเรียกชื่อตัวแปรทั้งหมดในชุดข้อมูล `train` ขึ้นมาดู\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(train) %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"label\"  \"pixel0\" \"pixel1\" \"pixel2\" \"pixel3\" \"pixel4\"\n```\n:::\n\n```{.r .cell-code}\nnames(train) %>% tail()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"pixel778\" \"pixel779\" \"pixel780\" \"pixel781\" \"pixel782\" \"pixel783\"\n```\n:::\n:::\n\n\nชุดข้อมูลทั้งสองชุดประกอบด้วยข้อมูลภาพของตัวเลข 0 - 9 ที่เขียนด้วยลายมือที่มีขนาด 28 x 28 pixels โดยรูปภาพแต่ละรูปถูกแปลงให้อยู่ในรูปของ feature จำนวน 784 ตัว (คอลัมน์) ซึ่งแทน pixels แต่ละจุดบนรูปภาพต้นฉบับ และข้อมูลภายในแต่ละ pixel ถูกแปลงให้เป็นค่าความเข้มของสีดำ ที่มีค่าอยู่ในช่วง 0 - 255 นอกจากนี้ภายในชุดข้อมูลทั้งสองชุดยังมีตัวแปร `label` ที่ใช้ระบุว่าข้อมูลภายในแต่ละแถวของชุดข้อมูลนั้นเป็นภาพของตัวเลขใด\n\n![](images/image-1842286070.png){width=\"262\"}\n\nลองดึงข้อมูลของรูปภาพรูปที่หนึ่ง ซึ่งอยู่ในแถวแรกของชุดข้อมูล `train` ขึ้นมาดู พบว่ามีลักษณะดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot function\nplot_digit <- function(data, index)\n{\ntitle_lab <- data[index,\"label\"]\ndata %>%\n  slice(index) %>%\n  select(starts_with(\"pixel\")) %>%\n  pivot_longer(starts_with(\"pixel\")) %>%\n  mutate(x = rep(1:28,28),\n         y = rep(1:28, each = 28)) %>%\n  ggplot(aes(x = x, y = y))+\n  geom_tile(aes(fill = value))+\n  scale_y_reverse()+\n  scale_fill_gradient(low = \"white\", high = \"black\")+\n  theme_light()+\n  theme(text = element_text(family = \"ChulaCharasNew\"),\n        panel.grid = element_blank()\n  )+\n  ggtitle(title_lab)+\n  labs(fill = \"Darkness\")\n}\n# -------\nlibrary(gridExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'gridExtra'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n```{.r .cell-code}\np1 <- plot_digit(train, 1)\np2 <- plot_digit(train,10)\np3 <- plot_digit(train,20)\np4 <- plot_digit(train,30)\ngrid.arrange(p1,p2,p3,p4, ncol=2)\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nสุดท้ายลองแจกแจงความถี่ของ `label` ว่ามีตัวเลขอะไรบ้างในชุดข้อมูล\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = train, aes(x = label)) +\n  geom_bar()+\n  scale_x_continuous(breaks=seq(0,9,1))\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Model Training\n\nปัญหาจำแนกตัวเลขจากลายมือนี้จัดอยู่ในกลุ่มปัญหาที่เรียกว่า multi-class classfication ซึ่งเกี่ยวกับการจำแนกข้อมูลที่มีจำนวนหลาย categories อัลกอริทึมที่จะใช้ในตัวอย่างนี้ประกอบด้วย\n\n-   Multinomial logistic regression with regularization\n\n-   Decision Tree\n\n-   Random Forest\n\nรายละเอียดการพัฒนาโมเดลมีดังนี้\n\n### 1. สร้าง recipe object\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# แปลงให้ outcom เป็น factor ก่อน\ntrain$label <- factor(train$label)\ntrain <- train %>% drop_na()\n# sampling ข้อมูลบางส่วนมา train model\nset.seed(123)\nsplit <- initial_split(train, prop = 0.2, strata = \"label\")\nsample_train <- training(split)\nsample_test <- testing(split)\n\ntable(sample_train$label)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  0   1   2   3   4   5   6   7   8   9 \n824 980 845 866 816 747 815 890 834 777 \n```\n:::\n\n```{.r .cell-code}\nsample_train %>%\n  ggplot(aes(x=label))+\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### 2. ทดลอง fit model (no tuning)\n\nกำหนดโมเดลด้วย parsnip package รายละเอียดสามารถศึกษาได้จาก <https://www.tidymodels.org/find/parsnip/> สำหรับ multinomial logistic regression สามารถกำหนดโมเดลได้ดังนี้\n\n#### Multinomial logistic regression\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- Sys.time()\nmultinom_reg <- multinom_reg(penalty = 0.01,\n                             mixture = 1) %>%\n  set_engine(\"glmnet\") %>%\n  set_mode(\"classification\") %>%\n  fit(label ~ . , data= sample_train)\ntime_usage <- Sys.time() - start\ntime_usage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 1.757481 mins\n```\n:::\n\n```{.r .cell-code}\n#par(mfrow = c(5,2), mar=c(5,5,1,1))\n#multinom_reg %>% extract_fit_engine() %>%plot()\n\npred_multinom_test <- predict(multinom_reg, new_data = sample_test)\n\npred_result <- sample_test %>%\n  bind_cols(pred_multinom_test) %>%\n  select(label, .pred_class)\n\nconfusion_glmnet<-pred_result %>% conf_mat(truth = label,\n                         estimate = .pred_class)\nconfusion_glmnet %>% autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nconfusion_glmnet %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.864\n 2 kap                  multiclass     0.848\n 3 sens                 macro          0.862\n 4 spec                 macro          0.985\n 5 ppv                  macro          0.863\n 6 npv                  macro          0.985\n 7 mcc                  multiclass     0.848\n 8 j_index              macro          0.846\n 9 bal_accuracy         macro          0.923\n10 detection_prevalence macro          0.1  \n11 precision            macro          0.863\n12 recall               macro          0.862\n13 f_meas               macro          0.862\n```\n:::\n:::\n\n\n#### Decision Trees\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: rpart\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rpart'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dials':\n\n    prune\n```\n:::\n\n```{.r .cell-code}\nstart <- Sys.time()\ndt_fit <- decision_tree(min_n = 900,\n                    tree_depth = 30,\n                    cost_complexity = 0.0001) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\") %>%\n  fit(label ~ ., data = sample_train)\ntime_usage <- Sys.time() - start\ntime_usage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 6.030022 secs\n```\n:::\n\n```{.r .cell-code}\ndt_fit %>% extract_fit_engine() %>% rpart.plot(type = 4)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: All boxes will be white (the box.palette argument will be ignored) because\nthe number of classes in the response 10 is greater than length(box.palette) 6.\nTo silence this warning use box.palette=0 or trace=-1.\n```\n:::\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\npred_dt_test <- predict(dt_fit, new_data = sample_test)\n\npred_result_dt <- sample_test %>%\n  bind_cols(pred_dt_test) %>%\n  select(label, .pred_class)\n\nconfusion_dt<-pred_result_dt %>% conf_mat(truth = label,\n                         estimate = .pred_class)\nconfusion_dt %>% autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n\n```{.r .cell-code}\nconfusion_dt %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.575\n 2 kap                  multiclass     0.528\n 3 sens                 macro          0.571\n 4 spec                 macro          0.953\n 5 ppv                  macro          0.572\n 6 npv                  macro          0.953\n 7 mcc                  multiclass     0.529\n 8 j_index              macro          0.524\n 9 bal_accuracy         macro          0.762\n10 detection_prevalence macro          0.1  \n11 precision            macro          0.572\n12 recall               macro          0.571\n13 f_meas               macro          0.569\n```\n:::\n:::\n\n\n#### Random Forest\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstart <- Sys.time()\nrf_fit <- rand_forest(mtry = 46,\n                      trees = 1000,\n                      min_n = 61) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\") %>%\n  fit(label ~ ., data = sample_train)\ntime_usage <- Sys.time() - start\ntime_usage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 1.199842 mins\n```\n:::\n\n```{.r .cell-code}\nrf_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~46,      x), num.trees = ~1000, min.node.size = min_rows(~61, x),      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      8394 \nNumber of independent variables:  784 \nMtry:                             46 \nTarget node size:                 61 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1836287 \n```\n:::\n\n```{.r .cell-code}\npred_rf_test <- predict(rf_fit, new_data = sample_test)\n\npred_result_rf <- sample_test %>%\n  bind_cols(pred_rf_test) %>%\n  select(label, .pred_class)\n\nconfusion_rf<-pred_result_rf %>% conf_mat(truth = label,\n                         estimate = .pred_class)\nconfusion_rf %>% autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nconfusion_rf %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.933\n 2 kap                  multiclass     0.926\n 3 sens                 macro          0.933\n 4 spec                 macro          0.993\n 5 ppv                  macro          0.933\n 6 npv                  macro          0.993\n 7 mcc                  multiclass     0.926\n 8 j_index              macro          0.925\n 9 bal_accuracy         macro          0.963\n10 detection_prevalence macro          0.1  \n11 precision            macro          0.933\n12 recall               macro          0.933\n13 f_meas               macro          0.933\n```\n:::\n:::\n\n\nจากการทดลองรันโมเดลทำนายเบื้องต้นพบว่า glmnet และ random forest เป็นโมเดลที่มีประสิทธิภาพค่อนข้างดีใกล้เคียงกัน อย่างไรก็ตาม glmnet ใช้เวลาประมวลผลค่อนข้างนานมาก ดังนั้นการ fine tune hyperparameter จะทำกับโมเดล random forest อย่างเดียว รายละเอียดอยู่ในหัวข้อถัดไป\n\n### 3. สร้าง Workflow set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create recipe object for workflow\npreproc <- recipe(label ~. , data= sample_train)\n```\n:::\n\n\nกำหนด workflow เพื่อ fine tune hyperparameter ของ random forest จากการวิเคราะห์เบื้องต้นพบว่า hyperparameter ที่ค่อนข้างมีผลต่อประสิทธิภาพการทำนายคือ `mtry` ดังนั้นจึงจะ fine tune เฉพาะ hyperparameter ตัวนี้เพียงตัวเดียว\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## model specification\nrf_model <- rand_forest(mtry = tune(),\n                      trees = 200,\n                      min_n = tune()) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nrf_workflow <- workflow() %>%\n  add_recipe(preproc) %>%\n  add_model(rf_model)\n\nrf_workflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 200\n  min_n = tune()\n\nComputational engine: ranger \n```\n:::\n:::\n\n\n### 4. Hyperparameters Tuning\n\nนำโมเดลทั้งหมดมา train และปรับแต่งค่า hyperparameter ของโมเดล ดังนี้\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create 10-folds CV \nboot <- bootstraps(sample_train, times = 25, strata = \"label\")\n#vfold_cv(sample_train, v = 5, repeats = 3, strata= \"label\")\n# create eval metric\neval_metrics <- metric_set(accuracy,roc_auc, sens, spec)\n# create grid\nparams <- parameters(mtry(range = c(5,50)),\n                     min_n(range = c(50,400)))\nmygrid <- grid_max_entropy(params, size = 15)\n\n## tuning with default grid\nlibrary(doMC)\nregisterDoMC(cores = parallel::detectCores())\nstart <- Sys.time()\nall_tuning_results <- rf_workflow %>%\n  tune_grid(resamples = boot,\n               grid = mygrid,\n               metrics = eval_metrics,\n               control = control_grid(save_pred = TRUE,\n                                      verbose = TRUE)\n               )\n# stop parallel\ntime_usage <- Sys.time() - start\ntime_usage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 5.700393 mins\n```\n:::\n:::\n\n\n\n### 5. วิเคราะห์ hyperparameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_tuning_results %>% collect_metrics(summarise = T) %>%\n  filter(.metric == \"sens\") %>%\n  arrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 × 8\n    mtry min_n .metric .estimator  mean     n  std_err .config              \n   <int> <int> <chr>   <chr>      <dbl> <int>    <dbl> <chr>                \n 1    29    60 sens    macro      0.923    25 0.000791 Preprocessor1_Model14\n 2    10    62 sens    macro      0.920    25 0.000739 Preprocessor1_Model06\n 3    49    93 sens    macro      0.916    25 0.000705 Preprocessor1_Model11\n 4    38   132 sens    macro      0.910    25 0.000816 Preprocessor1_Model07\n 5    23   162 sens    macro      0.906    25 0.000860 Preprocessor1_Model09\n 6     6   126 sens    macro      0.902    25 0.00112  Preprocessor1_Model05\n 7    32   239 sens    macro      0.896    25 0.000769 Preprocessor1_Model02\n 8    46   222 sens    macro      0.896    25 0.00110  Preprocessor1_Model10\n 9    14   233 sens    macro      0.894    25 0.000956 Preprocessor1_Model13\n10     7   195 sens    macro      0.893    25 0.00113  Preprocessor1_Model08\n11    20   306 sens    macro      0.887    25 0.00121  Preprocessor1_Model12\n12    32   366 sens    macro      0.880    25 0.00104  Preprocessor1_Model03\n13    42   367 sens    macro      0.879    25 0.000939 Preprocessor1_Model04\n14    15   393 sens    macro      0.877    25 0.00125  Preprocessor1_Model15\n15     6   328 sens    macro      0.873    25 0.00148  Preprocessor1_Model01\n```\n:::\n\n```{.r .cell-code}\nall_tuning_results %>% autoplot()\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nshow_best(all_tuning_results,3, metric = \"roc_auc\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 8\n   mtry min_n .metric .estimator  mean     n   std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>     <dbl> <chr>                \n1    29    60 roc_auc hand_till  0.995    25 0.0000811 Preprocessor1_Model14\n2    49    93 roc_auc hand_till  0.994    25 0.0000848 Preprocessor1_Model11\n3    10    62 roc_auc hand_till  0.994    25 0.0000882 Preprocessor1_Model06\n```\n:::\n\n```{.r .cell-code}\nshow_best(all_tuning_results,3, metric = \"sens\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 8\n   mtry min_n .metric .estimator  mean     n  std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>    <dbl> <chr>                \n1    29    60 sens    macro      0.923    25 0.000791 Preprocessor1_Model14\n2    10    62 sens    macro      0.920    25 0.000739 Preprocessor1_Model06\n3    49    93 sens    macro      0.916    25 0.000705 Preprocessor1_Model11\n```\n:::\n\n```{.r .cell-code}\nshow_best(all_tuning_results,3, metric = \"spec\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 8\n   mtry min_n .metric .estimator  mean     n   std_err .config              \n  <int> <int> <chr>   <chr>      <dbl> <int>     <dbl> <chr>                \n1    29    60 spec    macro      0.992    25 0.0000889 Preprocessor1_Model14\n2    10    62 spec    macro      0.991    25 0.0000831 Preprocessor1_Model06\n3    49    93 spec    macro      0.991    25 0.0000768 Preprocessor1_Model11\n```\n:::\n:::\n\n\n\n### 6. Finalized workflow\n\nนำโมเดลที่ดีที่สุดจากข้้างต้นมา fit ใหม่\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmybest_mod <- show_best(all_tuning_results,1, metric = \"sens\")\nset.seed(123)\nsplit <- initial_split(train, prop = 0.7, strata = \"label\")\nsample_train <- training(split)\nsample_test <- testing(split)\n\nrf_model2 <- rand_forest(mtry = tune(),\n                      trees = 1000,\n                      min_n = tune()) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nrf_workflow2 <- workflow() %>%\n  add_recipe(preproc) %>%\n  add_model(rf_model2)\n\nstart <- Sys.time()\nregisterDoMC(cores = parallel::detectCores())\nrf_fit_final <- rf_workflow2 %>%\n  finalize_workflow(mybest_mod) %>%\n  last_fit(split,\n           metrics = eval_metrics)\ntime_usage <- Sys.time() - start\ntime_usage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTime difference of 4.628267 mins\n```\n:::\n\n```{.r .cell-code}\nrf_fit_final %>% \n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy multiclass     0.951 Preprocessor1_Model1\n2 sens     macro          0.951 Preprocessor1_Model1\n3 spec     macro          0.995 Preprocessor1_Model1\n4 roc_auc  hand_till      0.998 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\nrf_fit_final %>%\n  collect_predictions() %>%\n  conf_mat(truth = label,\n           estimate = .pred_class) %>%\n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             multiclass     0.951\n 2 kap                  multiclass     0.946\n 3 sens                 macro          0.951\n 4 spec                 macro          0.995\n 5 ppv                  macro          0.951\n 6 npv                  macro          0.995\n 7 mcc                  multiclass     0.946\n 8 j_index              macro          0.945\n 9 bal_accuracy         macro          0.973\n10 detection_prevalence macro          0.1  \n11 precision            macro          0.951\n12 recall               macro          0.951\n13 f_meas               macro          0.951\n```\n:::\n\n```{.r .cell-code}\n## mosaic plot\nrf_fit_final %>%\n  collect_predictions() %>%\n  conf_mat(truth = label,\n           estimate = .pred_class) %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## heatmap of confusion matrix\nrf_fit_final %>%\n  collect_predictions() %>%\n  conf_mat(truth = label,\n           estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n\n\n### 7. Predict a new dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_val <- rf_fit_final %>%\n  extract_workflow() %>%\n  predict(test[1:6,])\npredict_val <- as.character(predict_val$.pred_class)\np1<-plot_digit(test,1)+\n  ggtitle(predict_val[1])\np2<-plot_digit(test,2)+\n  ggtitle(predict_val[2])\np3<-plot_digit(test,3)+\n  ggtitle(predict_val[3])\np4<-plot_digit(test,4)+\n  ggtitle(predict_val[4])\np5<-plot_digit(test,5)+\n  ggtitle(predict_val[5])\np6<-plot_digit(test,6)+\n  ggtitle(predict_val[6])\ngrid.arrange(p1,p2,p3,p4,p5,p6,ncol=3)\n```\n\n::: {.cell-output-display}\n![](hw03_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "hw03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}