recipe_obj <- recipe(~ tokens, data = temp) %>%
prep(NULL) %>%
juice()
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
head(temp)
# Your dataset
data <- tibble(
id = 1:3,
text = c("สวัสดีครับ", "ผมชื่อแชทจีพีท", "ฉันชอบภาษาไทย")
)
# Tokenize text using PyThaiNLP's word_tokenize()
data <- data %>%
mutate(tokenized_text = map_chr(text, ~paste(pythainlp$word_tokenize(.x), collapse = "|")))
head(data)
# Create a recipe and use step_tokenize() with "|" as the delimiter
recipe_obj <- recipe(~ tokenized_text, data = data) %>%
step_tokenize(tokenized_text, custom_token  = "|")
# Prepare the recipe
recipe_obj_prepped <- recipe_obj %>%
prep()
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
head(temp)
temp$token
recipe_obj <- recipe(~ tokens, data = temp) %>%
step_tokenize(token) %>%
prep(NULL) %>%
juice()
recipe_obj <- recipe(~ token, data = temp) %>%
step_tokenize(token) %>%
prep(NULL) %>%
juice()
recipe_obj
show_tokens(recipe_obj)
recipe(~ token, data = temp) %>%
step_tokenize(token) %>% show_tokens()
recipe(~ token, data = temp) %>%
step_tokenize(token) %>%
prep(NULL) %>%
juice()
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3)) %>%
mutate(token = str_replace_all(word, "\\d+", ""))
temp
temp[,3]
temp[,4]
temp[,4]%>%data.frame()
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
temp[,4]
recipe_obj <- recipe(~ token, data = temp) %>%
step_tokenize(token) %>%
step_mutate(token, str_replace_all(word, "\\d+", "")) %>%
step_mutate()
recipe_obj <- recipe(~ token, data = temp) %>%
step_tokenize(token) %>%
step_mutate(token, str_replace_all(word, "\\d+", "")) %>%  prep(NULL) %>%
juice()
recipe_obj
?step_tfidf
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
step_tokenize()
?step_tokenize
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
head(temp)
recipe_obj <- recipe(~ token, data = temp) %>%
step_tokenize(token, token = "ptb") %>%
prep(NULL) %>%
juice()
recipe_obj
recipe_obj <- recipe(~ token, data = temp) %>%
step_tokenize(token, token = "ptb") %>%
step_tfidf(token) %>%
prep(NULL) %>%
juice()
recipe_obj
recipe_obj
glimpse(recipe_obj)
?step_rm_tokenn
?step_rm_token
temp<-temp %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".",""))
temp
## tokenized via mutate function
temp<-hw3 %>%
mutate(token = tokenize_thai(text3))
glimpse(temp)
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3))
temp<-temp %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".",""))
head(temp)
glimpse(temp)
temp$word %>% data.frame()
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
temp %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".",""))
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".",""))
recipe_obj <- recipe(~ token, data = temp) %>%
step_tokenize(token, token = "ptb") %>%
step_tfidf(token) %>%
prep(NULL) %>%
juice()
head(temp)
recipe_obj <- recipe(~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
prep(NULL) %>%
juice()
glimpse(recipe_obj)
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".","")) %>%
filter(!word %in% py$stopword()) %>%
#filter(!word %in% py$negations()) %>%
filter(!word %in% py$syllables())
recipe_obj <- recipe(~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
prep(NULL) %>%
juice()
glimpse(recipe_obj)
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm",
custom_dict = extended_dict)
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".","")) %>%
filter(!word %in% py$stopword()) %>%
#filter(!word %in% py$negations()) %>%
filter(!word %in% py$syllables())
glimpse(temp)
library(themis)
head*temp
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".","")) %>%
filter(!word %in% py$stopword()) %>%
#filter(!word %in% py$negations()) %>%
filter(!word %in% py$syllables()) %>%
mutate(result3 = factor(result3, levels=c(0,1,2),
labels=c("ผิด","ถูกบางส่วน","ถูก")))
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_smote()
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_smote() %>%
prep(NULL) %>%
juice()
table(recipe_obj$result3)
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_smote(result3) %>%
prep(NULL) %>%
juice()
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_adasyn(result3) %>%
prep(NULL) %>%
juice()
## tokenized via mutate function
temp<-hw3 %>%
mutate(word = tokenize_thai(text3)) %>%
# remove all numbers
mutate(word = str_replace_all(word, "\\d+", "")) %>%
# remove all english words
mutate(word = str_remove_all(word, "\\b(?!R2\\b|\\bR\\s?\\^\\s?2\\b)[A-Za-z]+\\b")) %>%
# correct word
mutate(word = str_replace(word, "อารีสแควร์","อาร์สแควร์"))%>%
#mutate(word = str_replace(word, "เหมาะสม","ความเหมาะสม")) %>%
#mutate(word = str_replace(word , "ถดถอย","สมการถดถอย")) %>%
# remove punctuation
mutate(word = str_remove(word, "^[[:punct:]]+$")) %>%
mutate(word = str_remove(word," ")) %>%
# remove unnescessary words
filter(!word %in% c("มีค่า","ที่จะ","=","นำมา","^","เ","ใกล้เคียง","ยังมี","ยกกำลัง","ทำการ","ใชต้รวจ","ขอ้","ืองตน้","ดังนั้น","ว่าการ",
"บางประการ","ทั้งสอง")) %>%
filter(!word %in% c(".","")) %>%
filter(!word %in% py$stopword()) %>%
#filter(!word %in% py$negations()) %>%
filter(!word %in% py$syllables()) %>%
mutate(result3 = factor(result3, levels=c(0,1,2),
labels=c("ผิด","ถูก","ถูก")))
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_adasyn(result3) %>%
prep(NULL) %>%
juice()
table(recipe_obj$result3)
### --- model specification
lasso_spec <- multinom_reg(penalty = tune(),
mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
wf<-workflow() %>%
add_recipe(recipe_obj) %>%
add_model(lasso_spec)
###----
recipe_obj <- recipe(result3~ word, data = temp) %>%
step_tokenize(word, token = "ptb") %>%
step_tfidf(word) %>%
step_adasyn(result3)
### --- model specification
lasso_spec <- multinom_reg(penalty = tune(),
mixture = 1) %>%
set_engine("glmnet") %>%
set_mode("classification")
wf<-workflow() %>%
add_recipe(recipe_obj) %>%
add_model(lasso_spec)
reticulate::repl_python()
setwd("~/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/Deeplearning")
np <- import("numpy")
library(retuculate)
library(reticulate)
np <- import("numpy")
tf <- import("tensorflow")
keras <- import("keras")
tuner <- import("keras_tuner")
ai <- import("openai")
os <- import("os")
## importing data
ai$api_key <- os$getenv("sk-...QVht")
## importing data
response <- ai$Image$create(
prompt = "a white siamese cat",
n=1,
size = "100x100"
)
ai$api_key <- os$getenv("sk-...QVht")
## importing data
response <- ai$Image$create(
prompt = "a white siamese cat",
n=1,
size = "100x100"
)
ai$api_key <- os$getenv("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")
## importing data
response <- ai$Image$create(
prompt = "a white siamese cat",
n=1,
size = "100x100"
)
ai$organization <- "org-MqyowqYNV9fRFkI16yN0rs7F"
ai$api_key <- os$getenv("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")
ai$Model$list()
ai$organization <- "org-MqyowqYNV9fRFkI16yN0rs7F"
ai$api_key <- os$getenv("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")
ai$Model$list()
openai <- import("openai")
openai$version
print(openai$version)
key <- "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"
key <- "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"
openai$api_key<-key
## importing data
response<-openai$Completion$create(
model = "text-davinci-002",
prompt = "This is a story about using keras-tuner to tune hyperparameter in NN models via reticulate in Rstudio",
temperature = 0.7,
max_tokens = 300,
)
## importing data
response<-openai$Completion$create(
model = "text-davinci-002",
prompt = "This is a story about using keras-tuner to tune hyperparameter in NN models via reticulate in Rstudio",
temperature = 0.7,
max_tokens = 300
)
## importing data
response<-openai$Completion$create(
model = "text-davinci-002",
prompt = "This is a story about using keras-tuner to tune hyperparameter in NN models via reticulate in Rstudio",
temperature = 0.7,
max_tokens = 100
)
np <- import("numpy")
tf <- import("tensorflow")
keras <- tf$keras
layers <- keras$layers
RandomSearch <- import("kerastuner.tuners")$RandomSearch
HyperParameters <- import("kerastuner.engine.hyperparameters")$HyperParameters
fashion_mnist <- keras$datasets$fashion_mnist
data <- fashion_mnist$load_data()
py_run_string("import certifi; import ssl; ssl.create_default_context = ssl.create_default_context(cafile=certifi.where())")
fashion_mnist <- keras$datasets$fashion_mnist
data <- fashion_mnist$load_data()
reticulate::py_run_string("import certifi; import ssl; ssl.create_default_context = ssl.create_default_context(cafile=certifi.where())")
reticulate::py_run_string("import certifi; import ssl; ssl.create_default_context = lambda purpose, *, cafile=None, capath=None, cadata=None: ssl.create_default_context(purpose, cafile=certifi.where(), capath=capath, cadata=cadata)")
fashion_mnist <- keras$datasets$fashion_mnist
data <- fashion_mnist$load_data()
requests <- import("requests")
download_and_save <- function(url, filepath) {
response <- requests$get(url, verify = FALSE)
if (response$status_code == 200L) {
writeBin(response$content, filepath)
} else {
stop(paste("Failed to download from", url))
}
}
# Download and save the dataset files
urls <- c(
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz"
)
filepaths <- file.path(tempdir(), basename(urls))
mapply(download_and_save, urls, filepaths)
download_and_save <- function(url, filepath) {
response <- requests$get(url, verify = FALSE)
if (response$status_code == 200L) {
writeBin(as.raw(response$content), filepath)
} else {
stop(paste("Failed to download from", url))
}
}
# Download and save the dataset files
urls <- c(
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz"
)
filepaths <- file.path(tempdir(), basename(urls))
mapply(download_and_save, urls, filepaths)
library(httr)
download_and_save <- function(url, filepath) {
response <- GET(url, config(ssl_verifypeer = 0L))
if (response$status_code == 200L) {
writeBin(content(response, "raw"), filepath)
} else {
stop(paste("Failed to download from", url))
}
}
# Download and save the dataset files
urls <- c(
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz",
"https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz"
)
filepaths <- file.path(tempdir(), basename(urls))
mapply(download_and_save, urls, filepaths)
# Load the dataset using the saved files
data <- fashion_mnist$load_data(path = filepaths[1])
# Load the dataset using the saved files
data <- fashion_mnist$load_data(path = filepaths[1])
numpy <- import("numpy")
gzip <- import("gzip")
load_data <- function(labels_file, images_file) {
with(gzip$open(labels_file, "rb") as f,
load_data <- function(labels_file, images_file) {
with(gzip$open(labels_file, "rb") as f,
fashion_mnist <- keras$datasets$fashion_mnist
data <- fashion_mnist$load_data()
fashion_mnist$load_data
data <- fashion_mnist$load_data()
data <- np$load("mnist.npz")
x_train <- data[[1]][[1]]
y_train <- data[[1]][[2]]
class(data)
data
x_train<-data$get("x_train")
class(x_train)
x_train[1]
py_to_r(x_train)
x_test<-data$get("x_test")
x_train<-data$get("x_train")
x_test<-data$get("x_test")
y_train<-data$get("y_train")
y_test<-data$get("y_test")
# Scale the pixel values to the range [0, 1]
x_train <- x_train$astype("float32") / 255.0
# Scale the pixel values to the range [0, 1]
x_train <- x_train / 255.0
x_test <- x_test / 255.0
reticulate::repl_python()
library(reticulate)
reticulate::repl_python()
