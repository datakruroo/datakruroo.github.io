header=F, col.names = c("text","sentiment"),
sep = "\t")
dat$id <- 1:dim(dat)[1]
head(dat)
glimpse(dat, width=60)
neg<- read.csv("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/neg.txt", header = F, col.names = "word")
pos <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/pos.txt", header = F, col.names = "word")
neg$sentiment <- "neg"
pos$sentiment <- "pos"
lexicon <- neg %>% bind_rows(pos)
head(lexicon)
dim(lexicon)
lexicon <- distinct(lexicon)
dim(lexicon)
library(reticulate)
# Import pythainlp library
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
tokenized_dat
source_python("stopword.py")
length(stopword())
head(stopword(),10)
tail(stopword(),10)
tokenized_dat<-tokenized_dat %>%
filter(!word %in% stopword()) %>%
filter(!word %in% c("ค่ะ","ครับ"))
tokenized_dat %>% dim()
sentiment_dat <- tokenized_dat %>%
inner_join(lexicon, by="word")
sentiment_dat
library(ggplot2)
sentiment_dat %>%
mutate(sentiment_score = ifelse(sentiment.y=="pos",1,-1)) %>%
group_by(id) %>%
summarise(sentiment_score = sum(sentiment_score)) %>%
ggplot(aes(x=sentiment_score))+
geom_histogram(bins=5, col="white")
sentiment_dat %>%
mutate(sentiment_score = ifelse(sentiment.y=="pos",1,-1)) %>%
#  group_by(id, word) %>%
count(word, sentiment.y, sort = TRUE) %>%
ggplot(aes(x=n, y=word, fill=sentiment.y))+
geom_col()+
facet_wrap(~sentiment.y)+
theme(text=element_text(family="ChulaCharasNew"))
library(tidyverse)
library(tidymodels)
library(textrecipes)
glimpse(dat, width=60)
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
tokenized_dat
# remove stopwords
source_python("stopword.py")
length(stopword())
head(stopword(),10)
tail(stopword(),10)
tokenized_dat<-tokenized_dat %>%
filter(!word %in% stopword()) %>%
filter(!word %in% c("ค่ะ","ครับ","เ","เตก","อะ","10","ป","ก๊อ",
"ๆๆๆๆ","ๆ","ๆๆๆ","ๆๆ","ใด","ค","ร",
"ส","ง","วจะ"))
tokenized_dat
set.seed(123)
split<-initial_split(tokenized_dat, strata = sentiment)
train<-training(split)
test<-testing(split)
## create preprocessing recipe
train_rec <- recipe(sentiment~text, data=train) %>%
step_tokenize(text) %>%
step_stopwords(text) %>%
step_tokenfilter(text) %>%
step_tfidf(text) %>%
step_normalize(all_numeric_predictors())
## model specification 1
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
## model specification 2
rf_spec <- rand_forest(mtry = tune(),
trees=300,
min_n=tune()) %>%
set_engine("ranger",importance = "permutation") %>%
set_mode("classification")
## create workflowset
wf_set <- workflow_set(
preproc = list(train_rec),
models = list(lasso_spec, rf_spec)
)
## training
doParallel::registerDoParallel()
set.seed(123)
result <- workflow_map(
wf_set,
resamples = folds,
grid = 50,
control = control_grid(save_pred = T),
metrics = metric_set(roc_auc, sens,spec)
)
autoplot(result)
result %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
arrange(desc(mean))
result %>%
extract_workflow_set_result(id = "recipe_rand_forest") %>%
collect_metrics() %>%
ggplot(aes(mtry, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
result %>%
extract_workflow_set_result(id = "recipe_rand_forest") %>%
collect_metrics() %>%
ggplot(aes(min_n, mean, color = .metric)) +
geom_line(size = 1.5, show.legend = FALSE) +
facet_wrap(~.metric) +
scale_x_log10()
## the best logistic model
best<-result %>%
extract_workflow_set_result(id = "recipe_logistic_reg") %>%
show_best(n=1, metric = "roc_auc")
best
## extract logistic regressionworkflow
logit_wf <- wf_set%>%
extract_workflow(id = "recipe_logistic_reg")
final_logit <- logit_wf %>%
finalize_workflow(best)
final_logit
## sentiment analysis
final_logit %>%
last_fit(split) %>%
extract_fit_engine() %>%
vi() %>%
group_by(Sign) %>%
top_n(10, wt = abs(Importance)) %>%
ungroup() %>%
mutate(
Importance = abs(Importance),
Variable = str_remove(Variable, "tfidf_text_"),
Variable = fct_reorder(Variable, Importance)
) %>%
ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
geom_col(show.legend = FALSE) +
facet_wrap(~Sign, scales = "free_y") +
labs(y = NULL)+
theme(text=element_text(family="ChulaCharasNew"))
## the best random forest
best<-result %>%
extract_workflow_set_result(id = "recipe_rand_forest") %>%
show_best(n=1, metric = "roc_auc")
best
## extract logistic regressionworkflow
rf_wf <- wf_set%>%
extract_workflow(id = "recipe_rand_forest")
final_rf <- rf_wf %>%
finalize_workflow(best)
final_rf
## create model using whole training dataset
rf_lastfit <-final_rf %>%
last_fit(split, metrics=metric_set(roc_auc, sens,spec))
rf_lastfit %>%
collect_metrics()
new_dat <- c("ซื้อมาแล้วใช้งานไม่ได้เลย",
"ถ้าราคาถูกกว่านี้จะดีมาก",
"แพคกล่องมาแย่มาก แต่ของไม่เสีย")
new_dat <- tibble(text = new_dat)
rf_lastfit  %>%
extract_workflow() %>%
predict(new_data= new_dat)
library(dplyr)
library(tidyr)
library(tidytext)
library(readxl)
dat <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/review_shopping.csv",
header=F, col.names = c("text","sentiment"),
sep = "\t")
dat$id <- 1:dim(dat)[1]
head(dat)
glimpse(dat, width=60)
neg<- read.csv("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/neg.txt", header = F, col.names = "word")
pos <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/pos.txt", header = F, col.names = "word")
neg$sentiment <- "neg"
pos$sentiment <- "pos"
lexicon <- neg %>% bind_rows(pos)
head(lexicon)
dim(lexicon)
lexicon <- distinct(lexicon)
dim(lexicon)
library(reticulate)
# Import pythainlp library
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
tokenized_dat
source_python("stopword.py")
length(stopword())
head(stopword(),10)
tail(stopword(),10)
tokenized_dat<-tokenized_dat %>%
filter(!word %in% stopword()) %>%
filter(!word %in% c("ค่ะ","ครับ"))
tokenized_dat %>% dim()
sentiment_dat <- tokenized_dat %>%
inner_join(lexicon, by="word")
sentiment_dat
library(ggplot2)
sentiment_dat %>%
mutate(sentiment_score = ifelse(sentiment.y=="pos",1,-1)) %>%
group_by(id) %>%
summarise(sentiment_score = sum(sentiment_score)) %>%
ggplot(aes(x=sentiment_score))+
geom_histogram(bins=5, col="white")
sentiment_dat %>%
mutate(sentiment_score = ifelse(sentiment.y=="pos",1,-1)) %>%
#  group_by(id, word) %>%
count(word, sentiment.y, sort = TRUE) %>%
ggplot(aes(x=n, y=word, fill=sentiment.y))+
geom_col()+
facet_wrap(~sentiment.y)+
theme(text=element_text(family="ChulaCharasNew"))
library(tidyverse)
library(tidymodels)
library(textrecipes)
glimpse(dat, width=60)
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
tokenized_dat
# remove stopwords
source_python("stopword.py")
length(stopword())
head(stopword(),10)
tail(stopword(),10)
tokenized_dat<-tokenized_dat %>%
filter(!word %in% stopword()) %>%
filter(!word %in% c("ค่ะ","ครับ","เ","เตก","อะ","10","ป","ก๊อ",
"ๆๆๆๆ","ๆ","ๆๆๆ","ๆๆ","ใด","ค","ร",
"ส","ง","วจะ"))
tokenized_dat
set.seed(123)
split<-initial_split(tokenized_dat, strata = sentiment)
train<-training(split)
test<-testing(split)
## create preprocessing recipe
train_rec <- recipe(sentiment~text, data=train) %>%
step_tokenize(text) %>%
step_stopwords(text) %>%
step_tokenfilter(text) %>%
step_tfidf(text) %>%
step_normalize(all_numeric_predictors())
## model specification 1
lasso_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
## model specification 2
rf_spec <- rand_forest(mtry = tune(),
trees=300,
min_n=tune()) %>%
set_engine("ranger",importance = "permutation") %>%
set_mode("classification")
## create workflowset
wf_set <- workflow_set(
preproc = list(train_rec),
models = list(lasso_spec, rf_spec)
)
## training
doParallel::registerDoParallel()
set.seed(123)
result <- workflow_map(
wf_set,
resamples = folds,
grid = 50,
control = control_grid(save_pred = T),
metrics = metric_set(roc_auc, sens,spec)
)
?vfold_cv
homework<-read_excel("documents/answer.xlsx")
homework<-read_excel("answer.xlsx")
glimpse(homework)
homework<-read_excel("answer.xlsx")
glimpse(homework, width=60)
neg<- read.csv("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/neg.txt", header = F, col.names = "word")
neg<-read.csv("https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/neg.txt",
header=F, col.names="word")
pos <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/lexicon-thai/master/%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1/pos.txt", header = F, col.names = "word")
pos <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/wisesight-sentiment/master/pos.txt", header = F, col.names = "word")
neg$sentiment <- "neg"
pos$sentiment <- "pos"
lexicon <- neg %>% bind_rows(pos)
head(lexicon)
dim(lexicon)
lexicon <- distinct(lexicon)
dim(lexicon)
library(reticulate)
# Import pythainlp library
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
glimpse(tokenized_dat, width=60)
source_python("stopword.py")
length(stopword())
head(stopword(),10)
tail(stopword(),10)
tokenized_dat<-tokenized_dat %>%
filter(!word %in% stopword()) %>%
filter(!word %in% c("ค่ะ","ครับ"))
tokenized_dat %>% dim()
sentiment_dat <- tokenized_dat %>%
inner_join(lexicon, by="word")
glimpse(sentiment_dat, width=60)
library(ggplot2)
sentiment_dat %>%
mutate(sentiment_score = ifelse(sentiment.y=="pos",1,-1)) %>%
group_by(id) %>%
summarise(sentiment_score = sum(sentiment_score)) %>%
ggplot(aes(x=sentiment_score))+
geom_histogram(bins=5, col="white")
dat <- read.csv("https://raw.githubusercontent.com/PyThaiNLP/thai-sentiment-analysis-dataset/master/review_shopping.csv",
header=F, col.names = c("text","sentiment"),
sep = "\t")
dat<-read_excel("/Users/siwachoat/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/res_feedback.xlsx")
dat$id <- 1:dim(dat)[1]
head(dat)
glimpse(dat, width=60)
library(reticulate)
# Import pythainlp library
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
glimpse(tokenized_dat, width=60)
library(reticulate)
# Import pythainlp library
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="longest")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
glimpse(tokenized_dat, width=60)
library(reticulate)
# Import pythainlp library
pythainlp <- import("pythainlp")
# Define a custom tokenization function for textrecipes
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="deepcut")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
## tokenized via mutate function
temp<-dat %>%
mutate(token = tokenize_thai(text))
head(temp)
tokenized_dat <- temp %>%
unnest_tokens(input = token, token = "words",
output = word)
glimpse(tokenized_dat, width=60)
source_python("stopword.py")
length(stopword())
head(stopword(),10)
tail(stopword(),10)
tokenized_dat<-tokenized_dat %>%
filter(!word %in% stopword()) %>%
filter(!word %in% c("ค่ะ","ครับ"))
tokenized_dat %>% dim()
sentiment_dat <- tokenized_dat %>%
inner_join(lexicon, by="word")
glimpse(sentiment_dat, width=60)
library(ggplot2)
sentiment_dat %>%
mutate(sentiment_score = ifelse(sentiment.y=="pos",1,-1)) %>%
group_by(id) %>%
summarise(sentiment_score = sum(sentiment_score)) %>%
ggplot(aes(x=sentiment_score))+
geom_histogram(bins=5, col="white")
library(ggplot2)
sentiment_dat %>%
mutate(sentiment_score = ifelse(sentiment=="pos",1,-1)) %>%
group_by(id) %>%
summarise(sentiment_score = sum(sentiment_score)) %>%
ggplot(aes(x=sentiment_score))+
geom_histogram(bins=5, col="white")
salary <- read.csv("/Users/siwachoat/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/TeacherSalaryData.csv")
head(salary)
salary <- read.csv("/Users/siwachoat/Library/CloudStorage/OneDrive-ChulalongkornUniversity/Documents/ML/MLcourse/MLCourse/documents/TeacherSalaryData.csv")
head(salary)
fit <- lm(salary ~ yrs.service, data= salary)
summary(fit)
salary <- read.csv("/Users/siwachoat/Downloads/Salary_Data (1).csv")
head(salary)
fit <- lm(salary ~ yrs.service, data= salary)
salary <- read.csv("/Users/siwachoat/Downloads/Salary_Data (1).csv")
head(salary)
fit <- lm(Salary ~ Experience, data= salary)
summary(fit)
new_dat<-c("หากครูมีประสบการณ์เพิ่มขึ้นอีก 1 ปี จะมีเงินเดือนเพิ่มขึ้นโดยเฉลี่ย 9450 บาท",
"สมการถดถอยคือ y = mx+c",
"ครูจะมีเงินเดือนเพิ่ม 9450 บาท เมื่อมีประสบการณ์เพิ่มขึ้น 1 ปี",
"ถ้าอาจารย์มีประสบการณ์เพิ่ม1 ปี จะมีเงินเดือนเพิ่ม 9449 บาท",
"ประสบการณ์ที่มากกขึ้น 1 ปี จะทำให้ครูมีเงินเดือนเพิม 9449 บาท",
"โดเรมอนโตขึ้นสัก 1 ปี ก็จะมีเงินเพิ่มขึ้น 9000 บาท",
"ครูที่ไม่มีประสบการณ์ทำงานจะมีเงินเดือน 9000 บาท",
"เลือกอีป้อมเป็นนายกกันเถอะ")
new_dat <- tibble(text = new_dat)
mod %>%
extract_workflow() %>%
predict(new_data= new_dat)
?step_tokenize
recipe_obj <- recipe(~ text, data = data.frame(text = c("สวัสดีครับ", "สวัสดีค่ะ"))) %>%
step_tokenize(text, token_func = tokenize_thai) %>%
prep()
tokenize_thai <- function(text) {
tokens <- lapply(text, pythainlp$word_tokenize, engine="newmm")
tokens_list <- lapply(tokens, function(x) {
paste(x, collapse = " ")})
tokens_list <- unlist(tokens_list)
return(tokens_list)
}
recipe_obj <- recipe(~ text, data = data.frame(text = c("สวัสดีครับ", "สวัสดีค่ะ"))) %>%
step_tokenize(text, token_func = tokenize_thai) %>%
prep()
?step_stopwords
