tree_depth = tune(),
learn_rate = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
# create workflowset
wfset <- workflow_set(
preproc = list(rec_smote),
models = list(logit_mod, rf_mod, boost_mod)
)
fold<-vfold_cv(train, v=10, repeats=2, strata = Ach_class)
boost_mod <- boost_tree(trees=200,
# min_n = tune(),
#  tree_depth = tune(),
# learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# create workflowset
wfset <- workflow_set(
preproc = list(rec_smote),
models = list(logit_mod, rf_mod, boost_mod)
)
start <- Sys.time()
result<-workflow() %>%
add_recipe(rec_smote) %>%
add_model(boost_mod) %>%
fit_resamples(fold)
paste("time usage", round(Sys.time() - start,2), "mins")
100000/60
100000/3600
50000/3600
start <- Sys.time()
result<-workflow() %>%
add_recipe(rec_smote_knn) %>%
add_model(knn_mod) %>%
fit_resamples(fold)
knn_mod <- nearest_neighbor() %>%
set_engine("kknn") %>%
set_mode("classification")
rec_smote_knn
start <- Sys.time()
result<-workflow() %>%
add_recipe(rec_smote_knn) %>%
add_model(knn_mod) %>%
fit_resamples(fold)
paste("time usage", round(Sys.time() - start,2), "secs")
1200/60
1200/3600
### random forest
rf_mod <- rand_forest(trees=200,
#mtry=tune(),
#min_n=tune()
) %>%
set_engine("ranger", importance = "permutation") %>%
set_mode("classification")
start <- Sys.time()
result<-workflow() %>%
add_recipe(rec_smote) %>%
add_model(rf_mod) %>%
fit_resamples(fold)
paste("time usage", round(Sys.time() - start,2), "secs")
# preprocessing
rec_smote <- recipe(Ach_class ~. , data= train) %>%
step_rm(clus2:clus4, Ach) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_smote(Ach_class, neighbors = 5)
# model specification
### regularized logistic regression
logit_mod <- multinom_reg(penalty=tune(),
mixture=tune()) %>%
set_engine("glmnet") %>%
set_mode("classification")
### random forest (ุ6.3 secs)
rf_mod <- rand_forest(trees=200,
mtry=tune(),
min_n=tune()
) %>%
set_engine("ranger", importance = "permutation") %>%
set_mode("classification")
### random forest (ุ6.3 secs)
rf_mod <- rand_forest(trees=200,
mtry=tune(),
min_n=tune()
) %>%
set_engine("ranger", importance = "permutation") %>%
set_mode("classification")
### KNN (4.44 sec per round)
knn_mod <- nearest_neighbor(neighbors = tune(),
weight_func = tune(),
dist_power = 2) %>%
set_engine("kknn") %>%
set_mode("classification")
### gradient boosting (10.42 secs per round)
library(xgboost)
boost_mod <- boost_tree(trees=200,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# create workflowset
wfset <- workflow_set(
preproc = list(rec_smote),
models = list(logit_mod, knn_mod)
)
fold<-vfold_cv(train, v=5, repeats=2, strata = Ach_class)
# tuning hyperparameters
eval_metrics <- metric_set(roc_auc, sens, spec)
eval_metrics <- metric_set(roc_auc, sens, spec)
start <- Sys.time()
result <- wfset %>%
workflow_map(resamples = fold,
grid = 30,
control = control_grid(save_pred = TRUE),
metrics = eval_metrics)
paste("time usage", round(Sys.time() - start,2), "mins")
result %>% autoplot()
# tuning hyperparameters
eval_metrics <- metric_set(roc_auc, sens, spec)
start <- Sys.time()
result <- wfset %>%
workflow_map(resamples = fold,
grid = 50,
control = control_grid(save_pred = TRUE),
metrics = eval_metrics)
paste("time usage", round(Sys.time() - start,2), "mins")
result %>% autoplot()
result
result %>% extract_workflow_set_result(id = "recipe_multinom_reg")
result %>% extract_workflow_set_result(id = "recipe_multinom_reg") %>%
autoplot()
result
result %>% extract_workflow_set_result(id = "recipe_nearest_neighbor") %>%
autoplot()
result %>% autoplot()
### MARS
library(earth)
install.packages("earth")
### MARS
library(earth)
?mars
mars_mod <- mars(num_terms = tune(),
prod_degree = tune()) %>%
set_engine("earth") %>%
set_mode("classification")
# create workflowset
wfset <- workflow_set(
preproc = list(rec_smote),
models = list(logit_mod, knn_mod, mars_mod)
)
# tuning hyperparameters
eval_metrics <- metric_set(roc_auc, sens, spec)
start <- Sys.time()
result <- wfset %>%
workflow_map(resamples = fold,
grid = 50,
control = control_grid(save_pred = TRUE),
metrics = eval_metrics)
paste("time usage", round(Sys.time() - start,2), "mins")
show_notes(.Last.tune.result)
mars_mod <- mars(prod_degree = 2 %>%
mars_mod <- mars(prod_degree = 2) %>%
set_engine("earth") %>%
set_mode("classification")
# create workflowset
wfset <- workflow_set(
preproc = list(rec_smote),
models = list(logit_mod, knn_mod, mars_mod)
)
fold<-vfold_cv(train, v=5, repeats=2, strata = Ach_class)
# tuning hyperparameters
eval_metrics <- metric_set(roc_auc, sens, spec)
start <- Sys.time()
result <- wfset %>%
workflow_map(resamples = fold,
grid = 50,
control = control_grid(save_pred = TRUE),
metrics = eval_metrics)
paste("time usage", round(Sys.time() - start,2), "mins")
show_notes(.Last.tune.result)
result
result %>% autoplot()
result %>% extract_workflow_set_result(id = "recipe_multinom_reg") %>%
autoplot()
result
result %>% extract_workflow_set_result(id = "recipe_mars") %>%
autoplot()
result %>% extract_workflow_set_result(id = "recipe_mars")
mars_mod <- mars() %>%
set_engine("earth") %>%
set_mode("classification")
# create workflowset
wfset <- workflow_set(
preproc = list(rec_smote),
models = list(logit_mod, knn_mod, mars_mod)
)
# tuning hyperparameters
eval_metrics <- metric_set(roc_auc, sens, spec)
start <- Sys.time()
result <- wfset %>%
workflow_map(resamples = fold,
grid = 50,
control = control_grid(save_pred = TRUE),
metrics = eval_metrics)
result <- wfset %>%
workflow_map(resamples = fold,
grid = 50,
control = control_grid(save_pred = TRUE),
metrics = eval_metrics)
paste("time usage", round(Sys.time() - start,2), "mins")
show_notes(.Last.tune.result)
### random forest (ุ6.3 secs)
rf_mod <- rand_forest(trees=300,
mtry=tune(),
min_n=tune()
) %>%
set_engine("ranger", importance = "permutation") %>%
set_mode("classification")
# preprocessing
rec_smote <- recipe(Ach_class ~. , data= train) %>%
step_rm(clus2:clus4, Ach) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_smote(Ach_class, neighbors = 5)
# model specification
### regularized logistic regression
logit_mod <- multinom_reg(penalty=tune(),
mixture=tune()) %>%
set_engine("glmnet") %>%
set_mode("classification")
### random forest (ุ6.3 secs)
rf_mod <- rand_forest(trees=300,
mtry=tune(),
min_n=tune()
) %>%
set_engine("ranger", importance = "permutation") %>%
set_mode("classification")
### KNN (4.44 sec per round)
knn_mod <- nearest_neighbor(neighbors = tune(),
weight_func = tune(),
dist_power = 2) %>%
set_engine("kknn") %>%
set_mode("classification")
### gradient boosting (10.42 secs per round)
library(xgboost)
boost_mod <- boost_tree(trees=300,
min_n = tune(),
tree_depth = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
# create workflowset
wfset <- workflow_set(
preproc = list(rec_smote),
models = list(logit_mod, knn_mod, rf_mod)
)
fold<-vfold_cv(train, v=5, repeats=2, strata = Ach_class)
# tuning hyperparameters
eval_metrics <- metric_set(roc_auc, sens, spec)
start <- Sys.time()
result <- wfset %>%
workflow_map(resamples = fold,
grid = 50,
control = control_grid(save_pred = TRUE),
metrics = eval_metrics)
paste("time usage", round(Sys.time() - start,2), "mins")
result
result %>% autoplot()
(result %>% extra)
best <- show_best(result %>% extract_workflow_set_result(id = "recipe_multinom_reg"),
n=1, metric = "sens")
best
## last fit
logit_result <- workflow() %>%
add_recipe(rec_smote) %>%
add_model(logit_mod) %>%
finalize_workflow(best) %>%
last_fit(split)
logit_result
logit_result %>% collect_metrics(summarise=T)
logit_result %>% collect_predictions()
logit_result %>% collect_predictions() %>%
conf_mat(truth = Ach_class , estimate = .pred_class)
logit_result %>% extract_fit_engine() %>% vip()
logit_result %>% extract_fit_engine() %>% vip(num_feature=20)
logit_result %>% extract_fit_engine() %>% vip(num_feature=20)+
scale_x_continuous(breaks=seq(0,18,1))
logit_result %>% extract_fit_engine() %>% vip(num_feature=20)+
scale_y_continuous(breaks=seq(0,18,1))
logit_result %>% extract_fit_engine() %>% vip(num_feature=20)+
scale_y_continuous(breaks=seq(0,18,1))+
theme(panel.grid.minor = element_blank())
logit_result %>% extract_fit_engine() %>% tidy() -> coef
coef
coef
logit_result %>% extract_fit_engine() %>% coef() -> coef
coef
coef$1
coef%>%summary()
coef%>%class()
coef[[1]]
head(dat_preproc)
glimpse(dat_preproc, width=50)
miss_var_summary(dat_preproc)
dat_preproc %>%
bind_shadow() %>%
glimpse()
dat_preproc %>%
bind_shadow() %>%
glimpse()
na<-function(x){
y<-ifelse(x=="NA",1,0)
return(y)
}
missing <- dat_preproc %>%
bind_shadow() %>%
dplyr::select(contains("NA")) %>%
mutate_all(na)
missing %>% dplyr::select(traveltime_NA, fam_relation_NA, PreTest_NA) %>%
summary()
missing %>% dplyr::select(traveltime_NA, fam_relation_NA, PreTest_NA)
# do PCA
pca_result<-PCA(missing, graph = F)
pca_result$eig
head(missing)
p1 <- plot(pca_result, choix = "var")
missing_factorscore <- pca_result$ind$coord
p2 <- missing_factorscore %>% data.frame() %>%
ggplot(aes(x = Dim.1, y=Dim.2))+
geom_jitter(width=0.3, height=0.3, alpha=0.5)+
ggtitle("factor score plot")
grid.arrange(p1,p2,ncol=2)
pca_result$var$coord
pca_result$var$coord %>% round(2)
dat_na <- dat_preproc%>%bind_shadow() %>%
dplyr::select(school:Ach,
PreTest_NA, traveltime_NA, fam_relation_NA) %>%
mutate(MomEdu = factor(MomEdu, levels=c(0,1,2,3,4),
labels=c("none","primary","highsch1","highsch2","grad")),
DadEdi = factor(DadEdi, levels=c(0,1,2,3,4),
labels=c("none","primary","highsch1","highsch2","grad")),
traveltime = factor(traveltime, levels=c(1,2,3,4),
labels=c("<15mins",
"15-30mins",
"30-60mins",
">60mins")),
readingtime = factor(readingtime, levels=c(1,2,3,4),
labels=c("<2hours",
"2-5hours",
"5-10hours",
">10hours")),
fam_relation = factor(fam_relation, levels=c(1,2,3,4,5),
labels=c("worst",
"bad",
"fair",
"good",
"very good")),
freetime = factor(freetime, levels=c(1,2,3,4,5),
labels=c("very little",
"litter",
"moderate",
"high",
"highest")),
goout = factor(goout, levels=c(1,2,3,4,5),
labels=c("very little",
"litter",
"moderate",
"high",
"highest")),
Drink_alc = factor(Drink_alc, levels=c(1,2,3,4,5),
labels=c("very little",
"litter",
"moderate",
"high",
"highest")),
health = factor(health, levels=c(1,2,3,4,5),
labels=c("worst",
"bad",
"fair",
"good",
"very good"))
)
imputed_bag %>% glimpse()
imputed_bag %>% glimpse(width=50)
### A
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_normalize(all_numeric()) %>% # calculate Z score
step_dummy(all_nominal_predictors()) %>%
prep(NULL) %>%
juice() %>%
glimpse(width=50)
### Lin
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric()) %>% # calculate Z score
prep(NULL) %>%
juice() %>%
glimpse(width = 50)
### Lin
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric()) %>% # calculate Z score
prep(NULL) %>%
juice() %>%
summary()
### Lin
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric()) %>% # calculate Z score
prep(NULL) %>%
juice() %>%
glimpse(width=50)
## C
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_range(all_numeric(),min=0,max=1) %>%
step_dummy(all_nominal_predictors()) %>%
prep(NULL) %>%
juice() %>%
glimpse(wisth=50)
## C
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_range(all_numeric(),min=0,max=1) %>%
step_dummy(all_nominal_predictors()) %>%
prep(NULL) %>%
juice() %>%
glimpse(width=50)
## C
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_range(all_numeric(),min=0,max=1) %>%
step_dummy(all_nominal_predictors()) %>%
prep(NULL) %>%
juice() %>%
glimpse(width=50) ->temp2
temp2$age %>% summary()
temp2$PreTest %>% summary()
### Lin
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric()) %>% # calculate Z score
prep(NULL) %>%
juice() %>%
glimpse(width=50)
### A
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_normalize(all_numeric()) %>% # calculate Z score
step_dummy(all_nominal_predictors()) %>%
prep(NULL) %>%
juice() %>%
glimpse(width=50)
### A
recipe(Ach~., data= imputed_bag) %>%
step_rm(clus2:clus4) %>%
step_normalize(all_numeric()) %>% # calculate Z score
step_dummy(all_nominal_predictors()) %>%
prep(NULL) %>%
juice() %>%
glimpse(width=50)->temp11
temp11$age %>% summary()
library(FactoMineR)
library(factoextra)
library(ggrepel)
library(cluster)
#do cluster analysis with mixed variable
library(clustMixType)
k_range <- 2:10
tot_withinSS <- numeric(length(k_range))
sil_scores <- numeric(length(k_range))
imputed_bag_scaled<-recipe(Ach~.,data=imputed_bag) %>%
step_normalize(all_numeric()) %>%
prep(NULL) %>%
juice()
gower_dist <- daisy(imputed_bag_scaled, metric = "gower")
for(k in k_range){
kproto_res <- kproto(imputed_bag_scaled, k)
tot_withinSS[k-1]<-kproto_res$tot.withinss
sil_scores[k-1]<-mean(silhouette(kproto_res$cluster,
gower_dist)[,3])
}
# Plot the total within-cluster sum of squares for each k
plot(k_range, tot_withinSS, type = "b", xlab = "Number of clusters (k)", ylab = "Total within-cluster sum of squares", main = "Elbow Method")
# Plot the average silhouette width for each k
plot(k_range, sil_scores, type = "b", xlab = "Number of clusters (k)", ylab = "Average silhouette width", main = "Silhouette Method")
cluster_result2$cluster
result_smote %>% collect_metrics(summarise=T) %>%
filter(.metric %in% c("roc_auc")) %>%
arrange(desc(mean)) %>%
dplyr::select(wflow_id, .metric, mean, n, std_err)
result_smote %>% collect_metrics(summarise=T) %>%
filter(.metric %in% c("sens")) %>%
arrange(desc(mean)) %>%
dplyr::select(wflow_id, .metric, mean, n, std_err)
6.3+4.44+10.42
10*50
500*21.16
500*21.16/3600
21.16-10
500*11.16/3600
result %>% extract_workflow_set_result(id = "recipe_multinom_reg") %>%
autoplot()
result %>% autoplot()
