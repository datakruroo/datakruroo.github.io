---
title: "Regression"
format: html
toc: true
message: false
warning: false
editor: visual
---

Regression เป็นวิธีการทางสถิติที่ใช้วิเคราะห์ความสัมพันธ์ระหว่างตัวแปรตาม (Dependent Variable) และตัวแปรอิสระ (Independent Variables) หนึ่งตัวหรือมากกว่า ซึ่งจะช่วยให้เราเข้าใจว่าตัวแปรตามมีการเปลี่ยนแปลงอย่างไรเมื่อมีการเปลี่ยนแปลงในตัวแปรอิสระ

วัตถุประสงค์ของการวิเคราะห์ regression หลัก ๆ มี 2 ข้อ

คือการสร้างโมเดลทางสถิติที่สามารถใช้

1.  อธิบายความสัมพันธ์ระหว่างตัวแปรตามกับตัวแปรอิสระ

2.  ทำนายแนวโน้มของตัวแปรตามเมื่อกำหนดตัวแปรอิสระ

การวิเคราะห์การถดถอยมีหลายประเภท

-   Simple regression

-   Multiple regression

-   Polynomial regression

-   Logistic regression

...

## Simple Linear Regression

เป็นโมเดลพื้นฐานใช้อธิบายความสัมพันธ์ระหว่างตัวแปรตามกับตัวแปรอิสระที่เป็นตัวแปรเดียว ทางสังคมศาสตร์อาจไม่ค่อยใช้โมเดลนี้ในทางปฏิบัติเท่าไหร่ แต่มักใช้เป็นโมเดลแรกในการอธิบาย concept เกี่ยวกับ regression

อย่างที่บอกว่า regression analysis จะทำการอธิบายและทำนายตัวแปรตามด้วยตัวแปรอิสระ ผ่านโมเดลทางสถิติ สำหรับ simple regression หรือเรียกเต็ม ๆ ว่า simple linear regression จะมีโมเดลเป็นสมการเส้นตรงที่มีส่วนประกอบจำแนกเป็นสองส่วนได้แก่

-   ส่วนที่เป็นความสัมพันธ์เชิงฟังก์ชัน

-   ส่วน noise หรือ error

$$
Y_i = \beta_0 + \beta_1X_i + \epsilon_i
$$

```{r}
library(tidyverse)
data <- read_csv("/Users/choat/Downloads/exam.csv")
glimpse(data)
```

สมมุติว่าวัตถุประสงค์การวิเคราะห์ เพื่อวิเคราะห์ความสัมพันธ์ระหว่าง ach กับ learning_performance โดยใช้ simple linear regression

### 1. exploring data

```{r}
data %>% 
  select(ach, learning_performance) %>% 
  cor()

data %>% 
  ggplot(aes(x = learning_performance, y = ach))+
  geom_point()
```

### 2. fit simple regression

การประมาณค่าพารามิเตอร์ใน linear regression ทุกโมเดลจะใช้ ordinary least squares (OLS)

วิธีการประมาณนี้จะพยายามหาค่าพารามิเตอร์ intercept และ slope ที่ทำให้ sum squared error น้อยที่สุด ---\> แปลว่าต้องการโมเดล regression ที่ทำนายได้แม่นยำมากที่สุด หรือ bias น้อยที่สุด

$$
Q = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i)^2
$$ ตอนประมาณค่าพารามิเตอร์จะใช้การหาอนุพันธ์ของ SSE เทียบกับพารามิเตอร์ intercept และ slope

$$
\frac{\partial Q}{\partial \beta_0}
$$

```{r}
simple_reg <- lm(ach ~ learning_performance, data = data)
summary(simple_reg)
```

```{r}
library(patchwork)
p1<-data %>% 
  ggplot(aes(x = learning_performance, y = ach))+
  geom_point()+
  geom_abline(intercept = 17.7039 , slope = 0.5654 , col = "steelblue",
              linewidth = 2)

p2<-data %>% 
  ggplot(aes(x = learning_performance, y = ach))+
  geom_point()+
  geom_smooth(method = "lm")
p1/p2

data %>% summary()
```

### 3. การแปลความหมายพารามิเตอร์ในโมเดล

```{r}
sqrt(86)
summary(simple_reg)
anova(simple_reg)
sqrt(966.9)
83212/ (33132 +83212)
```

-   Residual standard error --\> square root ของ MSE ดัชนีตัวนี้ใช้บอกว่า ค่าทำนายของ regression model ของเรามีความคลาดเคลื่อนไปจากค่าจริงโดยเฉลี่ยกี่หน่วย (คะแนน)

การแปลความหมายพารามิเตอร์หลัก ๆ ดูที่ intercept และ slope

-   slope คือ อัตราการเปลี่ยนแปลงของ y เมื่อเทียบกับ --\> delta_y/delta_x

ถ้า slope = 0.57 แปลว่า ถ้านักเรียนมี learning performance สูงขึ้น 1 หน่วย แล้ว ach ของนักเรียนมีแนวโน้มที่จะเพิ่มขึ้นโดยเฉลี่ย 0.57 คะแนน

-   intercept คือ จุดตัดแกน y --\> ค่าของ y เมื่อ x = 0

ถ้าเด็กไม่มี learning performance เลย (LP = 0) เด็กมีแนวโน้มที่จะมีคะแนน ach โดยเฉลี่ย 17.7 คะแนน

```{r}
fit_simreg <- lm(ach ~ learning_performance, data = data)
summary(fit_simreg)
```

## Multiple Regression

```{r}
data %>% count(engage)
data %>% 
#  mutate(engage = factor(engage, levels=c("no engage","little engage",
 #                                         "moderate engage","much engage"))) %>% 
 mutate(engage = fct_relevel(engage, "no engage","little engage",  "moderate engage","much engage"))  %>% 
with(lm(ach ~ learning_performance + engage)) -> fit_mulreg
summary(fit_mulreg)
```

-   เมื่อควบคุมให้ตัวแปรอิสระอื่น ๆ คงที่ ถ้านักเรียนมี LP สูงขึ้น 1 คะแนน นักเรียนมีแนวโน้มที่จะมีค่าเฉลี่ย ach สูงขึ้น 0.55 คะแนน

-   เมื่อควบคุมให้ตัวแปรอิสระอื่น ๆ คงที่ นักเรียนที่มีส่วนร่วมในการเรียนน้อยมีแนวโน้มที่จะมี ach โดยเฉลี่ยต่ำกว่านักเรียนที่ไม่มีส่วนร่วมในการเรียน 2.21 คะแนน อย่างไรก็ตามความแตกต่างดังกล่าวไม่พบนัยสำคัญทางสถิติที่ระดับ .05

```{r}
fit_mulreg <- lm(ach ~ learning_performance + engage, data = data)
summary(fit_mulreg)
```

## Regression with Interaction

อิทธิพลปฏิสัมพันธ์ (interaction effect) คืออิทธิพลรวมกันระหว่างตัวแปรอิสระอย่างน้อย 2 ตัวแปร ที่มีผลต่อตัวแปรตาม ที่เมื่อเกิดอิทธิพลของตัวแปรอิสระดังกล่าวจะมีแนวโน้มเปลี่ยนแปลงไปตามค่าของตัวแปรอิสระอื่น ๆ ที่อยู่ในกลุ่มปฏิสัมพันธ์เดียวกัน

![](https://datakruroo.github.io/Programming/data_analysis/img/interaction.png)

### Two-way interaction effect

-   ~~ตัวแปรอิสระทั้งสองเป็นตัวแปรจัดประเภท~~

-   ตัวแปรอิสระเป็นตัวแปรจัดประเภทและตัวแปรอิสระเชิงปริมาณ

-   ตัวแปรอิสระทั้งสองเป็นตัวแปรเชิงปริมาณ

#### ตัวแปรอิสระเป็นตัวแปรจัดประเภทและตัวแปรอิสระเชิงปริมาณ

ใช้ชุดข้อมูล [teachersalary.csv](TeacherSalaryData.csv)

```{r}
data <- read_csv("TeacherSalaryData.csv")
glimpse(data)
fit_interac1 <- lm(salary ~ yrs.service*discipline, data = data)
summary(fit_interac1)
```

```{r}
fit_interac2 <- data %>% 
  mutate(rank = fct_relevel(rank, "AsstProf", "AssocProf", "Prof")) %>% 
  with(lm(salary~ yrs.service + discipline + yrs.service:discipline))
summary(fit_interac2)
```

การอ่านผล

1.  เราสามารถประเมินอิทธิพลของ main effect ที่ไม่เกี่ยวข้องกับ interaction term ได้เลย

-   เมื่อควบคุมให้ตัวแปรอิสระอื่นคงที่ อาจารย์ที่เป็นรองศาสตราจารย์มีแนวโน้มจะมีเงินเดือนสูงกว่า ผศ. ประมาณ 14000 บท อย่างมีนัยสำคัญทางสถิติที่ระดับ .05 ()

-   เมื่อควบคุมให้ตัวแปรอิสระอื่นคงที่ ศาสตราจารย์ของมหาลัยมีแนวโน้มจะมีเงินเดือนสูงกว่า ผศ. ประมาณ 50000 บาท อย่างมีนัยสำคัญทางสถิติที่ระดับ .05

2.  พิจารณาส่วนปฏิสัมพันธ์

-   พิจารณา term ปฏิสัมพันธ์ก่อนว่ามีนัยสำคัญรึเปล่า (เชิงสถิติ/เชิงปฏฺิบัติ) ถ้าพบนัยสำคัญทางสถิติเราจะไม่อ่านผลที่ main effect

-   เราต้องไปประเมิน simple effect แทน

    -   ประเมินผลของ yrs.service ที่มีต่อ ach จำแนกตาม discipline A และ B \<-- การวิเคราะห์ simple slope

    -   ประเมินผลของ discipline ที่มีต่อ ach จำแนกตาม yrs.service

```{r}
data %>% 
  ggplot(aes(x=yrs.service))+
  geom_histogram()
```

##### การวิเคราะห์ simple slope ของ yrs.service จำแนกตาม discipline

วิเคราะห์อิทธิพลของ yrs.service ต่อ ach จำแนกตาม discipline

เราจะดู simple slope ของ cont. จำแนกตาม cat.

```{r}
library(emmeans)
simple_slopes <- emtrends(fit_interac2, var = "yrs.service", spec = "discipline")
## slope ของ yrs.service จำแนกตาม discipline
simple_slopes 
## ความแตกต่างของ slope ของ yrs.service ระหว่าง 2 discipline
contrast(simple_slopes, method = "pairwise")
```

ลองใช้ `simple_slopes <- emmeans(fit_interac1, pairwise ~ discipline | yrs.service)` ผลลัพธ์ที่ได้แตกต่างกันอย่างไร

##### วิเคราะห์ simple effect ของ discipline บน yrs.service

simple effect ของ discipline ที่วิเคราะห์ได้แต่ละค่าของ yrs.service

```{r}
data %>% summary()
```

ในทำนองเดียวกับ anova เราสามารถวิเคราะห์ simple effect ของ discipline แต่ในกรณีนี้จะเป็นการจำแนกตามระดับของ yrs.service ซึ่งมีจำนวนได้มากมาย

```{r}
summary(data)
emmeans(fit_interac2, pairwise ~ yrs.service | discipline)

emmeans <- emmeans(fit_interac2, pairwise ~ yrs.service | discipline, 
        at = list(yrs.service = c(7, 8,27)))
```

```{r}
emmeans$emmeans %>% data.frame() %>% 
  ggplot(aes(x=emmean, y=factor(yrs.service), col =discipline ))+
  geom_point()+
  geom_errorbar(aes(xmin = lower.CL, xmax = upper.CL), width = 0.1)
```

เราอาจ plot แผนภาพแสดงความสัมพันธ์แบบ interaction ได้ง่าย ๆ ดังนี้

```{r}
fit_interac2 %>% summary()

## partial dependence plot (pdp)

### 1. สร้าง grid ที่เป็น combination ระหว่างตัวแปรอิสระที่ interact กันก่อน
### สร้าง grid ของ yrs.service
yrs_grid <- seq(min(data$yrs.service), max(data$yrs.service), 2)
### สร้าง grid ของ discipline
dis_grid <- c("A","B")

### combination ของ grid
expand_grid(yrs.service = yrs_grid, discipline = dis_grid)

### ทำหน้าที่เป็นค่าของตัวแปรอิสระที่ต้องการนำไปหาค่าทำนาย
grid_pred <- expand_grid(yrs.service = yrs_grid,
                      discipline = unique(data$discipline))
predict(fit_interac2)
predict(fit_interac2, newdata = grid_pred)

grid_pred %>% 
  mutate(pred = predict(fit_interac2, newdata = grid_pred)) %>% 
  ggplot(aes(x = yrs.service, y = pred, color = discipline)) +
  geom_line()



```

```{r}
simple_slopes 
```

```{r}
p1<-data %>% 
  ggplot(aes(x=yrs.service, y=salary))+
  geom_point()+
  geom_smooth(method = "lm")

p2<-data %>% 
  ggplot(aes(x=yrs.service, y=salary, col = discipline))+
  geom_point()+
  geom_smooth(method = "lm")

p1/p2
```

#### ตัวแปรอิสระทั้งสองเป็นตัวแปรเชิงปริมาณ

![](img/interaction2.png){width="50%"}

```{r}
data <- read_csv("eff.csv")
glimpse(data)
```

$$
eff = \beta_0 + \beta_1motiv + \beta_2seef + \beta_3motiv \times seef + \epsilon
$$

\$\$ eff = \beta\_0 + \beta\_1motiv + (\beta\_2seef + \beta\_3motiv \times seef) + \epsilon

\$\$

\$\$ eff = \beta\_0 + \beta\_1motiv + (\beta\_2 + \beta\_3motiv)seef + \epsilon

\$\$

```{r}
data %>% summary()
```

```{r}
fit_interac3 <- data %>% 
  with(lm(eff ~ motiv + seef + motiv:seef))

summary(fit_interac3)
```

ดังนั้นเราสรุปได้ว่า

อิทธิพลของ seef ที่มีต่อ eff อธิบายได้ด้วยสมการ 7.75 - 1.09 x motiv

```{r}
motiv<-3
slope_seef <- 7.75 - 1.09*motiv
```

```{r}
motiv<-5.048
slope_seef <- 7.75 - 1.09*motiv
```

```{r}
data %>% 
  ggplot(aes(x=seef, y=eff))+
  geom_point()+
  geom_smooth(method = "lm", se = F)+
  geom_abline(intercept = 13.6969,slope = 4.48, col ="black", linetype = 2)+
  geom_abline(intercept = 13.6969,slope = 2.24768, col ="black", linetype = 2)+
  geom_abline(intercept = 13.6969,slope = 0.12, col ="black", linetype = 2)+
  ylim(0,80)


simple_slope_seef <- emtrends(fit_interac3, var="seef", spec = "motiv",
         at = list(motiv= c(4.209,5.062,5.954)))

simple_slope_seef %>% contrast(method = "pairwise")

simple_slope_seef %>% data.frame()

data %>% ggplot(aes(x=seef, y=eff))+
  geom_abline(intercept = ,slope = )
```

```{r}
#install.packages("ggeffects")
library(ggeffects)
```

```{r}
ggpredict(fit_interac3, terms = c("seef", "motiv")) |> plot()
```

```{r}
data %>% summary()
```

## Polynomial Regression

สมการถดถอยพหุนาม

ความสัมพันธ์ระหว่างตัวแปรบนโลก อาจจำแนกได้เป็น 2 ลักษณะ

-   linear relationship

-   non-linear relationship

โมเดลการวิเคราะห์ความสัมพันธ์ระหว่างตัวแปรอาจจำแนกได้เป็น 2 ลักษณะเช่นกัน

-   linear (in parameters) model

-   non-linear model

$$
y = mx + c
$$
$$
y = ax^2+bx+c
$$

$$
y = \frac{exp(ax+b)}{1+exp(ax+b)}
$$


```{r}
library(gapminder)
library(ggforce)

gapminder %>% 
  filter(year=="2007") %>% 
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth(method = "lm")+
  scale_x_log10()
```

การสร้าง term polynomial อาจทำได้สองวิธีการ วิธีการแรกคือการใช้ฟังก์ชัน identity function (`I()`) และวิธีการที่สองคือการใช้ `poly()` ความแตกต่างระหว่างการสร้างเทอมพหุนามของทั้งสองวิธีการคือ

-   `I()` เป็นวิธีการสร้างพหุนามโดยตรง โดยเราจะใส่ตัวแปรที่ต้องการยกกำลังในฟังก์ชันนี้ ตัวอย่างเช่น I(x\^2) หมายถึงการสร้างเทอม x\^2 ที่ใช้ในโมเดลการถดถอย ดังนั้นการแปลความหมาย slope ของตัวแปรที่เป็นพหุนามนี้สามารถทำได้อย่างตรงไปตรงมา เพราะตัวแปรต่าง ๆ อยู่ในสเกลต้นฉบับ

-   ปัญหาหนึ่งของการใช้ `I()` คือ multicollinearity!!!

-   `poly()` จะสร้างเทอมพหุนามที่เรียกว่า orthogonal polynomial ซึ่งจะทำให้เราไม่ต้องกังวลเรื่อง multicollinearity แต่ต้องแลกมาด้วยการแปลความหมายความสัมพันธ์ที่ยาก เพราะตัวแปรที่ถูกแปลงเป็น orthogonal polynomial จะอยู่คนละสเกลกับตัวแปรเดิม การแปลงลักษณะนี้จึงเหมาะกับวัตถุประสงค์ในการทำนายมากกว่าอธิบายความสัมพันธ์ื

```{r}
cubic_fit <- gapminder %>% 
  filter(year=="2007") %>% 
  with(lm(lifeExp ~ gdpPercap + I(gdpPercap^2) + I(gdpPercap^3))) 
```

$$
lifeExp = (5.296e+01) + (2.734e-03)gdpPercap - (9.270e-08)gdpPercap^2 + (1.009e-12)gdpPercap^3
$$ \

### ปัญหา Multicollinearity ใน Polynomial Regression

variance inflation factor --\> เป็นดัชนีที่ใช้วัด multicollinearity ของตัวแปรอิสระในโมเดล regression ถ้าค่า VIF มีค่ามากกว่า 10 แสดงว่ามี multicollinearity ในโมเดล

```{r}
#install.packages("car")
library(car)
vif(cubic_fit)
```

```{r}
gapminder %>% 
  filter(year=="2007") %>% 
  mutate(x = gdpPercap,
         x2 =gdpPercap^2,
         x3 = gdpPercap^3) %>% 
  select(x,x2,x3) %>% cor()

```

```{r}
## correlation matrix between centering variables
gapminder %>% 
  filter(year=="2007") %>% 
  mutate(x = gdpPercap,
         x2 =gdpPercap^2,
         x3 = gdpPercap^3) %>% 
  mutate(x_center = x-mean(x),
         x_center2 = x_center^2,
         x_center3 = x_center^3) %>%
  select(x_center, x_center2,x_center3) %>% cor()

## scatter plot
gapminder %>% 
  filter(year=="2007") %>% 
  mutate(x = gdpPercap,
         x2 =gdpPercap^2,
         x3 = gdpPercap^3) %>% 
  mutate(x_center = x-mean(x),
         x_center2 = x_center^2,
         x_center3 = x_center^3) %>%
  select(x_center, x_center2,x_center3) %>% 
  ggplot(aes(x_center,x_center3 ))+
  geom_line()
```

```{r}
cubic_fit <- gapminder %>% 
  filter(year=="2007") %>% 
  with(lm(lifeExp ~ poly(gdpPercap, 3)))
cubic_fit %>% summary()
```

## Regression Diagnostics

regression เป็นโมเดลทางสถิติแบบ parametric ที่การใช้งานจะต้องอยู่ภายใต้ข้อตกลงเบื้องต้นของโมเดลที่ค่อนข้าง strict (แต่ก็ไม่ได้แปลว่าต้อง strict มาก ๆ )

-   Independence

-   Linearity

-   Normality

-   Homoscedasticity

-   No Missing Values

-   No Influential Outlier

    -   Outlier คือ ค่าสังเกตที่ทำนายไม่ได้หรือมีประสิทธิภาพการทำนายด้วย regression ต่ำ (กล่าวคือมีค่าสัมบูรณ์ของ residual ที่มากเกินไป)

```{r}
gapminder %>% 
  filter(year=="2007") %>% 
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth()+
  geom_ellipse(data = gapminder %>% 
                        filter(year=="2007") %>% 
                 filter(lifeExp < 60 & gdpPercap > 8000),
               aes(x0 = mean(gdpPercap), y0 = mean(lifeExp), 
                   a = 2*sd(gdpPercap), b = 2*sd(lifeExp), angle = 0),
               color = 'maroon', linetype = "dashed", fill = NA)
```

-   Leverage value คือ combination ของค่าสังเกตของตัวแปรอิสระที่ผิดปกติ กล่าวคือเป็น outlier ใน feature space ค่า leverage ไม่จำเป็นต้องมีผลต่อค่าของเส้นถดถอยเสมอไป มันเพียงแค่บอกว่าจุดนั้นอยู่ห่างไกลและมีศักยภาพที่จะส่งผลกระทบต่อโมเดล

$$
h_{ii} = x_i^T (X^T X)^{-1} x_i
$$ ค่า $h_{ii}$ มีค่าอยู่ในช่วง \[0,1\] ถ้ามีค่าใกล้ 1.0 แสดงว่าค่าสังเกตของตัวแปรอิสระนั้นมีแนวโน้มอยู่ใกล้จากแนวโน้มส่วนใหญ่

-   Influential observation คือค่าสังเกตที่มีอิทธิพลต่อการประมาณค่าพารามิเตอร์ในโมเดลมากกว่าปกติ การประเมินค่าสังเกตประเภทนี้ทำได้ด้วยสถิติ Cook's distance

$$
D_i = \frac{(e_i^2)}{p \cdot MSE} \left( \frac{h_{ii}}{(1 - h_{ii})^2} \right)
$$

```{r fig.height=8}
library(patchwork)
p1 <- gapminder %>% 
  filter(year=="2007") %>% 
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth()

p2 <- gapminder %>% 
  filter(year=="2007") %>% 
  filter(gdpPercap < 10000) %>%
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth()
  
p1/p2
```

## Regression Remedials

1.  Linearity

-   เพิ่ม polynomial หรือ interaction terms เพื่อเพิ่มความซับซ้อนของโมเดล

-   แปลงข้อมูลด้วยการใช้ log, square root, หรือการแปลงเชิงเส้นตรงแบบอื่น ๆ

-   ใช้ Multivariate Adaptive Regression Splines (MARs) Generalized Additive Models (GAMs) เพื่อจับความสัมพันธ์ที่ไม่เป็นเชิงเส้น

2.  Normality

-   แปลงข้อมูล (log-transformation, square root, Box-Cox) เพื่อให้ residuals เป็นปกติมากขึ้น

-   ใช้ robust regression เพื่อทนทานต่อผลกระทบของ outliers

-   เพิ่มตัวแปรอิสระหรือเทอมของตัวแปรอิสระที่อาจหายไปเพื่อปรับปรุงการกระจายของ residuals

3.  Homoscedasticity

-   แปลงค่าตัวแปรตาม (เช่น log, square root) เพื่อลดความแปรปรวน

-   ใช้ Weighted Least Squares (WLS) เพื่อปรับน้ำหนักให้กับจุดข้อมูลที่มีความแปรปรวนต่างกัน

-   ใช้ heteroscedasticity-robust standard errors เพื่อปรับ standard errors ของโมเดล

4.  No Influential Ouliers

-   ลบหรือปรับค่าสังเกตที่ถูกระบุว่าเป็น influential outiers

-   ใช้ robust regression

## Model Selection

ในบริบทของการวิเคราะห์การถดถอย โดยปกติการคัดเลือกโมเดลส่วนหลัก ๆ จะเกี่ยวข้องกับการคัดเลือกตัวแปรอิสระเข้าสู่โมเดล และอีกส่วนหนึ่งคือการกำหนด functional form ของโมเดล ในบทเรียนนี้จะกล่าวถึงส่วน feature selection 

การทำ feature selection คือการคัดกรองตัวแปรที่ไม่มีสาระสำคัญหรือมีความสำคัญน้อยในการพัฒนาโมเดลออกไป


```{mermaid}
flowchart TD

F1-->X1((X))
F2-->A[F2]
F3-->B[F3]
F4-->X2((X))
F5-->X3((X))
```


การคัดเลือกตัวแปรอิสระเข้าสู่โมเดลมีหลายขั้นตอน อาจจะจำแนกออกเป็นสองประเภทใหญ่ ได้แก่ theory-driven และ data-driven โดยที่ theory-driven จะใช้ความรู้ที่มีอยู่จากงานวิจัยหรือทฤษฎีที่เกี่ยวข้องเพื่อกำหนดและคัดเลือกตัวแปร  ในขณะที่ data-driven จะใช้ข้อมูลที่มีอยู่เพื่อคัดเลือกตัวแปรอิสระที่มีความเหมาะสมและมีประสิทธิภาพในการทำนายเข้าสู่โมเดล

- filter methods

- wrapper methods

- embedded methods


### กิจกรรมที่ 1 ทำลองวิเคราะห์การถดถอย

ลองทำ regression analysis เพื่อสร้างโมเดลทำนาย student_outcome_online แล้วพิจารณาผลลัพธ์ที่ได้

```{r}

large_data <- read_csv("/Users/choat/Downloads/onlinelearning_miss.csv")
large_data %>% glimpse()
large_data %>% dplyr::select(-1) %>% 
### ปรับชื่อตัวแปรใหม่
  rename(student_id = X,
         school_type = 2,
         school_location = 3,
         num_subject = 4,
         num_online = 5,
         num_onsite = 6,
         school_readiness = 7,
         avg_teacher_age = 15,
         avg_teacher_techcomp = 16,
         student_gender = 23,
         student_level = 24,
         student_location = 25,
         student_learningstyle = 26,
         student_internet_readiness = 30,
         student_home_climate = 31,
         student_techcomp = 32,
         student_learning_attitude = 33,
         student_stress_online = 40,
         student_stress_onsite = 41
         ) %>% 
### สร้างตัวแปรใหม่
  rowwise() %>% 
  mutate(
         student_climate_online = mean(c(climate.online1, climate.online2, climate.online3)),
         student_climate_onsite = rowMeans(across(starts_with("climate.onsite"))),
         student_interaction_online = mean(c(interaction.online1, interaction.online2, interaction.online3)),
         student_interaction_onsite = rowMeans(across(starts_with("interaction.onsite"))),
         student_outcome_online = (outcome.online1*20 + outcome.online2*20 + outcome.online3*20 +outcome.online4)/4,
         student_outcome_onsite = (outcome.onsite1*20 + outcome.onsite2*20 + outcome.onsite3*20 +outcome.onsite4)/4
         ) %>%
  dplyr::select(-starts_with("interaction"), -starts_with("outcome")) %>% 
  dplyr::select(-starts_with("climate")) %>%
  mutate(student_num_device = sum(c(smartphone,ipad,computer),na.rm = T))  %>% 
  ungroup()
```

ผลการวิเคราะห์ที่ได้มีข้อสังเกตอะไรบ้าง

### Filter Methods

Filter Methods เป็นหนึ่งในเทคนิคการคัดเลือกตัวแปร (Feature Selection) ที่ใช้ คุณสมบัติทางสถิติของตัวแปร ในการคัดกรองตัวแปรที่มีความสำคัญต่อการสร้างโมเดล โดยกระบวนการนี้จะไม่พึ่งพาโมเดลใด ๆ ในการประเมิน แต่จะใช้สถิติพื้นฐาน เช่น ความสัมพันธ์ (correlation) ระหว่างตัวแปรอิสระกับตัวแปรตาม และค่าสถิติอื่น ๆ เช่น Chi-square, ANOVA, Mutual Information เพื่อกรองตัวแปรที่มีความสำคัญสูงไว้

- filter zero or near-zero variance

- correlation-based filtering

จุดเด่นของวิธีการนี้คือทำได้อย่างรวดเร็ว แต่เป็นการวิเคราะห์ที่อาจจะละเลยความสัมพันธ์ที่ซับซ้อนระหว่างตัวแปรอิสระกับตัวแปรตาม นอกจากนี้ผลลัพธ์ที่ได้อาจจะไม่ได้เป็นผลลัพธ์ที่ดีที่สุดที่เป็นไปได้

```{r}
### create predictor matrix
use_data <- large_data %>% dplyr::select(-1) %>% 
### ปรับชื่อตัวแปรใหม่
  rename(student_id = X,
         school_type = 2,
         school_location = 3,
         num_subject = 4,
         num_online = 5,
         num_onsite = 6,
         school_readiness = 7,
         avg_teacher_age = 15,
         avg_teacher_techcomp = 16,
         student_gender = 23,
         student_level = 24,
         student_location = 25,
         student_learningstyle = 26,
         student_internet_readiness = 30,
         student_home_climate = 31,
         student_techcomp = 32,
         student_learning_attitude = 33,
         student_stress_online = 40,
         student_stress_onsite = 41
         ) %>% 
### สร้างตัวแปรใหม่
  rowwise() %>% 
  mutate(
         student_climate_online = mean(c(climate.online1, climate.online2, climate.online3)),
         student_climate_onsite = rowMeans(across(starts_with("climate.onsite"))),
         student_interaction_online = mean(c(interaction.online1, interaction.online2, interaction.online3)),
         student_interaction_onsite = rowMeans(across(starts_with("interaction.onsite"))),
         student_outcome_online = (outcome.online1*20 + outcome.online2*20 + outcome.online3*20 +outcome.online4)/4,
         student_outcome_onsite = (outcome.onsite1*20 + outcome.onsite2*20 + outcome.onsite3*20 +outcome.onsite4)/4
         ) %>%
  dplyr::select(-starts_with("interaction"), -starts_with("outcome")) %>% 
  dplyr::select(-starts_with("climate")) %>%
  mutate(student_num_device = sum(c(smartphone,ipad,computer),na.rm = T))  %>% 
  ungroup()
```


แบ่งชุดข้อมูล online learning 

```{r}
online_data <- use_data %>% 
  dplyr::select(-contains("onsite"))
glimpse(online_data)
```


#### Near-Zero Variance

สร้างชุดข้อมูล predictor matrix ของ online learning

- `freqCut` เปรียบเทียบความถี่ของค่าที่พบมากที่สุดกับค่าที่พบมากที่สุดเป็นอันดับสอง เพื่อดูว่าคอลัมน์นั้นมีการกระจายตัวของค่ามากน้อยเพียงใด

- `uniqueCut` เปรียบเทียบจำนวนค่าที่ไม่ซ้ำกันกับจำนวนตัวอย่างทั้งหมดในคอลัมน์

```{r}
predictor_online <- online_data %>% dplyr::select(-student_outcome_online)
## install.packages("caret")
library(caret)
nearZeroVar(predictor_online, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)
```

#### Correlation-based method




### Wrapper Methods

- สร้างโมเดลทำนายหลายชุด: ทดลองชุดของตัวแปรหลายชุด โดยเลือกเพิ่มหรือลบตัวแปรทีละตัว

- ใช้เกณฑ์การประเมินโมเดลเช่น AIC, BIC, AUC, F1-score, RMSE, MAE, หรือค่าสถิติอื่น ๆ ในการประเมินความสามารถของโมเดล

- เลือกชุดตัวแปรที่ดีที่สุด: ชุดตัวแปรที่ให้ประสิทธิภาพดีที่สุดจะถูกเลือกเพื่อใช้ในการสร้างโมเดลสุดท้าย

#### Best Subset Regression

- วิธีการหนึ่งที่ใช้ค้นหาโมเดลที่ดีที่สุดคือ best subset regression ซึ่งจะทำการสร้างโมเดลทุกโมเดลที่เป็นไปได้จากตัวแปรอิสระที่มี จากนั้นเปรียบเทียบ empirical fit ระหว่างโมเดลดังกล่าว

- อย่างไรก็ตามวิธีการดังกล่าวจะใช้ทรัพยากรค่อนข้างเยอะ เพราะจะต้องสร้างโมเดลจากทุกความเป็นไปได้ ยกตัวอย่างเช่น หากมีตัวแปรอิสระทั้งหมด p ตัว โมเดลที่มีเฉพาะ main effect ที่เป็นไปได้จะมีทั้งหมด 2^p โมเดล เทคนิคนี้จึงเหมาะสำหรับสถานการณ์ที่มีข้อมูลที่ขนาดไม่ใหญ่มากเกินไป


```{r}
#install.packages("leaps")
library(leaps)

reg_model <- regsubsets(student_outcome_online ~ . -student_id, data=online_data , nbest =1, nvmax = 10,
                        method = "exhaustive")
sum <- summary(reg_model)
```


ข้อมูลตัวอย่างสำหรับหัวข้อนี้คือ TeacherSalaryData.csv

```{r}
data <- read_csv("TeacherSalaryData.csv")
glimpse(data)
### visualize numeric variables
p1<-data %>% 
  select_if(is.numeric) %>%
  dplyr::select(-1) %>% 
  pivot_longer(everything()) %>%
  ggplot(aes(value))+
  geom_histogram(fill = "steelblue")+
  facet_wrap(~name, scales = "free")+
  theme_light()
### visualize categorical variables
p2<-data %>% 
  select_if(is.character) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value))+
  geom_bar(fill = "steelblue")+
  facet_wrap(~name, scales = "free")+
  theme_light()
p1/p2
```


