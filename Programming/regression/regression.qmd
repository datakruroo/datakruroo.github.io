---
title: "Regression"
format: html
toc: true
message: false
warning: false
editor: visual
---

Regression เป็นวิธีการทางสถิติที่ใช้วิเคราะห์ความสัมพันธ์ระหว่างตัวแปรตาม (Dependent Variable) และตัวแปรอิสระ (Independent Variables) หนึ่งตัวหรือมากกว่า ซึ่งจะช่วยให้เราเข้าใจว่าตัวแปรตามมีการเปลี่ยนแปลงอย่างไรเมื่อมีการเปลี่ยนแปลงในตัวแปรอิสระ

วัตถุประสงค์ของการวิเคราะห์ regression หลัก ๆ มี 2 ข้อ

คือการสร้างโมเดลทางสถิติที่สามารถใช้

1.  อธิบายความสัมพันธ์ระหว่างตัวแปรตามกับตัวแปรอิสระ

2.  ทำนายแนวโน้มของตัวแปรตามเมื่อกำหนดตัวแปรอิสระ

การวิเคราะห์การถดถอยมีหลายประเภท

-   Simple regression

-   Multiple regression

-   Polynomial regression

-   Logistic regression

...

## Simple Linear Regression

เป็นโมเดลพื้นฐานใช้อธิบายความสัมพันธ์ระหว่างตัวแปรตามกับตัวแปรอิสระที่เป็นตัวแปรเดียว ทางสังคมศาสตร์อาจไม่ค่อยใช้โมเดลนี้ในทางปฏิบัติเท่าไหร่ แต่มักใช้เป็นโมเดลแรกในการอธิบาย concept เกี่ยวกับ regression

อย่างที่บอกว่า regression analysis จะทำการอธิบายและทำนายตัวแปรตามด้วยตัวแปรอิสระ ผ่านโมเดลทางสถิติ สำหรับ simple regression หรือเรียกเต็ม ๆ ว่า simple linear regression จะมีโมเดลเป็นสมการเส้นตรงที่มีส่วนประกอบจำแนกเป็นสองส่วนได้แก่

-   ส่วนที่เป็นความสัมพันธ์เชิงฟังก์ชัน

-   ส่วน noise หรือ error

$$
Y_i = \beta_0 + \beta_1X_i + \epsilon_i
$$

```{r}
library(tidyverse)
data <- read_csv("/Users/choat/Downloads/exam.csv")
glimpse(data)
```

สมมุติว่าวัตถุประสงค์การวิเคราะห์ เพื่อวิเคราะห์ความสัมพันธ์ระหว่าง ach กับ learning_performance โดยใช้ simple linear regression

### 1. exploring data

```{r}
data %>% 
  select(ach, learning_performance) %>% 
  cor()

data %>% 
  ggplot(aes(x = learning_performance, y = ach))+
  geom_point()
```

### 2. fit simple regression

การประมาณค่าพารามิเตอร์ใน linear regression ทุกโมเดลจะใช้ ordinary least squares (OLS)

วิธีการประมาณนี้จะพยายามหาค่าพารามิเตอร์ intercept และ slope ที่ทำให้ sum squared error น้อยที่สุด ---\> แปลว่าต้องการโมเดล regression ที่ทำนายได้แม่นยำมากที่สุด หรือ bias น้อยที่สุด

$$
Q = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i)^2
$$ ตอนประมาณค่าพารามิเตอร์จะใช้การหาอนุพันธ์ของ SSE เทียบกับพารามิเตอร์ intercept และ slope

$$
\frac{\partial Q}{\partial \beta_0}
$$

```{r}
simple_reg <- lm(ach ~ learning_performance, data = data)
summary(simple_reg)
```

```{r}
library(patchwork)
p1<-data %>% 
  ggplot(aes(x = learning_performance, y = ach))+
  geom_point()+
  geom_abline(intercept = 17.7039 , slope = 0.5654 , col = "steelblue",
              linewidth = 2)

p2<-data %>% 
  ggplot(aes(x = learning_performance, y = ach))+
  geom_point()+
  geom_smooth(method = "lm")
p1/p2

data %>% summary()
```

### 3. การแปลความหมายพารามิเตอร์ในโมเดล

```{r}
sqrt(86)
summary(simple_reg)
anova(simple_reg)
sqrt(966.9)
83212/ (33132 +83212)
```

-   Residual standard error --\> square root ของ MSE ดัชนีตัวนี้ใช้บอกว่า ค่าทำนายของ regression model ของเรามีความคลาดเคลื่อนไปจากค่าจริงโดยเฉลี่ยกี่หน่วย (คะแนน)

การแปลความหมายพารามิเตอร์หลัก ๆ ดูที่ intercept และ slope

-   slope คือ อัตราการเปลี่ยนแปลงของ y เมื่อเทียบกับ --\> delta_y/delta_x

ถ้า slope = 0.57 แปลว่า ถ้านักเรียนมี learning performance สูงขึ้น 1 หน่วย แล้ว ach ของนักเรียนมีแนวโน้มที่จะเพิ่มขึ้นโดยเฉลี่ย 0.57 คะแนน

-   intercept คือ จุดตัดแกน y --\> ค่าของ y เมื่อ x = 0

ถ้าเด็กไม่มี learning performance เลย (LP = 0) เด็กมีแนวโน้มที่จะมีคะแนน ach โดยเฉลี่ย 17.7 คะแนน

```{r}
fit_simreg <- lm(ach ~ learning_performance, data = data)
summary(fit_simreg)
```

## Multiple Regression

```{r}
data %>% count(engage)
data %>% 
#  mutate(engage = factor(engage, levels=c("no engage","little engage",
 #                                         "moderate engage","much engage"))) %>% 
 mutate(engage = fct_relevel(engage, "no engage","little engage",  "moderate engage","much engage"))  %>% 
with(lm(ach ~ learning_performance + engage)) -> fit_mulreg
summary(fit_mulreg)
```

-   เมื่อควบคุมให้ตัวแปรอิสระอื่น ๆ คงที่ ถ้านักเรียนมี LP สูงขึ้น 1 คะแนน นักเรียนมีแนวโน้มที่จะมีค่าเฉลี่ย ach สูงขึ้น 0.55 คะแนน

-   เมื่อควบคุมให้ตัวแปรอิสระอื่น ๆ คงที่ นักเรียนที่มีส่วนร่วมในการเรียนน้อยมีแนวโน้มที่จะมี ach โดยเฉลี่ยต่ำกว่านักเรียนที่ไม่มีส่วนร่วมในการเรียน 2.21 คะแนน อย่างไรก็ตามความแตกต่างดังกล่าวไม่พบนัยสำคัญทางสถิติที่ระดับ .05

```{r}
fit_mulreg <- lm(ach ~ learning_performance + engage, data = data)
summary(fit_mulreg)
```

## Regression with Interaction

อิทธิพลปฏิสัมพันธ์ (interaction effect) คืออิทธิพลรวมกันระหว่างตัวแปรอิสระอย่างน้อย 2 ตัวแปร ที่มีผลต่อตัวแปรตาม ที่เมื่อเกิดอิทธิพลของตัวแปรอิสระดังกล่าวจะมีแนวโน้มเปลี่ยนแปลงไปตามค่าของตัวแปรอิสระอื่น ๆ ที่อยู่ในกลุ่มปฏิสัมพันธ์เดียวกัน

![](https://datakruroo.github.io/Programming/data_analysis/img/interaction.png)

### Two-way interaction effect

-   ~~ตัวแปรอิสระทั้งสองเป็นตัวแปรจัดประเภท~~

-   ตัวแปรอิสระเป็นตัวแปรจัดประเภทและตัวแปรอิสระเชิงปริมาณ

-   ตัวแปรอิสระทั้งสองเป็นตัวแปรเชิงปริมาณ

#### ตัวแปรอิสระเป็นตัวแปรจัดประเภทและตัวแปรอิสระเชิงปริมาณ

ใช้ชุดข้อมูล [teachersalary.csv](TeacherSalaryData.csv)

```{r}
data <- read_csv("TeacherSalaryData.csv")
glimpse(data)
fit_interac1 <- lm(salary ~ yrs.service*discipline, data = data)
summary(fit_interac1)
```

```{r}
fit_interac2 <- data %>% 
  mutate(rank = fct_relevel(rank, "AsstProf", "AssocProf", "Prof")) %>% 
  with(lm(salary~ yrs.service + discipline + yrs.service:discipline))
summary(fit_interac2)
```

การอ่านผล

1.  เราสามารถประเมินอิทธิพลของ main effect ที่ไม่เกี่ยวข้องกับ interaction term ได้เลย

-   เมื่อควบคุมให้ตัวแปรอิสระอื่นคงที่ อาจารย์ที่เป็นรองศาสตราจารย์มีแนวโน้มจะมีเงินเดือนสูงกว่า ผศ. ประมาณ 14000 บท อย่างมีนัยสำคัญทางสถิติที่ระดับ .05 ()

-   เมื่อควบคุมให้ตัวแปรอิสระอื่นคงที่ ศาสตราจารย์ของมหาลัยมีแนวโน้มจะมีเงินเดือนสูงกว่า ผศ. ประมาณ 50000 บาท อย่างมีนัยสำคัญทางสถิติที่ระดับ .05

2.  พิจารณาส่วนปฏิสัมพันธ์

-   พิจารณา term ปฏิสัมพันธ์ก่อนว่ามีนัยสำคัญรึเปล่า (เชิงสถิติ/เชิงปฏฺิบัติ) ถ้าพบนัยสำคัญทางสถิติเราจะไม่อ่านผลที่ main effect

-   เราต้องไปประเมิน simple effect แทน

    -   ประเมินผลของ yrs.service ที่มีต่อ ach จำแนกตาม discipline A และ B \<-- การวิเคราะห์ simple slope

    -   ประเมินผลของ discipline ที่มีต่อ ach จำแนกตาม yrs.service

```{r}
data %>% 
  ggplot(aes(x=yrs.service))+
  geom_histogram()
```

##### การวิเคราะห์ simple slope ของ yrs.service จำแนกตาม discipline

วิเคราะห์อิทธิพลของ yrs.service ต่อ ach จำแนกตาม discipline

เราจะดู simple slope ของ cont. จำแนกตาม cat.

```{r}
library(emmeans)
simple_slopes <- emtrends(fit_interac2, var = "yrs.service", spec = "discipline")
## slope ของ yrs.service จำแนกตาม discipline
simple_slopes 
## ความแตกต่างของ slope ของ yrs.service ระหว่าง 2 discipline
contrast(simple_slopes, method = "pairwise")
```

ลองใช้ `simple_slopes <- emmeans(fit_interac1, pairwise ~ discipline | yrs.service)` ผลลัพธ์ที่ได้แตกต่างกันอย่างไร

##### วิเคราะห์ simple effect ของ discipline บน yrs.service

simple effect ของ discipline ที่วิเคราะห์ได้แต่ละค่าของ yrs.service

```{r}
data %>% summary()
```

ในทำนองเดียวกับ anova เราสามารถวิเคราะห์ simple effect ของ discipline แต่ในกรณีนี้จะเป็นการจำแนกตามระดับของ yrs.service ซึ่งมีจำนวนได้มากมาย

```{r}
summary(data)
emmeans(fit_interac2, pairwise ~ yrs.service | discipline)

emmeans <- emmeans(fit_interac2, pairwise ~ yrs.service | discipline, 
        at = list(yrs.service = c(7, 8,27)))
```

```{r}
emmeans$emmeans %>% data.frame() %>% 
  ggplot(aes(x=emmean, y=factor(yrs.service), col =discipline ))+
  geom_point()+
  geom_errorbar(aes(xmin = lower.CL, xmax = upper.CL), width = 0.1)
```

เราอาจ plot แผนภาพแสดงความสัมพันธ์แบบ interaction ได้ง่าย ๆ ดังนี้

```{r}
fit_interac2 %>% summary()

## partial dependence plot (pdp)

### 1. สร้าง grid ที่เป็น combination ระหว่างตัวแปรอิสระที่ interact กันก่อน
### สร้าง grid ของ yrs.service
yrs_grid <- seq(min(data$yrs.service), max(data$yrs.service), 2)
### สร้าง grid ของ discipline
dis_grid <- c("A","B")

### combination ของ grid
expand_grid(yrs.service = yrs_grid, discipline = dis_grid)

### ทำหน้าที่เป็นค่าของตัวแปรอิสระที่ต้องการนำไปหาค่าทำนาย
grid_pred <- expand_grid(yrs.service = yrs_grid,
                      discipline = unique(data$discipline))
predict(fit_interac2)
predict(fit_interac2, newdata = grid_pred)

grid_pred %>% 
  mutate(pred = predict(fit_interac2, newdata = grid_pred)) %>% 
  ggplot(aes(x = yrs.service, y = pred, color = discipline)) +
  geom_line()



```

```{r}
simple_slopes 
```

```{r}
p1<-data %>% 
  ggplot(aes(x=yrs.service, y=salary))+
  geom_point()+
  geom_smooth(method = "lm")

p2<-data %>% 
  ggplot(aes(x=yrs.service, y=salary, col = discipline))+
  geom_point()+
  geom_smooth(method = "lm")

p1/p2
```

#### ตัวแปรอิสระทั้งสองเป็นตัวแปรเชิงปริมาณ

![](img/interaction2.png){width="50%"}

```{r}
data <- read_csv("eff.csv")
glimpse(data)
```

$$
eff = \beta_0 + \beta_1motiv + \beta_2seef + \beta_3motiv \times seef + \epsilon
$$

\$\$ eff = \beta\_0 + \beta\_1motiv + (\beta\_2seef + \beta\_3motiv \times seef) + \epsilon

\$\$

\$\$ eff = \beta\_0 + \beta\_1motiv + (\beta\_2 + \beta\_3motiv)seef + \epsilon

\$\$

```{r}
data %>% summary()
```

```{r}
fit_interac3 <- data %>% 
  with(lm(eff ~ motiv + seef + motiv:seef))

summary(fit_interac3)
```

ดังนั้นเราสรุปได้ว่า

อิทธิพลของ seef ที่มีต่อ eff อธิบายได้ด้วยสมการ 7.75 - 1.09 x motiv

```{r}
motiv<-3
slope_seef <- 7.75 - 1.09*motiv
```

```{r}
motiv<-5.048
slope_seef <- 7.75 - 1.09*motiv
```

```{r}
data %>% 
  ggplot(aes(x=seef, y=eff))+
  geom_point()+
  geom_smooth(method = "lm", se = F)+
  geom_abline(intercept = 13.6969,slope = 4.48, col ="black", linetype = 2)+
  geom_abline(intercept = 13.6969,slope = 2.24768, col ="black", linetype = 2)+
  geom_abline(intercept = 13.6969,slope = 0.12, col ="black", linetype = 2)+
  ylim(0,80)


simple_slope_seef <- emtrends(fit_interac3, var="seef", spec = "motiv",
         at = list(motiv= c(4.209,5.062,5.954)))

simple_slope_seef %>% contrast(method = "pairwise")

simple_slope_seef %>% data.frame()

data %>% ggplot(aes(x=seef, y=eff))+
  geom_abline(intercept = ,slope = )
```

```{r}
#install.packages("ggeffects")
library(ggeffects)
```

```{r}
ggpredict(fit_interac3, terms = c("seef", "motiv")) |> plot()
```

```{r}
data %>% summary()
```

## Polynomial Regression

สมการถดถอยพหุนาม

ความสัมพันธ์ระหว่างตัวแปรบนโลก อาจจำแนกได้เป็น 2 ลักษณะ

-   linear relationship

-   non-linear relationship

โมเดลการวิเคราะห์ความสัมพันธ์ระหว่างตัวแปรอาจจำแนกได้เป็น 2 ลักษณะเช่นกัน

-   linear (in parameters) model

-   non-linear model

$$
y = mx + c
$$
$$
y = ax^2+bx+c
$$

$$
y = \frac{exp(ax+b)}{1+exp(ax+b)}
$$


```{r}
library(gapminder)
library(ggforce)

gapminder %>% 
  filter(year=="2007") %>% 
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth(method = "lm")+
  scale_x_log10()
```

การสร้าง term polynomial อาจทำได้สองวิธีการ วิธีการแรกคือการใช้ฟังก์ชัน identity function (`I()`) และวิธีการที่สองคือการใช้ `poly()` ความแตกต่างระหว่างการสร้างเทอมพหุนามของทั้งสองวิธีการคือ

-   `I()` เป็นวิธีการสร้างพหุนามโดยตรง โดยเราจะใส่ตัวแปรที่ต้องการยกกำลังในฟังก์ชันนี้ ตัวอย่างเช่น I(x\^2) หมายถึงการสร้างเทอม x\^2 ที่ใช้ในโมเดลการถดถอย ดังนั้นการแปลความหมาย slope ของตัวแปรที่เป็นพหุนามนี้สามารถทำได้อย่างตรงไปตรงมา เพราะตัวแปรต่าง ๆ อยู่ในสเกลต้นฉบับ

-   ปัญหาหนึ่งของการใช้ `I()` คือ multicollinearity!!!

-   `poly()` จะสร้างเทอมพหุนามที่เรียกว่า orthogonal polynomial ซึ่งจะทำให้เราไม่ต้องกังวลเรื่อง multicollinearity แต่ต้องแลกมาด้วยการแปลความหมายความสัมพันธ์ที่ยาก เพราะตัวแปรที่ถูกแปลงเป็น orthogonal polynomial จะอยู่คนละสเกลกับตัวแปรเดิม การแปลงลักษณะนี้จึงเหมาะกับวัตถุประสงค์ในการทำนายมากกว่าอธิบายความสัมพันธ์ื

```{r}
cubic_fit <- gapminder %>% 
  filter(year=="2007") %>% 
  with(lm(lifeExp ~ gdpPercap + I(gdpPercap^2) + I(gdpPercap^3))) 
```

$$
lifeExp = (5.296e+01) + (2.734e-03)gdpPercap - (9.270e-08)gdpPercap^2 + (1.009e-12)gdpPercap^3
$$ \

### ปัญหา Multicollinearity ใน Polynomial Regression

variance inflation factor --\> เป็นดัชนีที่ใช้วัด multicollinearity ของตัวแปรอิสระในโมเดล regression ถ้าค่า VIF มีค่ามากกว่า 10 แสดงว่ามี multicollinearity ในโมเดล

```{r}
#install.packages("car")
library(car)
vif(cubic_fit)
```

```{r}
gapminder %>% 
  filter(year=="2007") %>% 
  mutate(x = gdpPercap,
         x2 =gdpPercap^2,
         x3 = gdpPercap^3) %>% 
  select(x,x2,x3) %>% cor()

```

```{r}
## correlation matrix between centering variables
gapminder %>% 
  filter(year=="2007") %>% 
  mutate(x = gdpPercap,
         x2 =gdpPercap^2,
         x3 = gdpPercap^3) %>% 
  mutate(x_center = x-mean(x),
         x_center2 = x_center^2,
         x_center3 = x_center^3) %>%
  select(x_center, x_center2,x_center3) %>% cor()

## scatter plot
gapminder %>% 
  filter(year=="2007") %>% 
  mutate(x = gdpPercap,
         x2 =gdpPercap^2,
         x3 = gdpPercap^3) %>% 
  mutate(x_center = x-mean(x),
         x_center2 = x_center^2,
         x_center3 = x_center^3) %>%
  select(x_center, x_center2,x_center3) %>% 
  ggplot(aes(x_center,x_center3 ))+
  geom_line()
```

```{r}
cubic_fit <- gapminder %>% 
  filter(year=="2007") %>% 
  with(lm(lifeExp ~ poly(gdpPercap, 3)))
cubic_fit %>% summary()
```

## Regression Diagnostics

regression เป็นโมเดลทางสถิติแบบ parametric ที่การใช้งานจะต้องอยู่ภายใต้ข้อตกลงเบื้องต้นของโมเดลที่ค่อนข้าง strict (แต่ก็ไม่ได้แปลว่าต้อง strict มาก ๆ )

-   Independence

-   Linearity (no misspecification)

-   Normality

-   Homoscedasticity

-   No Missing Values

-   No Influential Outlier

    -   Outlier คือ ค่าสังเกตที่ทำนายไม่ได้หรือมีประสิทธิภาพการทำนายด้วย regression ต่ำ (กล่าวคือมีค่าสัมบูรณ์ของ residual ที่มากเกินไป)


```{r}
library(tidyverse)
data <- read_csv("/Users/choat/Downloads/exam.csv")
glimpse(data)
```
เรา fit multiple regression ตามกรอบแนวคิดของการวิเคราะห์ก่อน


```{r}
simple_reg <- lm(ach ~ learning_performance , data=data)

x <- seq(0,100)
y<-0.5+0.1*x+0.08*x^2+ rnorm(101,0,50)

p1<-data.frame(x,y) %>% 
  ggplot(aes(x=x, y=y))+
  geom_point()+
  geom_smooth(method = "lm", se= F)
```

```{r}
data.frame(x,y)->temp
simple_reg <- lm(y~x, data=temp)

p2<-data.frame(y_truth = y, pred = predict(simple_reg)) %>% 
  mutate(residual = y_truth-pred) %>% 
  ggplot(aes(x=pred, y=residual))+
  geom_point()+
  geom_smooth()+
  geom_hline(yintercept = 0, linetype = "dashed")+
  ggtitle("residual plot")

library(patchwork)
p2
```


```{r}
simple_reg <- lm(y~x, data=temp)
poly_reg <- lm(y~x+I(x^2), data= temp)
anova(simple_reg , poly_reg)
```



```{r}
multi_reg <- lm(ach ~ . , data=data)
summary(multi_reg)
```

### Linearity

ความสัมพันธ์ระหว่างตัวแปรตามกับตัวแปรอิสระจะต้องเป็นความสัมพันธ์เชิงเส้นตรง วิธีการหนึ่งที่มีประโยชน์มากในการตรวจสอบ linearity assumption ระหว่างตัวแปรตามกับตัวแปรอิสระหลาย ๆ ตัว คือการพิจารณา error ของโมเดล เรียกการวิเคราะห์นี้ว่า การวิเคราะห์เศษเหลือ (residual analysis)

เศษเหลือคืออะไร???

กำหนดให้ สมการถดถอยที่ประมาณได้มีสมการเป็น $\hat{y} = b0+b1x1+b2x2+...+bpxp$

$$
e_i = y_i - \hat{y}_i
$$

```{r}
## ค่าทำนายของ y จาก multiple regression ข้างต้น (y_hat)
predict(multi_reg) 

## ค่าจริงของ y
data$ach

## เศษเหลือจากการทำนาย y ด้วย yhat
e = data$ach - predict(multi_reg)

data.frame(ach_truth = data$ach, pred = predict(multi_reg), e = e) %>% 
  mutate(std_res = e-mean(e)/sd(e)) %>% 
  ggplot(aes(x=pred, y=std_res))+
  geom_point()+
  geom_hline(yintercept = 0, linetype = "dashed")+
  ggtitle("Residual plot")
```

```{r}
residuals(multi_reg)
```


### Normality

ข้อตกลงเบื้องต้นระบุอยู่บน noise ของโมเดล แสดงว่าถ้าต้องการตรวจสอบว่าโมเดลมีข้อตกลงเบื้องต้นนี้จริง ๆ ก็จะตรวจสอบผ่าน residual ได้เหมือนกัน

```{r}
p1<-residuals(multi_reg) %>% 
  data.frame() %>% 
  rename(residual = 1) %>% 
  ggplot(aes(x=residual))+
  geom_histogram()

p2<-residuals(multi_reg) %>% 
  data.frame() %>% 
  rename(residual = 1) %>% 
  ggplot(aes(sample = residual)) +
  geom_qq()+
  geom_qq_line()
p1/p2
```

```{r}
residuals(multi_reg) %>% shapiro.test()
```

### homogeneity of variance

ความแปรปรวนของความคลาดเคลื่อนสุ่มมีความเป็นเอกพันธ์กันระหว่างกลุ่มของตัวแปรอิสระ


### homoscedasticity

ความแปรปรวนของความคลาดเคลื่อนสุ่มมีความเป็นเอกพันธ์ในทุกค่า x ที่เป็นไปได้ (ในทุกระดับของค่าทำนายที่เป็นไปได้)

```{r}
data.frame(ach_truth = data$ach, pred = predict(multi_reg), e = e) %>% 
  mutate(std_res = e-mean(e)/sd(e)) %>% 
  ggplot(aes(x=pred, y=std_res))+
  geom_point()+
  geom_hline(yintercept = 0, linetype = "dashed")+
  ggtitle("Residual plot")
```

$$
e_i = y_i - \hat{y}_i
$$

อยากหาความแปรปรวนของความคลาดเคลื่อนสุ่ม

$$
var(e) = \frac{\sum(e^2)}{n-1}
$$




```{r}
data.frame(ach_truth = data$ach, pred = predict(multi_reg), e = e) %>% 
  bind_cols(data) %>% 
  mutate(e2 = e^2) %>%  ## -- sufficient estimator of residual variance
  ggplot(aes(x=pred, y=e2))+
  geom_point()+
  geom_smooth()
```

Breuch-Pagan Test

```{r}
data.frame(ach_truth = data$ach, pred = predict(multi_reg), e = e) %>% 
  bind_cols(data) %>% 
  mutate(e2 = e^2)  %>% 
  with(lm(e2 ~ pred)) %>% summary()
```

White's Test

```{r}
data.frame(ach_truth = data$ach, pred = predict(multi_reg), e = e) %>% 
  bind_cols(data) %>% 
  mutate(e2 = e^2)  %>% 
  with(lm(e2 ~ pred + I(pred^2))) %>% summary()
```

### No Multicollinearity

ในการวิเคราะห์การถดถอยใช้การประมาณค่าพารามิเตอร์ด้วยระบบสมการหลายตัวแปร (simutaneous equation)

ตัวแปรอิสระในโมเดลจะต้องไม่มีความสัมพันธ์ิเชิงเส้นตรงต่อกันมากเกินไป การตรวจสอบข้อตกลงเบื้องต้นนี้สามารถทำได้หลายวิธีการ แต่ละวิธีการก็จะ based on การวิเคราะห์สหสัมพันธ์ระหว่างตัวแปรอิสระ

- correlation matrix ระหว่างตัวแปรอิสระ โดยปกติจะคัดกรองตัวแปรอิสระที่มีแนวโน้มจะมีปัญหาจาก cut-off ของค่า correlation ที่สูงเกินไป 0.7,  0.8, 0.9

- ใช้ดัชนีที่เรียกว่าค่า VIF (variance inflation factor) 

$$
VIF_j = \frac{1}{1-R^2_j}
$$

$R^2_j$ เป็นค่า R2 ที่คำนวณจากสมการถดถอย โดยตัวแปรตามเป็นตัวแปรอิสระตัวที่ j

โดยปกติการประเมินว่าในโมเดลการวิเคราะห์มีปัญหา multicollinearity หรือไม่จาก vif ก็จะมี cut-off ที่มาประกอบ 2, 4, 10, 20, ...

```{r}
library(car)
## vif ach model
vif(multi_reg)
```


### No influential outliers

ค่าผิดปกติ

- anomaly data ---> anomaly detection

- outlier ค่าสังเกตที่มีความแตกต่างไปจากค่าสังเกตโดยปกติ 

  - no influential outlier
  
  - influential outlier


```{r}
gapminder %>% 
  filter(year=="2007") %>% 
  ggplot(aes(x = gdpPercap))+
  geom_boxplot()
```



```{r}
library(gapminder)
library(ggforce)

gapminder %>% 
  filter(year=="2007") %>% 
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth()+
  geom_ellipse(data = gapminder %>% 
                        filter(year=="2007") %>% 
                 filter(lifeExp < 60 & gdpPercap > 8000),
               aes(x0 = mean(gdpPercap), y0 = mean(lifeExp), 
                   a = 2*sd(gdpPercap), b = 2*sd(lifeExp), angle = 0),
               color = 'maroon', linetype = "dashed", fill = NA)
```




-   Leverage value คือ combination ของค่าสังเกตของตัวแปรอิสระที่ผิดปกติ กล่าวคือเป็น outlier ใน feature space ค่า leverage ไม่จำเป็นต้องมีผลต่อค่าของเส้นถดถอยเสมอไป มันเพียงแค่บอกว่าจุดนั้นอยู่ห่างไกลและมีศักยภาพที่จะส่งผลกระทบต่อโมเดล

$$
h_{ii} = x_i^T (X^T X)^{-1} x_i
$$

ค่า $h_{ii}$ 

มีค่าอยู่ในช่วง \[0,1\] ถ้ามีค่าใกล้ 1.0 แสดงว่าค่าสังเกตของตัวแปรอิสระนั้นมีแนวโน้มอยู่ใกล้จากแนวโน้มส่วนใหญ่

-   Influential observation คือค่าสังเกตที่มีอิทธิพลต่อการประมาณค่าพารามิเตอร์ในโมเดลมากกว่าปกติ การประเมินค่าสังเกตประเภทนี้ทำได้ด้วยสถิติ Cook's distance

$$
D_i = \frac{(e_i^2)}{p \cdot MSE} \left( \frac{h_{ii}}{(1 - h_{ii})^2} \right)
$$

```{r fig.height=8}
library(patchwork)
p1 <- gapminder %>% 
  filter(year=="2007") %>% 
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth()

p2 <- gapminder %>% 
  filter(year=="2007") %>% 
  filter(gdpPercap < 10000) %>%
  ggplot(aes(x = gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth()
  
p1/p2
```




### กิจกรรมตรวจสอบข้อตกลงเบื้องต้น

```{r}
teacher_data <- read_csv("/Users/choat/Documents/GitHub/datakruroo.github.io/Programming/regression/TeacherSalaryData.csv")
```

```{r}
teacher_reg <- teacher_data %>% 
  rename(id = 1) %>% 
  with(lm(salary ~. -id, data = .))
plot(teacher_reg)
```


#### linearity

```{r}
data.frame(
pred = predict(teacher_reg),
res = residuals(teacher_reg)) %>% 
  ggplot(aes(x=pred, y=res))+
  geom_point()+
  geom_smooth()+
  geom_hline(yintercept = 0, linetype = "dashed")

data.frame(
pred = predict(teacher_reg),
res = residuals(teacher_reg)) %>% 
  ggplot(aes(sample = res))+
  geom_qq()+
  geom_qq_line()

data.frame(
pred = predict(teacher_reg),
res = residuals(teacher_reg)) %>% 
  ggplot(aes(x = res))+
  geom_histogram()
```


```{r}
multi_reg %>% plot()
teacher_reg %>% plot()
cooks.distance(teacher_reg) %>% data.frame() %>% 
  rename(cook = 1) %>% 
  filter(cook > 0.05)

teacher_data %>% 
  rename(id = 1) %>% 
  filter(id %in% c(250,331))
```



## Regression Remedials

###  Linearity

-   เพิ่ม polynomial หรือ interaction terms เพื่อเพิ่มความซับซ้อนของโมเดล

-   แปลงข้อมูลด้วยการใช้ log, square root, หรือการแปลงเชิงเส้นตรงแบบอื่น ๆ

-   ใช้ Multivariate Adaptive Regression Splines (MARs) Generalized Additive Models (GAMs) เพื่อจับความสัมพันธ์ที่ไม่เป็นเชิงเส้น



### Normality

- โดยส่วนเกือบใหญ่ปัญหา nonnormality มักเกิดจาก heteroscedasticity

- ปัญหา heteroscedasticity ส่วนนึงที่เกือบใหญ่ เกิดจาก model ที่เลือกไม่สอดคล้องกับธรรมชาติของข้อมูล เช่น ลืม interaction term ที่สำคัญในโมเดล

- เพิ่มตัวแปรอิสระหรือเทอมของตัวแปรอิสระที่อาจหายไปเพื่อปรับปรุงการกระจายของ residuals

- แปลงข้อมูล (log-transformation, square root, Box-Cox) เพื่อให้ residuals เป็นปกติมากขึ้น

- ใช้ robust regression เพื่อทนทานต่อผลกระทบของ outliers

- bootstraping regression

#### log-transformation

ตัวอย่างต่อไปนี้ทดลอง take-log ให้ตัวแปรตามคือ salary ในชุดข้อมูล teacher salary ลองพิจารณาเปรียบเทียบผลลัพธ์ที่ได้

```{r}
### log-transformation
teacher_reg_log <- teacher_data %>% 
  rename(id = 1) %>% 
  mutate(log_salary = log(salary)) %>% 
  with(lm(log_salary ~. -id, data = .))

teacher_reg %>% summary()
teacher_reg_log %>% summary()
```

```{r}
teacher_reg %>% plot(main = "Original Model")
teacher_reg_log %>% plot(main = "Log-Transformed Model")
```

#### Box-Cox Transformation

ฟังก์ชันนี้จะพยายามช่วยเราหา lambda (power parameter) ที่ทำให้ log-likelihood ของโมเดลมีค่าสูงสุด

```{r}
### Box-Cox transformation
library(MASS)
### คำนวณ output
boxcox_res <- boxcox(teacher_reg,  data=teacher_data %>% rename(id = 1),
       lambda = seq(-2,2,0.1)) ## box-cox power parameter
### ประเมินค่า lambda ที่ทำให้ log-likelihood มีค่าสูงสุด
data.frame(lambda = boxcox_res$x, 
           log_likelihood = boxcox_res$y) %>% 
filter(log_likelihood == max(log_likelihood))
```
เมื่อได้ค่า lambda จะนำค่า lambda นี้ไปใช้ในการ transform ตัวแปรตาม เราสามารถเขียนสูตรเองก็ได้ ดูสูตรจากในนี้ --> [https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/](https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/)

หรือใช้ฟังก์ชันสำเร็จรูปช่วยในการคำนวณจาก library-car

```{r fig.height = 6}
library(car)
par(mfrow=c(2,1))
teacher_data$salary %>% hist()
car::bcPower(teacher_data$salary, lambda = -0.8282828) %>% hist()
```
ลองไป fit multiple regression ใหม่

```{r}
teacher_reg_bc <- teacher_data %>% 
  rename(id=1) %>% 
  mutate(salary_bc = car::bcPower(teacher_data$salary, lambda = -0.8282828)) %>% 
  dplyr::select(-salary) %>% 
  with(lm(salary_bc ~. -id, data=.))
teacher_reg_bc %>% summary()
teacher_reg_bc %>% plot()
```

- จะเห็นว่าปัญหาหายไปเกือบหมดนอกจาก nonnormality แล้วยังช่วยแก้ปัญหา heteroscedasticity อีกด้วย สะท้อนว่าปัญหาทั้งสองมีความสัมพันธ์กันจริง ๆ 

- แต่การแปลผล coefficient ของ regression ให้มีความหมายทำได้ค่อนข้างยาก

#### เพิ่ม interaction term ในโมเดล

บางครั้งปัญหา nonnormality เกิดจากการละเมิดข้อตกลงเบื้องต้นข้ออื่นโดยเฉพาะ heteroscedasticity และสาเหตุของการเกิด heteroscedasticity บางครั้งก็เกิดจาก misspecification ของโมเดล เช่น การละเลย iteraction term ที่สำคัญ ดังนั้นหากมีเหตุผลหรือข้อมูล support การเพิ่ม interaction term ที่เหมาะสมอาจช่วยให้โมเดลมีคุณสมบัติที่ดีขึ้น

รูปด้านล่างแสดงให้เห็นว่าการเพิ่ม interaction มีแนวโน้มจะลดปัญหา heteroscedasticity ได้บ้าง เพราะเรามีโมเดลที่ซับซ้อนขึ้น และมีความสามารถในการทำนาย/อธิบายหน่วยข้อมูลคืออาจารย์มหาวิทยาลัยที่มีประสบกาณ์ทำงานสูงได้ดีขึ้นบ้างแต่ก็ไม่ได้ดีมากในกรณีนี้

```{r}
teacher_data %>% 
  ggplot(aes(x=yrs.since.phd, y=salary))+
  geom_point()+
  geom_smooth(method= "lm", aes(col = discipline))
```

ทดลองใส่ interaction term ลงในโมเดล อย่างไรก็ตามการแก้ปัญหาในกรณีนี้ไม่ได้มีผลอะไรมากเท่าไหร่

ืnote: การวิเคราะห์ด้านล่างเน้นโชว์ให้ดูความแตกต่างระหว่างโมเดลที่มีกับไม่มี interaction ดังนั้นการใส่ตัวแปรแบบนี้ไม่ได้ makesense 

```{r}
## witjout interaction
teacher_data %>% 
  rename(id = 1) %>% 
  with(lm(salary ~ yrs.since.phd + discipline + rank+sex -id, data = .)) %>% plot(3)

## with interaction
teacher_data %>% 
  rename(id = 1) %>% 
  with(lm(salary ~ yrs.since.phd*discipline*rank*sex -id, data = .)) %>% plot(3)
```


### Homoscedasticity

-   แปลงค่าตัวแปรตาม (เช่น log, square root) เพื่อลดความแปรปรวน

- ควรหาสาเหตุของการละเมิด hetero ว่าเกิดจาก misspecification ส่วนไหนรึเปล่า ---> ลืม interaction term ที่สำคัญ

-   ใช้ Weighted Least Squares (WLS), Generalized Least Squares (GLS), เพื่อปรับน้ำหนักให้กับจุดข้อมูลที่มีความแปรปรวนต่างกัน

-   ใช้ heteroscedasticity-robust standard errors เพื่อปรับ standard errors ของโมเดล

#### Robust Standard Error

ดังที่ได้กล่าวไว้ในคาบเรียนว่า ปัญหา nonnormality และ heteroscedasticity มีความสัมพันธ์กันและทั้งสองปัญหามีผลกระทบต่อการอนุมานเชิงสถิติของโมเดล ได้แก่ การทดสอบสมมุติฐาน และการประมาณช่วงความเชื่อมั่นของค่าพารามิเตอร์ต่าง ๆ 

- nonnormality อาจทำให้ sampling distribution ของสถิติทดสอบ t-test และ F-test สำหรับ regression รวมทั้ง linear model ไม่เป็นไปตามทฤษฎีควบคุม type I error ที่ต้องการไม่ได้

- heteroscedasticity ทำให้การประมาณค่า MSE ของโมเดลคลาดเคลื่อนค่าประมาณที่ได้ไม่เป็นตัวแทนของความแปรปรวรของความคลาดเคลื่อนสุ่มของโมเดล ค่า MSE เป็นส่วนประกอบสำคัญในการประมาณค่า standard error ของค่าประมาณพารามิเตอร์ในโมเดล ปัญหาดังกล่าวจึงกระทบโดยตรงต่อการอนุมานเชิงสถิติ

การแก้ปัญหา heteroscedasticity ในเชิงที่มองว่ามันทำให้การประมาณค่า standard error คลาดเคลื่อนคือการใช้สูตรการประมาณ standard error ใหม่เราเรียกมันว่า robust standard error เมื่อประมาณ SE ได้ดีขึ้น การอนุมานภายใต้การละเมิดข้อตกลงเบื้องต้นนี้ก็จะมีความถูกต้องมากขึ้นไปด้วย

การอนุมานเชิงสถิติด้วย robust standard error ใน R สามารถทำได้ดังนี้

```{r}
##install.packages("sandwich")
##install.packages("lmtest")
library(sandwich)
library(lmtest)
?coef.coeftest ## <-- ลองเรียก help แล้วอ่านดู ตัวประมาณ robust SE มักเรียกว่า sandwich estimator เพราะมีการสอดไส้ในสูตรการประมาณ SE เหมือนทำ sandwich
?vcovHC ## <-- อ่านอันนี้ด้วย
teacher_reg %>% summary()
teacher_reg %>% coeftest(., vcov = vcovHC(., type = "HC1"))
```

ผลการวิเคราะห์ข้างต้นจะเห็นว่าเมื่อใช้ robust SE จะมีการปรับการคำนวณทำให้ SE ที่ได้แตกต่างออกไป นัยสำคัญที่วิเคราะห์ได้ก็จะแตกต่างไปจากเดิม

การแก้ปัญหาด้วยวิธีนี้เป็นการแก้ปัญหาเฉพาะเจาะจงไม่ได้แก้ปัญหาอย่างเป็นระบบหรือที่สาเหตุของปัญหาอย่างแท้จริง โดยปกติมักใช้เมื่อเราจะไม่วิเคราะห์โมเดลอื่นแล้ว และวัตถุประสงค์หลักเป็นการอนุมานเกี่ยวกับพารามิเตอร์ของโมเดลเป็นหลัก



###  Multicollinearity

ปัญหานี้เกิดเมื่อตัวแปรอิสระในโมเดลมีความสัมพันธ์ซึ่งกันและกันสูงเกินไป ส่งผลต่อความเสถียรหรือความน่าเชื่อถือของค่าพารามิเตอร์ regression coefficient ในโมเดล รวมไปถึงการอนุมานเชิงสถิติด้วย ปัญหานี้ร้ายแรงหากตรวจพบควรแก้ไขให้ได้

การแก้ปัญหาสามารถทำได้หลายวิธีการ ได้แก่

#### ลบตัวแปรอิสระที่มีค่า VIF มากเกินไป และส่งผลกระทบต่อการวิเคราะห์

ตัวอย่างด้านล่างแสดงการลบตัวแปรออกจากโมเดลจะเห็นว่าค่าพารามิเตอร์ที่เกี่ยวข้องมีการเปลี่ยนแปลงอย่างมากเมื่อมีการลบตัวแปรออกไป สะท้อนปัญหา multicollinearity ที่ค่อนข้างรุนแรง


```{r}
teacher_reg_no_phd <- lm(salary ~ . -yrs.since.phd, data = teacher_data %>% 
                           dplyr::select(-1))
teacher_reg_no_ser <- lm(salary ~ . -yrs.service, data = teacher_data %>% 
                           dplyr::select(-1))
summary(teacher_reg)
summary(teacher_reg_no_phd)
summary(teacher_reg_no_ser)
vif(teacher_reg_no_ser)
```

อย่างไรก็ตาม regression coeffcient ของ yrs.service ยังมีค่าติดลบ ข้อสังเกตนี้สะท้อนว่าอาจมีปัญหาอื่นเกี่ยวกับการระบุโมเดลที่ผิดพลาดได้ เช่น อาจลืมตัวแปรอิสระที่สำคัญ หรือ interaction term เป็นต้น


#### ทำ feature extraction รวม/ผสาน ตัวแปรอิสระที่มีแนวโน้มจะเกิดปัญหา multi เข้าด้วยกันเป็นตัวแปรใหม่

วิธีการนี้คือการสร้างตัวแปรใหม่ที่รวมตัวแปรอิสระที่มีความสัมพันธ์สูง ๆ เข้าด้วยกัน เช่น การหาค่าเฉลี่ย การบวก หรือการใช้อัลกอริทึมเช่น PCA เข้ามาช่วย เรียกกระบวนการนี้รวม ๆ ว่าการสกัดตัวแปร (feature extraction) 

การรวมตัวแปรด้วยตัวผู้วิเคราะห์ควรพิจารณาเหตุผลในเชิงทฤษฎี หรืออย่างน้อยก็ดูว่าเมื่อนำตัวแปรสองตัวหรือมากกว่านั้นมารวมกัน จะแปลความหมายของตัวแปรใหม่ได้อย่างไร เช่น เงินเดือนพ่อ + เงินเดือนแม่ = รายได้ต่อเดือนของครอบครัว หรือ จำนวนครู/ห้องเรียน = อัตราส่วนครูต่อห้องเรียน

ส่วนการรวมด้วยอัลกอริทึม เช่น PCA การแปลความหมายตัวแปรใหม่ (องค์ประกอบหลัก) จะพิจารณาจากค่าสถิติ การใช้ PCA ในการวิเคราะห์ regression จะเรียกโมเดลการวิเคราะห์นี้ว่า PCR หรือ Principal component regression นอกจากนี้ยังมีวิธีการอื่นในคล้ายกับ PCR เช่น PLS == partial least square regression อีกด้วย

การทำ PCR สามารถทำได้ง่าย ๆ คือ

1. ทำ PCA กับตัวแปรอิสระทั้งหมด แล้วคัดเลือกองค์ประกอบหลักที่จำเป็น

2. แปลความหมายองค์ประกอบหลักที่เลือกมาให้ได้

3. ประมาณ factor score หรือ component score แล้วเอาคะแนนดังกล่าวไปเป็นตัวแปรอิสระใน regression model



### No Influential Ouliers

อย่างที่บอกในห้องเรียนว่า outlier ที่มีอิทธิพลสูงอาจบิดเบือนการประมาณค่าสัมประสิทธิ์ต่าง ๆ ในการวิเคราะห์การถดถอย การวิเคราะห์และตรวจจับค่าสังเกตที่มีอิทธิพลจะช่วยให้ผู้วิเคราะห์ทราบสถานะของค่าสังเกตในชุดข้อมูลว่า 

1. ค่าใดมีแนวโน้มจะเป็นค่าผิดปกติ (ผิดปกติใน y หรือ x)

2. ค่าผิดปกติใดมีอิทธิพลต่อการประมาณในโมเดล

เมื่อคัดกรองค่าผิดปกติที่มีอิทธิพลได้แล้ว ผู้วิเคราะห์ควรลองวิเคราะห์เปรียบเทียบว่าการมี หรือไม่มีค่าสังเกตที่ผิดปกติดังกล่าว ทำให้การประมาณค่าพารามิเตอร์ในโมเดลหรือประสิทธิภาพการทำนายของโมเดลมีการเปลี่ยนแปลงไปมากแค่ไหน มีนัยสำคัญต่อการวิเคราะห์หรือไม่ แล้วจึงพิจารณาว่าจะแก้หรือไม่แก้ปัญหาดังกล่าว

การแก้ปัญหาค่าผิดปกติที่มีอิทธิพลนี้สามารถทำได้หลัก ๆ อาจจะ 3 วิธีการ

1. ลบหรือปรับค่าสังเกตที่ถูกระบุว่าเป็น influential outiers อันนี้ง่าย ๆ คัดกรองค่าที่มี cook's distance สูงเกินเกณฑ์ที่กำหนด และลบออก

2. ทำ bining กล่าวคือหาก influential outlier ที่เกิดขึ้น เกิดขึ้นในตัวแปรอิสระเราอาจแปลงค่าตัวแปรอิสระเชิงปริมาณให้เป็นตัวแปรจัดประเภท โดยการแบ่งค่าที่เป็นไปได้ออกเป็นอันครภาคชั้น วิธีการนี้ช่วยลดผลกระทบของ influential outlier ได้อย่างมีนัยสำคัญ

3. มองค่าผิดปกติดังกล่าวเป็น missing value จากนั้น impute มันด้วย algorithm ที่เหมาะสม

4. ใช้ Robust Regression เช่น quantile regression ที่มีความไวต่อค่าผิดปกติน้อยกว่า

... อื่น ๆ 


## Model Selection

ในบริบทของการวิเคราะห์การถดถอย โดยปกติการคัดเลือกโมเดลส่วนหลัก ๆ จะเกี่ยวข้องกับการคัดเลือกตัวแปรอิสระเข้าสู่โมเดล และอีกส่วนหนึ่งคือการกำหนด functional form ของโมเดล ในบทเรียนนี้จะกล่าวถึงส่วน feature selection 

การทำ feature selection คือการคัดกรองตัวแปรที่ไม่มีสาระสำคัญหรือมีความสำคัญน้อยในการพัฒนาโมเดลออกไป


```{mermaid}
flowchart TD

F1-->X1((X))
F2-->A[F2]
F3-->B[F3]
F4-->X2((X))
F5-->X3((X))
```


การคัดเลือกตัวแปรอิสระเข้าสู่โมเดลมีหลายขั้นตอน อาจจะจำแนกออกเป็นสองประเภทใหญ่ ได้แก่ theory-driven และ data-driven โดยที่ theory-driven จะใช้ความรู้ที่มีอยู่จากงานวิจัยหรือทฤษฎีที่เกี่ยวข้องเพื่อกำหนดและคัดเลือกตัวแปร  ในขณะที่ data-driven จะใช้ข้อมูลที่มีอยู่เพื่อคัดเลือกตัวแปรอิสระที่มีความเหมาะสมและมีประสิทธิภาพในการทำนายเข้าสู่โมเดล

- filter methods

- wrapper methods

- embedded methods


### กิจกรรมที่ 1 ทำลองวิเคราะห์การถดถอย

ลองทำ regression analysis เพื่อสร้างโมเดลทำนาย student_outcome_online แล้วพิจารณาผลลัพธ์ที่ได้

```{r}

large_data <- read_csv("/Users/choat/Downloads/onlinelearning_miss.csv")
large_data %>% glimpse()
large_data %>% dplyr::select(-1) %>% 
### ปรับชื่อตัวแปรใหม่
  rename(student_id = X,
         school_type = 2,
         school_location = 3,
         num_subject = 4,
         num_online = 5,
         num_onsite = 6,
         school_readiness = 7,
         avg_teacher_age = 15,
         avg_teacher_techcomp = 16,
         student_gender = 23,
         student_level = 24,
         student_location = 25,
         student_learningstyle = 26,
         student_internet_readiness = 30,
         student_home_climate = 31,
         student_techcomp = 32,
         student_learning_attitude = 33,
         student_stress_online = 40,
         student_stress_onsite = 41
         ) %>% 
### สร้างตัวแปรใหม่
  rowwise() %>% 
  mutate(
         student_climate_online = mean(c(climate.online1, climate.online2, climate.online3)),
         student_climate_onsite = rowMeans(across(starts_with("climate.onsite"))),
         student_interaction_online = mean(c(interaction.online1, interaction.online2, interaction.online3)),
         student_interaction_onsite = rowMeans(across(starts_with("interaction.onsite"))),
         student_outcome_online = (outcome.online1*20 + outcome.online2*20 + outcome.online3*20 +outcome.online4)/4,
         student_outcome_onsite = (outcome.onsite1*20 + outcome.onsite2*20 + outcome.onsite3*20 +outcome.onsite4)/4
         ) %>%
  dplyr::select(-starts_with("interaction"), -starts_with("outcome")) %>% 
  dplyr::select(-starts_with("climate")) %>%
  mutate(student_num_device = sum(c(smartphone,ipad,computer),na.rm = T))  %>% 
  ungroup()
```

ผลการวิเคราะห์ที่ได้มีข้อสังเกตอะไรบ้าง

### Filter Methods

Filter Methods เป็นหนึ่งในเทคนิคการคัดเลือกตัวแปร (Feature Selection) ที่ใช้ คุณสมบัติทางสถิติของตัวแปร ในการคัดกรองตัวแปรที่มีความสำคัญต่อการสร้างโมเดล โดยกระบวนการนี้จะไม่พึ่งพาโมเดลใด ๆ ในการประเมิน แต่จะใช้สถิติพื้นฐาน เช่น ความสัมพันธ์ (correlation) ระหว่างตัวแปรอิสระกับตัวแปรตาม และค่าสถิติอื่น ๆ เช่น Chi-square, ANOVA, Mutual Information เพื่อกรองตัวแปรที่มีความสำคัญสูงไว้

- filter zero or near-zero variance

- correlation-based filtering

จุดเด่นของวิธีการนี้คือทำได้อย่างรวดเร็ว แต่เป็นการวิเคราะห์ที่อาจจะละเลยความสัมพันธ์ที่ซับซ้อนระหว่างตัวแปรอิสระกับตัวแปรตาม นอกจากนี้ผลลัพธ์ที่ได้อาจจะไม่ได้เป็นผลลัพธ์ที่ดีที่สุดที่เป็นไปได้

```{r}
### create predictor matrix
use_data <- large_data %>% dplyr::select(-1) %>% 
### ปรับชื่อตัวแปรใหม่
  rename(student_id = X,
         school_type = 2,
         school_location = 3,
         num_subject = 4,
         num_online = 5,
         num_onsite = 6,
         school_readiness = 7,
         avg_teacher_age = 15,
         avg_teacher_techcomp = 16,
         student_gender = 23,
         student_level = 24,
         student_location = 25,
         student_learningstyle = 26,
         student_internet_readiness = 30,
         student_home_climate = 31,
         student_techcomp = 32,
         student_learning_attitude = 33,
         student_stress_online = 40,
         student_stress_onsite = 41
         ) %>% 
### สร้างตัวแปรใหม่
  rowwise() %>% 
  mutate(
         student_climate_online = mean(c(climate.online1, climate.online2, climate.online3)),
         student_climate_onsite = rowMeans(across(starts_with("climate.onsite"))),
         student_interaction_online = mean(c(interaction.online1, interaction.online2, interaction.online3)),
         student_interaction_onsite = rowMeans(across(starts_with("interaction.onsite"))),
         student_outcome_online = (outcome.online1*20 + outcome.online2*20 + outcome.online3*20 +outcome.online4)/4,
         student_outcome_onsite = (outcome.onsite1*20 + outcome.onsite2*20 + outcome.onsite3*20 +outcome.onsite4)/4
         ) %>%
  dplyr::select(-starts_with("interaction"), -starts_with("outcome")) %>% 
  dplyr::select(-starts_with("climate")) %>%
  mutate(student_num_device = sum(c(smartphone,ipad,computer),na.rm = T))  %>% 
  ungroup()
```


แบ่งชุดข้อมูล online learning 

```{r}
online_data <- use_data %>% 
  dplyr::select(-contains("onsite"))
glimpse(online_data)
```


#### Near-Zero Variance

สร้างชุดข้อมูล predictor matrix ของ online learning

- `freqCut` เปรียบเทียบความถี่ของค่าที่พบมากที่สุดกับค่าที่พบมากที่สุดเป็นอันดับสอง เพื่อดูว่าคอลัมน์นั้นมีการกระจายตัวของค่ามากน้อยเพียงใด

- `uniqueCut` เปรียบเทียบจำนวนค่าที่ไม่ซ้ำกันกับจำนวนตัวอย่างทั้งหมดในคอลัมน์

```{r}
predictor_online <- online_data %>% dplyr::select(-student_outcome_online)
## install.packages("caret")
library(caret)
nearZeroVar(predictor_online, freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE)
```

#### Correlation-based method




### Wrapper Methods

- สร้างโมเดลทำนายหลายชุด: ทดลองชุดของตัวแปรหลายชุด โดยเลือกเพิ่มหรือลบตัวแปรทีละตัว

- ใช้เกณฑ์การประเมินโมเดลเช่น AIC, BIC, AUC, F1-score, RMSE, MAE, หรือค่าสถิติอื่น ๆ ในการประเมินความสามารถของโมเดล

- เลือกชุดตัวแปรที่ดีที่สุด: ชุดตัวแปรที่ให้ประสิทธิภาพดีที่สุดจะถูกเลือกเพื่อใช้ในการสร้างโมเดลสุดท้าย

#### Best Subset Regression

- วิธีการหนึ่งที่ใช้ค้นหาโมเดลที่ดีที่สุดคือ best subset regression ซึ่งจะทำการสร้างโมเดลทุกโมเดลที่เป็นไปได้จากตัวแปรอิสระที่มี จากนั้นเปรียบเทียบ empirical fit ระหว่างโมเดลดังกล่าว

- อย่างไรก็ตามวิธีการดังกล่าวจะใช้ทรัพยากรค่อนข้างเยอะ เพราะจะต้องสร้างโมเดลจากทุกความเป็นไปได้ ยกตัวอย่างเช่น หากมีตัวแปรอิสระทั้งหมด p ตัว โมเดลที่มีเฉพาะ main effect ที่เป็นไปได้จะมีทั้งหมด 2^p โมเดล เทคนิคนี้จึงเหมาะสำหรับสถานการณ์ที่มีข้อมูลที่ขนาดไม่ใหญ่มากเกินไป


```{r}
#install.packages("leaps")
library(leaps)

reg_model <- regsubsets(student_outcome_online ~ . -student_id, data=online_data , nbest =1, nvmax = 10,
                        method = "exhaustive")
sum <- summary(reg_model)
```


ข้อมูลตัวอย่างสำหรับหัวข้อนี้คือ TeacherSalaryData.csv

```{r}
data <- read_csv("TeacherSalaryData.csv")
glimpse(data)
### visualize numeric variables
p1<-data %>% 
  select_if(is.numeric) %>%
  dplyr::select(-1) %>% 
  pivot_longer(everything()) %>%
  ggplot(aes(value))+
  geom_histogram(fill = "steelblue")+
  facet_wrap(~name, scales = "free")+
  theme_light()
### visualize categorical variables
p2<-data %>% 
  select_if(is.character) %>%
  pivot_longer(everything()) %>%
  ggplot(aes(x = value))+
  geom_bar(fill = "steelblue")+
  facet_wrap(~name, scales = "free")+
  theme_light()
p1/p2
```


